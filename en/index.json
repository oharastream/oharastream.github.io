[{"authors":["admin"],"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1591757270,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://oharastream.github.io/en/author/vito-jeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/vito-jeng/","section":"authors","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.","tags":null,"title":"Vito Jeng","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592978868,"objectID":"e0fcc2468e9b55a81d73da63e60bde34","permalink":"https://oharastream.github.io/en/docs/0.10.x/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/en/docs/0.10.x/","section":"docs","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Ohara is open to users, developers and man living in earth. Hence, we prepare a bunch of docs to help you to understand ohara comprehensively but quickly.\nFor User This section is for the ohara users who are going to install and then test official streaming application. For this case, you don\u0026rsquo;t need to compile, build or write anything for ohara. All you have to read the user guide and then follow the advice of installation section.\nWe now provide a quickstart VM for you to quickly start Ohara and play with it in a single VM. You can learn more from Using Quickstart VM\nFor Developer If you want to know how to build ohara, please read: How to build Ohara\nApart from contributing code to ohara, you can also leverage ohara to design your custom connector, custom stream or build your UI interface via Ohara REST interface.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597898583,"objectID":"af502acda897105fb66141437614ad17","permalink":"https://oharastream.github.io/en/docs/0.11.x/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/en/docs/0.11.x/","section":"docs","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Ohara is open to users, developers and man living in earth. Hence, we prepare a bunch of docs to help you to understand ohara comprehensively but quickly.\nFor User This section is for the ohara users who are going to install and then test official streaming application. For this case, you don\u0026rsquo;t need to compile, build or write anything for ohara. All you have to read the user guide and then follow the advice of installation section.\nWe now provide a quickstart VM for you to quickly start Ohara and play with it in a single VM. You can learn more from Using Quickstart VM\nFor Developer If you want to know how to build ohara, please read: How to build Ohara\nApart from contributing code to ohara, you can also leverage ohara to design your custom connector, custom stream or build your UI interface via Ohara REST interface.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597894011,"objectID":"3b701f9c20aa8f279c3de3456043d719","permalink":"https://oharastream.github.io/en/docs/master/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/en/docs/master/","section":"docs","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Ohara provides a bunch of REST APIs of managing data, applications and cluster for Ohara users. Both request and response must have application/json content type, hence you should set content type to application/json in your request.\n Content-Type: application/json\n and add content type of the response via the HTTP Accept header:\n Accept: application/json\n Statuses \u0026amp; Errors\nOhara leverages akka http to support standards-compliant HTTP statuses. your clients should check the HTTP status before parsing response entities. The error message in response body are format to json content.\n{ \u0026quot;code\u0026quot;: \u0026quot;java.lang.IllegalArgumentException\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;Unsupported restful api:vasdasd. Or the request is invalid to the vasdasd\u0026quot;, \u0026quot;stack\u0026quot;: \u0026quot;java.lang.IllegalArgumentException: Unsupported restful api:vasdasd. Or the request is invalid to the vasdasd at\u0026quot; }   code (string) — the type of error. It is normally a type of java exception. message (string) — a brief description of error stack (string) — error stack captured by server  Manage clusters\nYou are tired to host a bunch of clusters when you just want to build a pure streaming application. So do we! Ohara aims to take over the heavy management and simplify your life. Ohara leverage the docker technology to run all process in containers. If you are able to use k8s, Ohara is good at deploying all containers via k8s. If you are too afraid to touch k8s, Ohara is doable to be based on ssh connection to control all containers.\nOhara automatically configure all clusters for you. Of course, you have the freedom to overwrite any settings. see section zookeeper, broker and worker to see more details.\nIn order to provide a great experience in exercising containers, Ohara pre-builds a lot of docker images with custom scripts. Of course, Ohara APIs allow you to choose other image instead of Ohara official images. However, it works only if the images you pick up are compatible to Ohara command. see here for more details. Also, all official images are hosted by docker hub\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597894011,"objectID":"47ed2a86eae67dfdaa76dd083661c284","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/docs/0.11.x/rest-api/","section":"docs","summary":"Ohara provides a bunch of REST APIs of managing data, applications and cluster for Ohara users. Both request and response must have application/json content type, hence you should set content type to application/json in your request.","tags":null,"title":"Ohara REST Interface","type":"docs"},{"authors":null,"categories":null,"content":"Ohara provides a bunch of REST APIs of managing data, applications and cluster for Ohara users. Both request and response must have application/json content type, hence you should set content type to application/json in your request.\n Content-Type: application/json\n and add content type of the response via the HTTP Accept header:\n Accept: application/json\n Statuses \u0026amp; Errors\nOhara leverages akka http to support standards-compliant HTTP statuses. your clients should check the HTTP status before parsing response entities. The error message in response body are format to json content.\n{ \u0026quot;code\u0026quot;: \u0026quot;java.lang.IllegalArgumentException\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;Unsupported restful api:vasdasd. Or the request is invalid to the vasdasd\u0026quot;, \u0026quot;stack\u0026quot;: \u0026quot;java.lang.IllegalArgumentException: Unsupported restful api:vasdasd. Or the request is invalid to the vasdasd at\u0026quot; }   code (string) — the type of error. It is normally a type of java exception. message (string) — a brief description of error stack (string) — error stack captured by server  Manage clusters\nYou are tired to host a bunch of clusters when you just want to build a pure streaming application. So do we! Ohara aims to take over the heavy management and simplify your life. Ohara leverage the docker technology to run all process in containers. If you are able to use k8s, Ohara is good at deploying all containers via k8s. If you are too afraid to touch k8s, Ohara is doable to be based on ssh connection to control all containers.\nOhara automatically configure all clusters for you. Of course, you have the freedom to overwrite any settings. see section zookeeper, broker and worker to see more details.\nIn order to provide a great experience in exercising containers, Ohara pre-builds a lot of docker images with custom scripts. Of course, Ohara APIs allow you to choose other image instead of Ohara official images. However, it works only if the images you pick up are compatible to Ohara command. see here for more details. Also, all official images are hosted by docker hub\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597829291,"objectID":"94040432c6a365fce80ca00cd7bfeb32","permalink":"https://oharastream.github.io/en/docs/master/rest-api/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/docs/master/rest-api/","section":"docs","summary":"Ohara provides a bunch of REST APIs of managing data, applications and cluster for Ohara users. Both request and response must have application/json content type, hence you should set content type to application/json in your request.","tags":null,"title":"Ohara REST Interface","type":"docs"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1597829291,"objectID":"4cdd37113783e47641dd300543c94e1b","permalink":"https://oharastream.github.io/en/docs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/docs/","section":"docs","summary":"","tags":null,"title":"Ohara documents","type":"docs"},{"authors":null,"categories":null,"content":"Prerequisites  Operation system: Linux  Make: Make is a build automation tool that automatically builds executable programs. Make is already built in Linux or macOS.  Packer 1.4+: Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.  VirtualBox 6.0+  Build OVA file  We use make command to execute all our tasks. For building the specific version of Ohara quickstart VM, you must provide the argument OHARA_VER when execute the make command.  Build the OVA file, following is an example(OHARA_VER=0.7.1):\n[quickstart]$ cd vm [vm]$ make OHARA_VER=0.7.1 ova OHARA_VER=0.7.1 Build time: 2019/09/10 10:10 Start building quickstart VM ova file... virtualbox-iso output will be in this color. ==\u0026gt; virtualbox-iso: Retrieving ISO ==\u0026gt; virtualbox-iso: Trying .cache/ubuntu-18.04.3-server-amd64.iso ==\u0026gt; virtualbox-iso: Trying .cache/ubuntu-18.04.3-server-amd64.iso?checksum=sha256%3A7d8e0055d663bffa27c1718685085626cb59346e7626ba3d3f476322271f573e ==\u0026gt; virtualbox-iso: .cache/ubuntu-18.04.3-server-amd64.iso?checksum=sha256%3A7d8e0055d663bffa27c1718685085626cb59346e7626ba3d3f476322271f573e =\u0026gt; /your/project/path/ohara/vms/quickstart/.cache/packer_cache/fdcf467e727a368c2aac26ac2284f0f517dc29fb.iso ==\u0026gt; virtualbox-iso: Starting HTTP server on port 8251 ==\u0026gt; virtualbox-iso: Creating virtual machine... ==\u0026gt; virtualbox-iso: Creating hard drive... ==\u0026gt; virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 3248) ==\u0026gt; virtualbox-iso: Executing custom VBoxManage commands... : : : : (SKIP) : : : : ==\u0026gt; virtualbox-iso: Gracefully halting virtual machine... ==\u0026gt; virtualbox-iso: Preparing to export machine... virtualbox-iso: Deleting forwarded port mapping for the communicator (SSH, WinRM, etc) (host port 3248) ==\u0026gt; virtualbox-iso: Exporting virtual machine... virtualbox-iso: Executing: export ohara-quickstart-0.7.1 --output build/ohara-quickstart-0.7.1.ova ==\u0026gt; virtualbox-iso: Deregistering and deleting VM... Build 'virtualbox-iso' finished. ==\u0026gt; Builds finished. The artifacts of successful builds are: --\u0026gt; virtualbox-iso: VM files in directory: build Done.  The OVA file will be output to: build/ohara-quickstart-{OHARA_VER}.ova\n Currently, we use Ubuntu 18.04.03 LTS as Quickstart VM\u0026rsquo;s operation system. Packer will try to find the ubuntu iso file in the quickstart/.cache folder first, and then download the Ubuntu iso file from internet if the iso file not be found in the cache folder.\nTo save your building time, you can download the Ubuntu iso file manually and put into quickstart/.cache folder.\n  Import OVA After generated the quickstart ova file, you can use VirtualBox user interface to import the OVA file(File -\u0026gt; Import Appliance) or use following command:\n[vm]$ make OHARA_VER=0.7.1 vm-import vboxmanage import build/ohara-quickstart-0.7.1.ova --vsys 0 --vmname ohara-quickstart-0.7.1 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% Interpreting /your/project/path/ohara/vms/quickstart/build/ohara-quickstart-0.7.1.ova... OK. Disks: vmdisk1 85899345920 -1 http://www.vmware.com/interfaces/specifications/vmdk.html#streamOptimized ohara-quickstart-0.7.1-disk001.vmdk -1 -1 Virtual system 0: 0: Suggested OS type: \u0026quot;Ubuntu_64\u0026quot; (change with \u0026quot;--vsys 0 --ostype \u0026lt;type\u0026gt;\u0026quot;; use \u0026quot;list ostypes\u0026quot; to list all possible values) 1: VM name specified with --vmname: \u0026quot;ohara-quickstart-0.7.1\u0026quot; 2: Description \u0026quot;Ohara Quickstart VM Ohara version: 0.7.1 Build time: 2019/09/10 10:10\u0026quot; (change with \u0026quot;--vsys 0 --description \u0026lt;desc\u0026gt;\u0026quot;) 3: Number of CPUs: 2 (change with \u0026quot;--vsys 0 --cpus \u0026lt;n\u0026gt;\u0026quot;) 4: Guest memory: 4096 MB (change with \u0026quot;--vsys 0 --memory \u0026lt;MB\u0026gt;\u0026quot;) 5: Network adapter: orig NAT, config 3, extra slot=0;type=NAT 6: Network adapter: orig HostOnly, config 3, extra slot=1;type=HostOnly 7: IDE controller, type PIIX4 (disable with \u0026quot;--vsys 0 --unit 7 --ignore\u0026quot;) 8: IDE controller, type PIIX4 (disable with \u0026quot;--vsys 0 --unit 8 --ignore\u0026quot;) 9: Hard disk image: source image=ohara-quickstart-0.7.1-disk001.vmdk, target path=/home/xxxx/VirtualBox VMs/ohara-quickstart-0.7.1/ohara-quickstart-0.7.1-disk001.vmdk, controller=7;channel=0 (change target path with \u0026quot;--vsys 0 --unit 9 --disk path\u0026quot;; disable with \u0026quot;--vsys 0 --unit 9 --ignore\u0026quot;)  Use Quickstart VM After import quickstart VM to VirtualBox, you can press Start button to start the VM. And then you can see following screen:\nUbuntu 10.04.03 LTS ohara-vm tty1 ohara-vm login:  Please use ohara as login account and oharastream as password to login to VM. If this is your first time to login Quickstart VM, the progress of pull Ohara docker images will be starting automatically. So please make sure your machine can connect to Internet.\nAfter download the images, and then you can see the ip address info of the VM, for example:\nIP address info: lo UNKNOWN 127.0.0.1/8 ::1/128 enp0s3 UP 10.0.2.15/24 fe80::a00:27ff:feac:ad8a/64 enp0s8 UP 192.168.56.114/24 fe80::a00:27ff:fe09:1a1e/64 docker0 DOWN 172.17.0.1/16  We can find the private IP address 192.168.56.114 (enp0s8) in the above list. So the configurator ip address is 192.168.56.114 .\nRun Ohara configurator(port 12345):\n$ ./ohara-configurator.sh docker run --rm -p 12345:12345 -d oharastream/configurator:0.7.1 --port 12345  Run Ohara manager(port 5050), provide the configurator ip address as parameter:\n$ ./ohara-manager.sh 192.168.56.114 docker run --rm -p 5050:5050 -d oharastream/manager:0.7.1 --port 5050 --configurator http://192.168.56.114:12345/v0  Now you can open your browser and input the link: http://192.168.56.114:5050 to open the main page of Ohara Manager.\ncommands Following are other commands for development purpose:\n[vm]$ make OHARA_VER=0.7.1 Usage: $ make OHARA_VER={version} {command} Both {version} and {command} is required. Command: clean: Remove following files: build/, .cache/packer_cache/, .cache/packer.log ova: Generate the OVA file. The output is build/ohara-quickstart-{OHARA_VER}.ova vm-import: Import the ova file into VirtualBox vm-start: Start quickstart VM vm-poweroff: Poweroff quickstart VM vm-reset: Reset quickstart VM vm-delete: Unregister \u0026amp; delete quickstart VM  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"de23a67dc5ee7c99c9c20a605f493311","permalink":"https://oharastream.github.io/en/docs/0.11.x/build/build-quickstart-vm/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/build/build-quickstart-vm/","section":"docs","summary":"Prerequisites  Operation system: Linux  Make: Make is a build automation tool that automatically builds executable programs. Make is already built in Linux or macOS.  Packer 1.4+: Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.","tags":null,"title":"How to build Quickstart VM","type":"docs"},{"authors":null,"categories":null,"content":"Prerequisites  Operation system: Linux  Make: Make is a build automation tool that automatically builds executable programs. Make is already built in Linux or macOS.  Packer 1.4+: Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.  VirtualBox 6.0+  Build OVA file  We use make command to execute all our tasks. For building the specific version of Ohara quickstart VM, you must provide the argument OHARA_VER when execute the make command.  Build the OVA file, following is an example(OHARA_VER=0.7.1):\n[quickstart]$ cd vm [vm]$ make OHARA_VER=0.7.1 ova OHARA_VER=0.7.1 Build time: 2019/09/10 10:10 Start building quickstart VM ova file... virtualbox-iso output will be in this color. ==\u0026gt; virtualbox-iso: Retrieving ISO ==\u0026gt; virtualbox-iso: Trying .cache/ubuntu-18.04.3-server-amd64.iso ==\u0026gt; virtualbox-iso: Trying .cache/ubuntu-18.04.3-server-amd64.iso?checksum=sha256%3A7d8e0055d663bffa27c1718685085626cb59346e7626ba3d3f476322271f573e ==\u0026gt; virtualbox-iso: .cache/ubuntu-18.04.3-server-amd64.iso?checksum=sha256%3A7d8e0055d663bffa27c1718685085626cb59346e7626ba3d3f476322271f573e =\u0026gt; /your/project/path/ohara/vms/quickstart/.cache/packer_cache/fdcf467e727a368c2aac26ac2284f0f517dc29fb.iso ==\u0026gt; virtualbox-iso: Starting HTTP server on port 8251 ==\u0026gt; virtualbox-iso: Creating virtual machine... ==\u0026gt; virtualbox-iso: Creating hard drive... ==\u0026gt; virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 3248) ==\u0026gt; virtualbox-iso: Executing custom VBoxManage commands... : : : : (SKIP) : : : : ==\u0026gt; virtualbox-iso: Gracefully halting virtual machine... ==\u0026gt; virtualbox-iso: Preparing to export machine... virtualbox-iso: Deleting forwarded port mapping for the communicator (SSH, WinRM, etc) (host port 3248) ==\u0026gt; virtualbox-iso: Exporting virtual machine... virtualbox-iso: Executing: export ohara-quickstart-0.7.1 --output build/ohara-quickstart-0.7.1.ova ==\u0026gt; virtualbox-iso: Deregistering and deleting VM... Build 'virtualbox-iso' finished. ==\u0026gt; Builds finished. The artifacts of successful builds are: --\u0026gt; virtualbox-iso: VM files in directory: build Done.  The OVA file will be output to: build/ohara-quickstart-{OHARA_VER}.ova\n Currently, we use Ubuntu 18.04.03 LTS as Quickstart VM\u0026rsquo;s operation system. Packer will try to find the ubuntu iso file in the quickstart/.cache folder first, and then download the Ubuntu iso file from internet if the iso file not be found in the cache folder.\nTo save your building time, you can download the Ubuntu iso file manually and put into quickstart/.cache folder.\n  Import OVA After generated the quickstart ova file, you can use VirtualBox user interface to import the OVA file(File -\u0026gt; Import Appliance) or use following command:\n[vm]$ make OHARA_VER=0.7.1 vm-import vboxmanage import build/ohara-quickstart-0.7.1.ova --vsys 0 --vmname ohara-quickstart-0.7.1 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% Interpreting /your/project/path/ohara/vms/quickstart/build/ohara-quickstart-0.7.1.ova... OK. Disks: vmdisk1 85899345920 -1 http://www.vmware.com/interfaces/specifications/vmdk.html#streamOptimized ohara-quickstart-0.7.1-disk001.vmdk -1 -1 Virtual system 0: 0: Suggested OS type: \u0026quot;Ubuntu_64\u0026quot; (change with \u0026quot;--vsys 0 --ostype \u0026lt;type\u0026gt;\u0026quot;; use \u0026quot;list ostypes\u0026quot; to list all possible values) 1: VM name specified with --vmname: \u0026quot;ohara-quickstart-0.7.1\u0026quot; 2: Description \u0026quot;Ohara Quickstart VM Ohara version: 0.7.1 Build time: 2019/09/10 10:10\u0026quot; (change with \u0026quot;--vsys 0 --description \u0026lt;desc\u0026gt;\u0026quot;) 3: Number of CPUs: 2 (change with \u0026quot;--vsys 0 --cpus \u0026lt;n\u0026gt;\u0026quot;) 4: Guest memory: 4096 MB (change with \u0026quot;--vsys 0 --memory \u0026lt;MB\u0026gt;\u0026quot;) 5: Network adapter: orig NAT, config 3, extra slot=0;type=NAT 6: Network adapter: orig HostOnly, config 3, extra slot=1;type=HostOnly 7: IDE controller, type PIIX4 (disable with \u0026quot;--vsys 0 --unit 7 --ignore\u0026quot;) 8: IDE controller, type PIIX4 (disable with \u0026quot;--vsys 0 --unit 8 --ignore\u0026quot;) 9: Hard disk image: source image=ohara-quickstart-0.7.1-disk001.vmdk, target path=/home/xxxx/VirtualBox VMs/ohara-quickstart-0.7.1/ohara-quickstart-0.7.1-disk001.vmdk, controller=7;channel=0 (change target path with \u0026quot;--vsys 0 --unit 9 --disk path\u0026quot;; disable with \u0026quot;--vsys 0 --unit 9 --ignore\u0026quot;)  Use Quickstart VM After import quickstart VM to VirtualBox, you can press Start button to start the VM. And then you can see following screen:\nUbuntu 10.04.03 LTS ohara-vm tty1 ohara-vm login:  Please use ohara as login account and oharastream as password to login to VM. If this is your first time to login Quickstart VM, the progress of pull Ohara docker images will be starting automatically. So please make sure your machine can connect to Internet.\nAfter download the images, and then you can see the ip address info of the VM, for example:\nIP address info: lo UNKNOWN 127.0.0.1/8 ::1/128 enp0s3 UP 10.0.2.15/24 fe80::a00:27ff:feac:ad8a/64 enp0s8 UP 192.168.56.114/24 fe80::a00:27ff:fe09:1a1e/64 docker0 DOWN 172.17.0.1/16  We can find the private IP address 192.168.56.114 (enp0s8) in the above list. So the configurator ip address is 192.168.56.114 .\nRun Ohara configurator(port 12345):\n$ ./ohara-configurator.sh docker run --rm -p 12345:12345 -d oharastream/configurator:0.7.1 --port 12345  Run Ohara manager(port 5050), provide the configurator ip address as parameter:\n$ ./ohara-manager.sh 192.168.56.114 docker run --rm -p 5050:5050 -d oharastream/manager:0.7.1 --port 5050 --configurator http://192.168.56.114:12345/v0  Now you can open your browser and input the link: http://192.168.56.114:5050 to open the main page of Ohara Manager.\ncommands Following are other commands for development purpose:\n[vm]$ make OHARA_VER=0.7.1 Usage: $ make OHARA_VER={version} {command} Both {version} and {command} is required. Command: clean: Remove following files: build/, .cache/packer_cache/, .cache/packer.log ova: Generate the OVA file. The output is build/ohara-quickstart-{OHARA_VER}.ova vm-import: Import the ova file into VirtualBox vm-start: Start quickstart VM vm-poweroff: Poweroff quickstart VM vm-reset: Reset quickstart VM vm-delete: Unregister \u0026amp; delete quickstart VM  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592897539,"objectID":"a6470a51f2fe32d3068a95b4dcf15cab","permalink":"https://oharastream.github.io/en/docs/master/build/build-quickstart-vm/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/master/build/build-quickstart-vm/","section":"docs","summary":"Prerequisites  Operation system: Linux  Make: Make is a build automation tool that automatically builds executable programs. Make is already built in Linux or macOS.  Packer 1.4+: Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.","tags":null,"title":"How to build Quickstart VM","type":"docs"},{"authors":null,"categories":null,"content":"Prerequisites   OS: Windows / Linux / MacOS\n   VirtualBox 6.0+: Oracle VM VirtualBox, is a free and open-source virtual machine.\n   Ohara Quickstart VM image: An OVA (Open Virtual Appliance) file, a pre-prepared virtual machine image for Ohara quickstart. You can download the image file (.ova) from the release page.\n     Please download the VirtualBox from here, and reference this article on how to install it on your machine.    You might have noticed there\u0026rsquo;s another jar file listed in the screenshot: \u0026ldquo;ohara-it-stream.jar\u0026rdquo;. It\u0026rsquo;s a stream jar that could be used later in our tutorial where we walks you through how to use our UI. And download it if you would like to follow along with our tutorial later on.   Installation Import VM and setup network adapter You can use VirtualBox user interface to import the Ohara Quickstart VM (ova file): Main menu -\u0026gt; File -\u0026gt; Import Appliance\nQuickstart VM requires a Host-only network adapter to be configured so that you can connect from the host machine to guest machine (Quickstart VM).\n Quickstart VM uses network adapter vboxnet0 with DHCP Server enabled as default Host-only adapter, if there is already a vboxnet0 adapter in your VirtualBox, you can just skip this step.   for Mac/Linux   Create new network adapter \u0026mdash; Click Tools and then click Create, ensure that the DHCP Enable option is checked.     Setting network adapter \u0026mdash; Select the imported ohara-quickstart-x.x.x VM, click Setting, then click Network, and click Adapter2, select Host-only Adapter, and select the newly added network card.     for Windows   Create new network adapter \u0026mdash; Click Global Tools and then click Create, ensure that the DHCP Enable option is checked.     Setting network adapter \u0026mdash; Select the imported ohara-quickstart-x.x.x VM, click Setting, click Network, click Adapter2, select Host-only Adapter, and select the newly added network card.     Install Ohara Once the Quickstart VM is imported and the network adapter is configured, you can press the Start button to start Quickstart VM and then use the following username and password to log into the system:\n Username: ohara Password: oharastream  The installation will be starting automatically if this is your first time log in to the system. This step will take some time to complete as it needs to download all Ohara docker images.\n    After the installation is completed, you should see something like the following:\n\u0026gt; Start ohara-configurator... 74e0a19a063ce665a0bafba215827d6edd47b9433efdf26af9880d0b4f5e3737 \u0026gt; Start ohara-manager... ecc6e2845f55b52c6a2ec4b2a203d249f117394cbc16c5387aa067ee5d02a096 \u0026gt; Ohara ready on http://192.168.56.102:5050 ohara@ohara-vm:~$  As we can see here, the VM\u0026rsquo;s IP address is 192.168.56.102 (this address will be varied depending on your VirtualBox network settings). We can then open the browser and enter this URL in browser\u0026rsquo;s address bar http://192.168.56.102:5050 to open Ohara Manager (Ohara\u0026rsquo;s UI, we will introduce it in the following section).\n After shutting down your VM, the docker containers will be deleted and restarted on your next login   Terminology Before jumping into the UI and create our very first workspace and pipeline. Let\u0026rsquo;s get to know some of the terms that we will be using throughout this guide.\n  Manager\nManager is the user interface of Ohara (UI). It provides a friendly user interface allowing user to design their data pipeline without even touching a single line of code.\n  Node\nNode is the basic unit of running service. It can be either a physical server or virtual machine.\n  Workspace\nWorkspace contains multiple OharaStream services including: Zookeepers, Brokers and Workers. And pipelines are services that run in a workspace.\n  Pipeline\nPipeline allows you to define your data stream by utilizing Connector to connect to external storage systems, as well as a Stream to customize data transformation and stream processing.\n  Connector\nConnector connects to external storage systems like Databases, HDFS or FTP. It has two types \u0026mdash; source connector and sink connector. Source connector is able to pull data from another system and then push the data to topic. By contrast, Sink connector pulls data from topic and then push the data to another system.\n  Stream\nStream is powered by Kafka Streams which provides users a simple way to write their own stream processing application.\n  Topic\nA topic is a place where all the data are written just like a database table where data is stored. It acts like a buffer, the data being pull in from the source connector is stored in the topic and so later can be pulled out again by another component (e.g., sink connectors)\n  UI overview Before we proceed, here is a screenshot of Ohara Manager where we show you each component\u0026rsquo;s name, so you are better prepared for the upcoming tutorial. You can always come back to this overview if you are lost or not sure what we\u0026rsquo;re talking about in the tutorial.\n   We do our best to make our docs as clear as we could, if you think there\u0026rsquo;s still room for improvement. We would love to hear from you: https://github.com/oharastream/ohara/issues   Now, Ohara Manager is up and running, we can use the UI to create our very first pipeline. Here are the steps that we will be going through together:\n Create a workspace Create a pipeline Add pipeline components, this includes:  A FTP source and a sink connector Two topics A stream Create connections between them   Start the pipeline   During the tutorial, we will be using FTP source/sink connectors. And so you will need to prepare your own FTP in order to follow along.   Create a workspace Open Ohara Manager with your browser (http://192.168.56.102:5050) and you should see a dialog showing up right in the middle of your screen:\n  Click on the QUICK CREATE button to open a new dialog     Using the default name: workspace1 and hit the NEXT button     Click on the Select nodes and click on the pencil icon to create a new node.\n        The node info that you need to enter are listed below:\n Hostname: 192.168.56.102 (fill your own hostname here) Port: 22 User: ohara Password: oharastream       The node should be added into the list. Select the node and click on the SAVE button to close the dialog     Now, click the NEXT button to finish selecting nodes     Click on the SUBMIT button to create this workspace.     A new dialog will open where it shows you the creating progress. This usually takes a while to finish. Once it\u0026rsquo;s done, You can close the dialog by clicking on the CLOSE button. And the UI will automatically redirect you to the newly created workspace: Workspace1       You can create more workspace with the plus icon(:heavy_plus_sign:) in the App bar.    Please keep in mind that this is a quick start VM where we run everything on a single node with only 8GB of RAM and 2 CPU cores. We highly recommend you to add more RAM and CPU core to your VM if you plan to create more than one workspace or lots of pipelines, connectors, streams, etc. This is due to when Ohara is running without enough resources, it could be very unstable and causing unexpected errors.   Create a pipeline Create a pipeline is fairly simple:\n On the Navigator, click on the plus icon. In the popup window, enter the name: pipeline1 and click the ADD button    The new pipeline will be added into the workspace and listed in the Navigator. And just like creating workspace, you will be redirected to the pipeline you just created.     Add pipeline components Since workspace and pipeline are both ready. We can now add new components into the pipeline. The pipeline connection we\u0026rsquo;re about to create will look like: FTP source -\u0026gt; topic -\u0026gt; stream -\u0026gt; topic -\u0026gt; FTP sink\nBefore we start, please make sure your FTP service is ready and let\u0026rsquo;s get started!\nDrag and drop new pipeline components FTP source:\n  From the Toolbox (Please refer to the overview image for what is Toolbox if needed)\n  Click on the title \u0026ldquo;Source\u0026rdquo;, the panel will be expanded and display all available source connectors     Drag the \u0026ldquo;FtpSource\u0026rdquo; from the list and drop into the Paper (Don\u0026rsquo;t worry about the position just yet. This can be changed later after it\u0026rsquo;s added into the Paper). A prompt will be asking you about the connector name, let\u0026rsquo;s name it \u0026ldquo;ftpsource\u0026rdquo; and click the ADD button.     The FTP source connector should display in the Paper:     Now, hover over the Ftp source connector, a couple of buttons will show up. These are action buttons, let\u0026rsquo;s take a quick look and see what can we do with them (starting from left to right):    Link: create a new link, once a link is created you can move your mouse and the link will follow along your mouse position. To link to another component, you can hover over it and do a mouse left-click. To cancel the link creation, just click on the blank area within the Paper. Start: start a component. Once the component is properly configured, you can then use this button to start the component. Stop: stop a running or failed component Configure: open the Property dialog and fill out necessary configuration for the component. Delete: delete the selected component.     A Link can also be interacted with. You can remove it by clicking on the \u0026ldquo;x\u0026rdquo; button. And click on any point of the link creates a vertex. The vertex can be moved and tweaked to change its position. You can also delete a vertex by double clicking on it.     Okay, that\u0026rsquo;s enough of these button things. Let\u0026rsquo;s click on the \u0026ldquo;configure\u0026rdquo; icon (a wrench) and fill in the following fields (Note that you need to use your own settings, and create completed, error, input and output directories on your own FTP service). For fields that we did not mention below, the default is used:\n Completed Folder: demo/completed Error Folder: demo/error Hostname of FTP Server: 10.2.0.28 Port of FTP Server: 21 User of FTP Server: ohara Password of FTP Server: oharastream Input Folder: demo/input    Once these settings had been filled out, click on the SAVE CHANGES button.      There are validation rules which will prevent you from submitting the form without filling require fields.   FTP sink:\nJust like FTP source connector, let\u0026rsquo;s drag and drop a FTP sink connector from the Toolbox and name it \u0026ldquo;ftpsink\u0026rdquo;. After it\u0026rsquo;s added in the Paper, click on its \u0026ldquo;configure\u0026rdquo; button. The settings are mostly like FTP source with the only exception: \u0026ldquo;output\u0026rdquo;:\n      Hostname of FTP Server: 10.2.0.28 Port of FTP Server: 21 User of FTP Server: ohara Password of FTP Server: oharastream Output Folder: demo/output  Great! Now we have both source and sink connectors ready. Let\u0026rsquo;s move on to create some topics.\n  Topic:\nIn this tutorial, we need two topics for the pipeline, let\u0026rsquo;s add them from the Toolbox like what we did in the previous steps for FTP connectors:\n  From the Toolbox, click on the title \u0026ldquo;Topic\u0026rdquo; to expand the topic panel.     Click and drag \u0026ldquo;Pipeline Only\u0026rdquo; item from the list and drop it into the Paper to add a new Topic. Unlike source or sink connector, adding a topic doesn\u0026rsquo;t require entering a name, the name will be auto-generated like (T1, T2, T3, etc.)\n  Repeat the above step to create another Topic. You should now have two topics (T1, T2) in your Paper in addition to those FTP connectors:\n    And luckily, there\u0026rsquo;s no need to configure these topics as they\u0026rsquo;re preconfigured and already running after you added.\n In Ohara, topics can either be a \u0026ldquo;Pipeline-only topic\u0026rdquo; or a \u0026ldquo;Shared topic\u0026rdquo;. The pipeline-only topics are topics that only live within a pipeline. And on the other hand, shared topics can be shared and used across different pipelines. For simplicity sake, we only use pipeline-only topics throughout this tutorial.    For quickly creating a new pipeline-only topic, you can also just add a source and a sink (or stream) connector and then link them together with the \u0026ldquo;Link\u0026rdquo; button from the source connector component. The pipeline-only topic will be created automatically.   Stream:\nStream is our last missing piece of the pipeline. Let\u0026rsquo;s add one very quick!\nRemember the stream jar you downloaded along with the quickstart image? It\u0026rsquo;s time to use it:\n Click on the \u0026ldquo;Stream\u0026rdquo; panel on the Toolbox and then click on the \u0026ldquo;Add streams\u0026rdquo; button     It will open workspace settings and redirect you to the stream jars page:     Click on the plus icon to open select file dialog:     The workspace file list are currently empty, let\u0026rsquo;s add a new file by clicking on the upload icon. This will open your OS file system, you can then select the stream jar file you downloaded. The stream class will be loaded and displayed in the list:        Select the stream and click on the SAVE button to close select file dialog     The stream file should now listed in your Stream Jars page. Close this page by clicking on the close button on the upper-right corner     Adding a stream is just like connector and topic, drag the \u0026ldquo;DumbStream\u0026rdquo; item from the Toolbox and drop it into the Paper and give a name \u0026ldquo;stream\u0026rdquo;     Now let\u0026rsquo;s work through the configuration together. Hover over the stream component and click on the \u0026ldquo;Configure\u0026rdquo; button to open the configure dialog Fill out the form with the following settings and click the \u0026ldquo;SAVE CHANGES\u0026rdquo; button:  header name to be filtered: \u0026ldquo;Sex\u0026rdquo; column value to be filtered: \u0026ldquo;M\u0026rdquo; Node name list: 192.168.56.111 (you should use you own IP)       This stream is capable of filtering out columns and values that we specified and then push the new result to a topic. Here we\u0026rsquo;re specifically setting the \u0026ldquo;Sex\u0026rdquo; as the filtered header and \u0026ldquo;M\u0026rdquo; as the filtered value (stands for Man) and so our output data will only \u0026ldquo;include\u0026rdquo; data that contains \u0026ldquo;M\u0026rdquo; value in the \u0026ldquo;Sex\u0026rdquo; column. We will verify the result later in the tutorial.   Everything is ready. We can now create the connection like we mentioned earlier: FTP source -\u0026gt; topic -\u0026gt; stream -\u0026gt; topic -\u0026gt; FTP sink. But before doing so, let\u0026rsquo;s move these components a bit and so we can have more room to work with (You can close the Toolbox by clicking on the close icon on the top right corner):\n  Okay, it\u0026rsquo;s time to create the connection:\n Hover over FTP source connector and click the \u0026ldquo;Link\u0026rdquo; button, and move your mouse to the first topic named \u0026ldquo;T1\u0026rdquo; and click on it. A connection should be created:     Repeat the same step but this time with \u0026ldquo;T1\u0026rdquo; to create a connection from T1 to stream And stream -\u0026gt; T2 then T2 -\u0026gt; FTP sink connector. After you are done, you should have a graph like this (Components position have been tweaked so it\u0026rsquo;s better to see the relation between these components):     You can also create the connection during configuring the connector or stream. For connector, just choose the topic from its topic list. For stream, you will need to choose both \u0026ldquo;from\u0026rdquo; and \u0026ldquo;to\u0026rdquo; topics from the topic list.   Start pipeline components So far so good, let\u0026rsquo;s start all the components simply by clicking on the \u0026ldquo;Start all components\u0026rdquo; button located on the Toolbar menu. If everything goes well you should see that all components\u0026rsquo; icons are now in green just like the following image:\n  Test our new pipeline Let\u0026rsquo;s test this \u0026ldquo;pipeline\u0026rdquo; to see if it\u0026rsquo;s capable of transferring some data. We have prepared a CSV file which looks like this (you can download this file from here):\n  Upload the file to the FTP service\u0026rsquo;s input folder. Wait for a while, the file should be consumed and moved to the output folder. You can verify if the data is properly transferred by using a FTP client to check the file (we\u0026rsquo;re using FileZilla).\n  The output data should be filtered with the result only \u0026ldquo;M\u0026rdquo; (man) data are listed as shown below:\n  Troubleshooting We now provide a few debugging tools that can help you pin down unexpected errors:\nEvent log panel All UI events are recorded, things like API request and response are also stored. You can view all you event log by simply opening up the Event log panel. As you can see in the screenshot, errors are highlighted and have more details that can be viewed when click on each of them.\n       Another thing that is worth mentioning here is the Event log icon will display the error log\u0026rsquo;s count on the icon\n  Developer Tools panel Open the dev tool from the App bar:\n  There are two main functionalities that could be utilized here:\n Topic panel: you can quickly preview the topic data here     Log panel: view all running service\u0026rsquo;s full log    Delete workspace Deleting a workspace is a new feature implemented in 0.10.0, with this feature, you can now delete a workspace with our UI\n All pipelines under the workspace that you\u0026rsquo;re about to delete should be stopped (in other word, everything except topics in the pipeline should have the status \u0026ldquo;stopped\u0026rdquo;, you can do so by going to each pipeline\u0026rsquo;s Toolbar and click on the \u0026ldquo;Stop all components\u0026rdquo; item from the Pipeline actions    From Navigator, click on the workspace name, and click on the \u0026ldquo;Settings\u0026rdquo; item from that dropdown:     In the Settings dialog, scroll to the very bottom of the page, and click \u0026ldquo;Delete this workspace\u0026rdquo;     A confirm dialog will pop up. Enter the workspace name and click on the DELETE button to start deleting. (Note that if you still have some services running in the workspace, you won\u0026rsquo;t able to proceed. You should following the instruction mention in the above to stop all pipelines first)     The deletion is in progress, after the deletion is completed, you will be redirected to home or a default workspace if you have one.    Restart workspace When new changes were made in workspace, the restart is required. Also, if you ever run into issues that cannot be recovered from, you can try to restart the workspace to fix the issue.\nSince delete workspace and restart workspace are almost identical, the following instruction won\u0026rsquo;t include any screenshots as they are already included in the Delete workspace section\n Same with delete a workspace, you will need to make sure all pipelines are stopped before starting:    From Navigator, click on the workspace name, and click on the \u0026ldquo;Settings\u0026rdquo; item from that dropdown. In the Settings dialog, scroll to the very bottom of the page, and click \u0026ldquo;Restart this workspace\u0026rdquo;. A confirm dialog will pop up. Click on the RESTART button to start restarting this workspace. The restarting is in progress, after the restart is completed, you can close the progress dialog and changes you made should applied to the workspace by now.  And if you think you ever encountered a bug, let us know: GitHub Repo.\n","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"1894c26f798abd2ace38080b45324a01","permalink":"https://oharastream.github.io/en/docs/0.11.x/quickstart/quickstartvm/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/quickstart/quickstartvm/","section":"docs","summary":"Prerequisites   OS: Windows / Linux / MacOS\n   VirtualBox 6.0+: Oracle VM VirtualBox, is a free and open-source virtual machine.\n   Ohara Quickstart VM image: An OVA (Open Virtual Appliance) file, a pre-prepared virtual machine image for Ohara quickstart.","tags":null,"title":"Using Quickstart VM","type":"docs"},{"authors":null,"categories":null,"content":"Prerequisites   OS: Windows / Linux / MacOS\n   VirtualBox 6.0+: Oracle VM VirtualBox, is a free and open-source virtual machine.\n   Ohara Quickstart VM image: An OVA (Open Virtual Appliance) file, a pre-prepared virtual machine image for Ohara quickstart. You can download the image file (.ova) from the release page.\n     Please download the VirtualBox from here, and reference this article on how to install it on your machine.    You might have noticed there\u0026rsquo;s another jar file listed in the screenshot: \u0026ldquo;ohara-it-stream.jar\u0026rdquo;. It\u0026rsquo;s a stream jar that could be used later in our tutorial where we walks you through how to use our UI. And download it if you would like to follow along with our tutorial later on.   Installation Import VM and setup network adapter You can use VirtualBox user interface to import the Ohara Quickstart VM (ova file): Main menu -\u0026gt; File -\u0026gt; Import Appliance\nQuickstart VM requires a Host-only network adapter to be configured so that you can connect from the host machine to guest machine (Quickstart VM).\n Quickstart VM uses network adapter vboxnet0 with DHCP Server enabled as default Host-only adapter, if there is already a vboxnet0 adapter in your VirtualBox, you can just skip this step.   for Mac/Linux   Create new network adapter \u0026mdash; Click \u0026ldquo;Tools\u0026rdquo; and then click \u0026ldquo;Create\u0026rdquo;, ensure that the DHCP Enable option is checked.     Setting network adapter \u0026mdash; Select the imported ohara-quickstart-x.x.x VM, click \u0026ldquo;Setting\u0026rdquo;, then click \u0026ldquo;Network\u0026rdquo;, and click \u0026ldquo;Adapter2\u0026rdquo;, select \u0026ldquo;Host-only Adapter\u0026rdquo;, and select the newly added network card.     for Windows   Create new network adapter \u0026mdash; Click \u0026ldquo;Global Tools\u0026rdquo; and then click \u0026ldquo;Create\u0026rdquo;, ensure that the DHCP Enable option is checked.     Setting network adapter \u0026mdash; Select the imported ohara-quickstart-x.x.x VM, click \u0026ldquo;Setting\u0026rdquo;, click \u0026ldquo;Network\u0026rdquo;, click \u0026ldquo;Adapter2\u0026rdquo;, select \u0026ldquo;Host-only Adapter\u0026rdquo;, and select the newly added network card.     Install Ohara Once the Quickstart VM is imported and the network adapter is configured, you can press the Start button to start Quickstart VM and then use the following username and password to log into the system:\n Username: ohara Password: oharastream  The installation will be starting automatically if this is your first time log in to the system. This step will take some time to complete as it needs to download all Ohara docker images.\n    After the installation is completed, you should see something like the following:\n\u0026gt; Start ohara-configurator... 74e0a19a063ce665a0bafba215827d6edd47b9433efdf26af9880d0b4f5e3737 \u0026gt; Start ohara-manager... ecc6e2845f55b52c6a2ec4b2a203d249f117394cbc16c5387aa067ee5d02a096 \u0026gt; Ohara ready on http://192.168.56.102:5050 ohara@ohara-vm:~$  As we can see here, the VM\u0026rsquo;s IP address is 192.168.56.102 (this address will be varied depending on your VirtualBox network settings). We can then open the browser and enter this URL in browser\u0026rsquo;s address bar http://192.168.56.102:5050 to open Ohara Manager (Ohara\u0026rsquo;s UI, we will introduce it in the following section).\n After shutting down your VM, the docker containers will be deleted and restarted on your next login   Terminology Before jumping into the UI and create our very first workspace and pipeline. Let\u0026rsquo;s get to know some of the terms that we will be using throughout this guide.\n  Manager\nManager is the user interface of Ohara (UI). It provides a friendly user interface allowing user to design their data pipeline without even touching a single line of code.\n  Node\nNode is the basic unit of running service. It can be either a physical server or virtual machine.\n  Workspace\nWorkspace contains multiple OharaStream services including: Zookeepers, Brokers and Workers. And pipelines are services that run in a workspace.\n  Pipeline\nPipeline allows you to define your data stream by utilizing \u0026ldquo;Connector\u0026rdquo; to connect to external storage systems, as well as a \u0026ldquo;Stream\u0026rdquo; to customize data transformation and stream processing.\n  Connector\nConnector connects to external storage systems like Databases, HDFS or FTP. It has two types \u0026mdash; source connector and sink connector. Source connector is able to pull data from another system and then push the data to topic. By contrast, Sink connector pulls data from topic and then push the data to another system.\n  Stream\nStream is powered by Kafka Streams which provides users a simple way to write their own stream processing application.\n  Topic\nA topic is a place where all the data are written just like a database table where data is stored. It acts like a buffer, the data being pull in from the source connector is stored in the topic and so later can be pulled out again by another component (e.g., sink connectors)\n  UI overview Before we proceed, here is a screenshot of \u0026ldquo;Ohara Manager\u0026rdquo; where we show you each component\u0026rsquo;s name, so you are better prepared for the upcoming tutorial. You can always come back to this overview if you are lost or not sure what we\u0026rsquo;re talking about in the tutorial.\n   We do our best to make our docs as clear as we could, if you think there\u0026rsquo;s still room for improvement. We would love to hear from you: https://github.com/oharastream/ohara/issues   Now, Ohara Manager is up and running, we can use the UI to create our very first pipeline. Here are the steps that we will be going through together:\n Create a workspace Create a pipeline Add pipeline components, this includes:  A FTP source and a sink connector Two topics A stream Create connections between them   Start the pipeline   During the tutorial, we will be using FTP source/sink connectors. And so you will need to prepare your own FTP in order to follow along.   Create a workspace Open Ohara Manager with your browser of choice (http://192.168.56.102:5050) and you should see a dialog showing up right in the middle of your screen:\n  Click on the \u0026ldquo;QUICK CREATE\u0026rdquo; button to open quick create workspace dialog     Using the default name: workspace1 and hit the \u0026ldquo;NEXT\u0026rdquo; button     Click on the \u0026ldquo;Select nodes\u0026rdquo; and click on the \u0026ldquo;plus\u0026rdquo; icon to create a new node.\n          The node info that you need to enter are listed below:\n Hostname: 192.168.56.102 (replace with your own hostname here) Port: 22 User: ohara Password: oharastream       The node should be added into the list. Select the node and click on the \u0026ldquo;SAVE\u0026rdquo; button to close the dialog     Now, click the \u0026ldquo;NEXT\u0026rdquo; button to finish this step     We won\u0026rsquo;t be using the volumes in this tutorial, so click on the \u0026ldquo;NEXT\u0026rdquo; button to skip this step.     Here you can see a summary of your workspace settings. If everything looks good to you, let\u0026rsquo;s click on the \u0026ldquo;SUBMIT\u0026rdquo; button to create this workspace!     A new dialog with the title \u0026ldquo;Create Workspace\u0026rdquo; will be opened where it shows you the creating progress. This usually takes a while to finish. Once it\u0026rsquo;s done, You can close the dialog by clicking on the \u0026ldquo;CLOSE\u0026rdquo; button. And the UI will redirect you to the newly created workspace: Workspace1       You can create more workspace with the plus icon(:heavy_plus_sign:) in the App bar.    Please keep in mind that since this is a just a quick start VM where we run everything on a single node with only 8GB of RAM and 2 CPU cores. If you plan to create more workspace in this VM or services like pipelines, connectors or stream, etc. We highly recommend you to add more RAM and CPU core to your VM. And when Ohara is running without enough resources, it could be very unstable and causing unexpected errors.   Create a pipeline Create a pipeline is fairly simple:\n On the Navigator, click on the \u0026ldquo;plus\u0026rdquo; icon. In the dialog, enter the name: \u0026ldquo;pipeline1\u0026rdquo; and click the \u0026ldquo;ADD\u0026rdquo; button    The new pipeline will be added right into the workspace and listed in the Navigator. And just like creating workspace, you will be redirected to the pipeline you just created.     Add pipeline components Since workspace and pipeline are both ready. We can now add new components into the pipeline. The pipeline connection we\u0026rsquo;re about to create will look like: FTP source -\u0026gt; topic -\u0026gt; stream -\u0026gt; topic -\u0026gt; FTP sink\nBefore we start, please make sure your FTP service is ready and let\u0026rsquo;s get started!\nDrag and drop new pipeline components FTP source:\n  From the Toolbox (Please refer to the overview image for what Toolbox is if necessary)\n  Click on the title \u0026ldquo;Source\u0026rdquo;, the panel will be expanded and display all available source connectors     Click and drag the \u0026ldquo;FtpSource\u0026rdquo; from the list and drop it into the Paper (Don\u0026rsquo;t worry about the position just yet. This can be changed after it\u0026rsquo;s added). A prompt will be asking you about the connector name, let\u0026rsquo;s name it \u0026ldquo;ftpsource\u0026rdquo; and click the \u0026ldquo;ADD\u0026rdquo; button.     The FTP source connector should display in the Paper:     Now, hover over the Ftp source connector, a couple of buttons will show up. These are action buttons, let\u0026rsquo;s take a quick look to see what we can do with them (starting from left to right):    Link: create a new link, once a link is created you can move your mouse and the link will follow along your mouse position. To link to another component, you can hover over it and do a mouse left-click. To cancel the link creation, just click on the blank area within the Paper. Start: start a component. Once the component is properly configured, you can then use this button to start the component. Stop: stop a running or failed component Configure: open the Property dialog and fill out necessary configuration for the component. Delete: delete the selected component.     A \u0026ldquo;Link\u0026rdquo; can also be interacted with. You can remove it by clicking on the \u0026ldquo;x\u0026rdquo; button. And click on any point of the link creates a vertex. The vertex can be moved and tweaked to change its position. You can also delete a vertex by double clicking on it.     Okay, that\u0026rsquo;s enough of these button things. Let\u0026rsquo;s click on the \u0026ldquo;configure\u0026rdquo; icon (a wrench) and fill in the following fields (Again, you need to use your own settings, and create completed, error, input and output directories on your own FTP service). For fields that we did not mention below, the default is used:\n Completed Folder: demo/completed Error Folder: demo/error Hostname of FTP Server: 10.2.0.28 Port of FTP Server: 21 User of FTP Server: ohara Password of FTP Server: oharastream Input Folder: demo/input    Once these settings had been filled out, click on the \u0026ldquo;SAVE CHANGES\u0026rdquo; button.      There are validation rules which will prevent you from submitting the form without filling required fields.   FTP sink:\nJust like FTP source connector, let\u0026rsquo;s drag and drop a FTP sink connector from the Toolbox and name it \u0026ldquo;ftpsink\u0026rdquo;. After it\u0026rsquo;s added into the Paper, click on its \u0026ldquo;configure\u0026rdquo; button. The settings are mostly like FTP source with the only exception: \u0026ldquo;output\u0026rdquo;:\n      Hostname of FTP Server: 10.2.0.28 Port of FTP Server: 21 User of FTP Server: ohara Password of FTP Server: oharastream Output Folder: demo/output  Great! Now we have both source and sink connectors ready. Let\u0026rsquo;s move on to create some topics.\n  Topic:\nIn this tutorial, we need two topics for the pipeline, let\u0026rsquo;s add them from the Toolbox like what we did in the previous steps for FTP connectors:\n  From the Toolbox, click on the title \u0026ldquo;Topic\u0026rdquo; to expand the topic panel.     Click and drag \u0026ldquo;Pipeline Only\u0026rdquo; item from the list and drop it into the Paper to add a new Topic. Unlike source or sink connector, adding a topic doesn\u0026rsquo;t require entering a name, the name will be auto-generated like (T1, T2, T3, etc.)\n  Repeat the above step to create another Topic. You should now have two topics (T1, T2) in your Paper in addition to those FTP connectors:\n    And luckily, there\u0026rsquo;s no need to configure these topics as they\u0026rsquo;re preconfigured and already running after you added.\n In Ohara, topics can either be a \u0026ldquo;Pipeline-only topic\u0026rdquo; or a \u0026ldquo;Shared topic\u0026rdquo;. The pipeline-only topics are topics that only live within a pipeline. And on the other hand, shared topics can be shared and used across different pipelines. For simplicity sake, we only use pipeline-only topics throughout this tutorial.    For quickly creating a new pipeline-only topic, you can also just add a source and a sink (or stream) connector and then link them together with the \u0026ldquo;Link\u0026rdquo; button from the source connector component. The pipeline-only topic will be created automatically.   Stream:\nStream is our last missing piece of the pipeline. Let\u0026rsquo;s add one very quick!\nRemember the stream jar you downloaded along with the quickstart image? It\u0026rsquo;s time to use it:\n Click on the \u0026ldquo;Stream\u0026rdquo; panel on the Toolbox and then click on the \u0026ldquo;Add streams\u0026rdquo; button     It will open workspace settings and redirect you to the stream jars page:     Click on the \u0026ldquo;plus\u0026rdquo; icon to open Select file dialog:     The workspace file list are empty right now, let\u0026rsquo;s add a new file by clicking on the upload icon. This will open your OS file system, you can then select the stream jar file you downloaded. The stream class will be loaded and displayed in the list:        Select the stream and click on the \u0026ldquo;SAVE\u0026rdquo; button to close select file dialog     The stream file should now listed in your Stream Jars page. Close this page by clicking on the close button on the upper-right corner   The stream file listed in the list should have a green check icon in the valid column (see below screenshot), if it somehow doesn\u0026rsquo;t have that, double check your stream file is the one downloaded from our release page. Or file an issue to our GitHub repository if the issue persists.      Adding a stream is just like connector and topic, drag the \u0026ldquo;DumbStream\u0026rdquo; item from the Toolbox and drop it into the Paper and give a name \u0026ldquo;stream\u0026rdquo;     Now let\u0026rsquo;s work through the configuration together. Hover over the stream component and click on the \u0026ldquo;Configure\u0026rdquo; button to open the configure dialog Fill out the form with the following settings and click the \u0026ldquo;SAVE CHANGES\u0026rdquo; button:  header name to be filtered: \u0026quot;Sex\u0026quot; column value to be filtered: \u0026quot;M\u0026quot; Node name list: 192.168.56.111 (replace with your own)       This stream is capable of filtering out columns and values that we specified and then push the new result to a topic. Here we\u0026rsquo;re specifically setting the \u0026ldquo;Sex\u0026rdquo; as the filtered header and \u0026ldquo;M\u0026rdquo; as the filtered value (M stands for Man) and so our output data will only \u0026ldquo;includes\u0026rdquo; data that contains \u0026ldquo;M\u0026rdquo; value in the \u0026ldquo;Sex\u0026rdquo; column. We will verify the result later.   Everything is ready. We can now create the connection like we mentioned earlier: FTP source -\u0026gt; topic -\u0026gt; stream -\u0026gt; topic -\u0026gt; FTP sink. But before doing so, let\u0026rsquo;s move these components a bit and so we can have more room to work with (You can close the Toolbox by clicking on the close icon on the top right corner):\n  Okay, it\u0026rsquo;s time to create the connection:\n Hover over FTP source connector and click the \u0026ldquo;Link\u0026rdquo; button, and move your mouse to the first topic named \u0026ldquo;T1\u0026rdquo; and click on it. A connection should be created:     Repeat the same step but this time with \u0026ldquo;T1\u0026rdquo; to create a connection from T1 to stream And stream -\u0026gt; T2 then T2 -\u0026gt; FTP sink connector.  After you complete the whole connection, you should have a graph like this (Components position have been tweaked as well so it\u0026rsquo;s easier to see the relation between them):\n   There is another way to create a connection. When configuring a connector, just choose the topic from its topic list. When configuring stream, you will need to choose both \u0026ldquo;from\u0026rdquo; and \u0026ldquo;to\u0026rdquo; topics from the topic list.   Start pipeline components So far so good, let\u0026rsquo;s start all the components simply by clicking on the \u0026ldquo;Start all components\u0026rdquo; button located on the Toolbar menu. If everything goes well you should see that all components\u0026rsquo; icons are now in green just like the following image:\n  Test our new pipeline Let\u0026rsquo;s test this \u0026ldquo;pipeline\u0026rdquo; to see if it\u0026rsquo;s capable of transferring some data. We have prepared a CSV file which looks like this (you can download this file from here):\n  Upload the file to the FTP service\u0026rsquo;s input folder. Wait for a while, the file should be consumed and moved to the output folder. You can verify if the data is properly transferred by using a FTP client to check the file (we\u0026rsquo;re using FileZilla).\n  The output data should be filtered with the result only \u0026ldquo;M\u0026rdquo; (man) data are listed as shown below:\n  Troubleshooting We now provide a few debugging tools that can help you pin down unexpected errors:\nEvent log panel All UI events are recorded, things like API request and response are also stored. You can view all you event log by simply opening up the Event log panel. As you can see in the screenshot, errors are highlighted and have more details that can be viewed when click on each of them.\n       Another thing that is worth mentioning here is the Event log icon will display the error log\u0026rsquo;s count on the icon\n  Developer Tools panel Open the dev tool from the App bar:\n  There are two main functionalities that could be utilized here:\n Topic panel: you can quickly preview the topic data here     Log panel: view all running service\u0026rsquo;s full log    Delete workspace Deleting a workspace is a new feature implemented in 0.10.0, with this feature, you can now delete a workspace with our UI\n All pipelines under the workspace that you\u0026rsquo;re about to delete should be stopped (in other word, everything except topics in the pipeline should have the status \u0026ldquo;stopped\u0026rdquo;, you can do so by going to each pipeline\u0026rsquo;s Toolbar and click on the \u0026ldquo;Stop all components\u0026rdquo; item from the Pipeline actions    From Navigator, click on the workspace name, and click on the \u0026ldquo;Settings\u0026rdquo; item from that dropdown:     In the Settings dialog, scroll to the very bottom of the page, and click \u0026ldquo;Delete this workspace\u0026rdquo;     A confirm dialog will pop up. Enter the workspace name and click on the \u0026ldquo;DELETE\u0026rdquo; button to start deleting. (Note that if you still have some services running in the workspace, you won\u0026rsquo;t able to proceed. You should following the instruction mention in the above to stop all pipelines first)     The deletion is in progress, after the deletion is completed, you will be redirected to home or a default workspace if you have one.    Restart workspace When new changes were made in workspace, the restart is required. Also, if you ever run into issues that cannot be recovered from, you can try to restart the workspace to fix the issue.\nSince delete workspace and restart workspace are almost identical, the following instruction won\u0026rsquo;t include any screenshots as they are already included in the Delete workspace section\n Same with delete a workspace, you will need to make sure all pipelines are stopped before starting:    From Navigator, click on the workspace name, and click on the \u0026ldquo;Settings\u0026rdquo; item from that dropdown. In the Settings dialog, scroll to the very bottom of the page, and click \u0026ldquo;Restart this workspace\u0026rdquo;. A confirm dialog will pop up. Click on the RESTART button to start restarting this workspace. The restarting is in progress, after the restart is completed, you can close the progress dialog and changes you made should applied to the workspace by now.  And if you think you ever encountered a bug, let us know: GitHub Repo.\n","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606714805,"objectID":"328fa01666169ab678daa4a2105ad6c7","permalink":"https://oharastream.github.io/en/docs/master/quickstart/quickstartvm/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/master/quickstart/quickstartvm/","section":"docs","summary":"Prerequisites   OS: Windows / Linux / MacOS\n   VirtualBox 6.0+: Oracle VM VirtualBox, is a free and open-source virtual machine.\n   Ohara Quickstart VM image: An OVA (Open Virtual Appliance) file, a pre-prepared virtual machine image for Ohara quickstart.","tags":null,"title":"Using Quickstart VM","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592978868,"objectID":"9bf99b8b9f35adf2e9d4022210692189","permalink":"https://oharastream.github.io/en/docs/0.10.x/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/docs/0.10.x/example1/","section":"docs","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592978868,"objectID":"43b9dc2e128f74b4770ca1e3c49a61ea","permalink":"https://oharastream.github.io/en/docs/0.10.x/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/docs/0.10.x/example2/","section":"docs","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":" Broker is core of data transmission in ohara. The topic, which is a part our data lake, is hosted by broker cluster. The number of brokers impacts the performance of transferring data and data durability. But it is ok to setup broker cluster in single node when testing. As with zookeeper, broker has many configs also. Ohara still provide default to most configs and then enable user to overwrite them.\nBroker is based on zookeeper, hence you have to create zookeeper cluster first. Noted that a zookeeper cluster can be used by only a broker cluster. It will fail if you try to multi broker cluster on same zookeeper cluster.\nThe properties which can be set by user are shown below.\n name (string) \u0026mdash; cluster name group (string) \u0026mdash; cluster group imageName (string) \u0026mdash; docker image clientPort (int) \u0026mdash; broker client port. jmxPort (int) \u0026mdash; port used by jmx service zookeeperClusterKey (object) \u0026mdash; key of zookeeper cluster used to store metadata of broker cluster  zookeeperClusterKey.group(option(string)) \u0026mdash; the group of zookeeper cluster zookeeperClusterKey.name(string) \u0026mdash; the name of zookeeper cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   nodeNames (array(string)) \u0026mdash; the nodes running the zookeeper process tags (object) \u0026mdash; the user defined parameters  The following information are updated by Ohara.\n aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of broker cluster state (option(string)) \u0026mdash; only started/failed broker has state (RUNNING or DEAD) error (option(string)) \u0026mdash; the error message from a failed broker. If broker is fine or un-started, you won\u0026rsquo;t get this field. lastModified (long) \u0026mdash; last modified this jar time  create a broker cluster POST /v0/brokers\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;], \u0026quot;zookeeperClusterKey\u0026quot;: \u0026quot;zk\u0026quot; }    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903360246, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    list all broker clusters GET /v0/brokers\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903360246, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    update broker cluster properties PUT /v0/brokers/$name?group=$group\n If the required broker (group, name) was not exists, we will try to use this request as POST\n{ \u0026quot;xmx\u0026quot;: 2048 }     Example Response { \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903494681, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    delete a broker properties DELETE /v0/brokers/$name?group=$group\nYou cannot delete properties of an non-stopped broker cluster. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 204 NoContent     It is ok to delete an nonexistent broker cluster, and the response is 204 NoContent.   get a broker cluster GET /v0/brokers/$name?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903494681, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    start a broker cluster PUT /v0/brokers/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     You should use Get broker cluster to fetch up-to-date status   stop a broker cluster Gracefully stopping a running broker cluster. It is disallowed to stop a broker cluster used by a running worker cluster.\nPUT /v0/brokers/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) \u0026mdash; true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted     You should use Get broker cluster to fetch up-to-date status   add a new node to a running broker cluster PUT /v0/brokers/$name/$nodeName?group=$group\nIf you want to extend a running broker cluster, you can add a node to share the heavy loading of a running broker cluster. However, the balance is not triggered at once.\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     Although it\u0026rsquo;s a rare case, you should not use the \u0026ldquo;API keyword\u0026rdquo; as the nodeName. For example, the following APIs are invalid and will trigger different behavior!\n /v0/brokers/$name/start /v0/brokers/$name/stop    remove a node from a running broker cluster DELETE /v0/brokers/$name/$nodeName?group=$group\nIf your budget is limited, you can decrease the number of nodes running broker cluster. BUT, removing a node from a running broker cluster invoke a lot of data move. The loading may burn out the remaining nodes.\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 204 NoContent     It is ok to delete an nonexistent broker node, and the response is 204 NoContent.   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"1365629cde157b1b2b2f07b770810274","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/brokers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/brokers/","section":"docs","summary":"Broker is core of data transmission in ohara. The topic, which is a part our data lake, is hosted by broker cluster. The number of brokers impacts the performance of transferring data and data durability.","tags":null,"title":"Broker","type":"docs"},{"authors":null,"categories":null,"content":" Broker is core of data transmission in ohara. The topic, which is a part our data lake, is hosted by broker cluster. The number of brokers impacts the performance of transferring data and data durability. But it is ok to setup broker cluster in single node when testing. As with zookeeper, broker has many configs also. Ohara still provide default to most configs and then enable user to overwrite them.\nBroker is based on zookeeper, hence you have to create zookeeper cluster first. Noted that a zookeeper cluster can be used by only a broker cluster. It will fail if you try to multi broker cluster on same zookeeper cluster.\nThe properties which can be set by user are shown below.\n name (string) \u0026mdash; cluster name group (string) \u0026mdash; cluster group imageName (string) \u0026mdash; docker image clientPort (int) \u0026mdash; broker client port. jmxPort (int) \u0026mdash; port used by jmx service zookeeperClusterKey (object) \u0026mdash; key of zookeeper cluster used to store metadata of broker cluster  zookeeperClusterKey.group(option(string)) \u0026mdash; the group of zookeeper cluster zookeeperClusterKey.name(string) \u0026mdash; the name of zookeeper cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   nodeNames (array(string)) \u0026mdash; the nodes running the zookeeper process tags (object) \u0026mdash; the user defined parameters  The following information are updated by Ohara.\n aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of broker cluster state (option(string)) \u0026mdash; only started/failed broker has state (RUNNING or DEAD) error (option(string)) \u0026mdash; the error message from a failed broker. If broker is fine or un-started, you won\u0026rsquo;t get this field. lastModified (long) \u0026mdash; last modified this jar time  create a broker cluster POST /v0/brokers\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;], \u0026quot;zookeeperClusterKey\u0026quot;: \u0026quot;zk\u0026quot; }    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903360246, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    list all broker clusters GET /v0/brokers\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903360246, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    update broker cluster properties PUT /v0/brokers/$name?group=$group\n If the required broker (group, name) was not exists, we will try to use this request as POST\n{ \u0026quot;xmx\u0026quot;: 2048 }     Example Response { \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903494681, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    delete a broker properties DELETE /v0/brokers/$name?group=$group\nYou cannot delete properties of an non-stopped broker cluster. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 204 NoContent     It is ok to delete an nonexistent broker cluster, and the response is 204 NoContent.   get a broker cluster GET /v0/brokers/$name?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot;, \u0026quot;offsets.topic.replication.factor\u0026quot;: 1, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;num.partitions\u0026quot;: 1, \u0026quot;lastModified\u0026quot;: 1578903494681, \u0026quot;num.network.threads\u0026quot;: 1, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;log.dirs\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 42020, \u0026quot;num.io.threads\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 39614, \u0026quot;zookeeperClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot; }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    start a broker cluster PUT /v0/brokers/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     You should use Get broker cluster to fetch up-to-date status   stop a broker cluster Gracefully stopping a running broker cluster. It is disallowed to stop a broker cluster used by a running worker cluster.\nPUT /v0/brokers/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) \u0026mdash; true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted     You should use Get broker cluster to fetch up-to-date status   add a new node to a running broker cluster PUT /v0/brokers/$name/$nodeName?group=$group\nIf you want to extend a running broker cluster, you can add a node to share the heavy loading of a running broker cluster. However, the balance is not triggered at once.\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     Although it\u0026rsquo;s a rare case, you should not use the \u0026ldquo;API keyword\u0026rdquo; as the nodeName. For example, the following APIs are invalid and will trigger different behavior!\n /v0/brokers/$name/start /v0/brokers/$name/stop    remove a node from a running broker cluster DELETE /v0/brokers/$name/$nodeName?group=$group\nIf your budget is limited, you can decrease the number of nodes running broker cluster. BUT, removing a node from a running broker cluster invoke a lot of data move. The loading may burn out the remaining nodes.\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 204 NoContent     It is ok to delete an nonexistent broker node, and the response is 204 NoContent.   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"c2970f3b31c71ecea6518b3701f78302","permalink":"https://oharastream.github.io/en/docs/master/rest-api/brokers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/brokers/","section":"docs","summary":"Broker is core of data transmission in ohara. The topic, which is a part our data lake, is hosted by broker cluster. The number of brokers impacts the performance of transferring data and data durability.","tags":null,"title":"Broker","type":"docs"},{"authors":null,"categories":null,"content":"Connector is core of application in Ohara pipeline. Connector has two type - source and sink. Source connector pulls data from another system and then push to topic. By contrast, Sink connector pulls data from topic and then push to another system. In order to use connector in pipeline, you have to set up a connector settings in Ohara and then add it to pipeline. Of course, the connector settings must belong to a existent connector in target worker cluster. By default, worker cluster hosts only the official connectors. If you have more custom requirement for connector, please follow custom connector guideline to write your connector.\nApart from custom settings, common settings are required by all connectors. The common settings are shown below.\n group (string) \u0026mdash; the value of group is always \u0026ldquo;default\u0026rdquo;. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; name (string) \u0026mdash; the name of this connector. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; connector.class (class) \u0026mdash; class name of connector implementation topicKeys(array(object)) \u0026mdash; the source topics or target topics for this connector columns (array(object)) \u0026mdash; the schema of data for this connector  columns[i].name (string) \u0026mdash; origin name of column columns[i].newName (string) \u0026mdash; new name of column columns[i].dataType (string) \u0026mdash; the type used to convert data columns[i].order (int) \u0026mdash; the order of this column   numberOfTasks (int) \u0026mdash; the number of tasks workerClusterKey (Object) \u0026mdash; target worker cluster.  workerClusterKey.group (option(string)) \u0026mdash; the group of cluster workerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   tags (object) - the extra description to this object  The following information are updated by Ohara:\n group (string) \u0026mdash; connector\u0026rsquo;s group name (string) \u0026mdash; connector\u0026rsquo;s name lastModified (long) \u0026mdash; the last time to update this connector state (option(string)) \u0026mdash; the state of a started connector aliveNodes (Set(string)) \u0026mdash; the nodes hosting this connector error (option(string)) \u0026mdash; the error message from a failed connector. If the connector is fine or un-started, you won\u0026rsquo;t get this field. tasksStatus (Array(object)) \u0026mdash; the tasks status of this connector  tasksStatus[i].state (string) \u0026mdash; the state of a started task. tasksStatus[i].nodeName (string) \u0026mdash; the node hosting this task tasksStatus[i].error (option(string)) \u0026mdash; the error message from a failed task. If the task is fine or un-started, you won\u0026rsquo;t get this field. tasksStatus[i].coordinator (boolean) \u0026mdash; true if this status is coordinator. otherwise, false    nodeMetrics (object) \u0026mdash; the metrics from a running connector  meters (array(object)) \u0026mdash; the metrics in meter type  meters[i].name (string) \u0026mdash; the number of this meter (normally, it is unique) meters[i].value (double) \u0026mdash; the value in double meters[i].valueInPerSec (double) \u0026mdash; the average value in per second meters[i].unit (string) \u0026mdash; the unit of value meters[i].document (string) \u0026mdash; human-readable description to this meter meters[i].queryTime (Long) \u0026mdash; the time we query this meter from remote nodes meters[i].startTime (Long) \u0026mdash; the time to start this meter (not all services offer this record) meters[i].lastModified (Long) \u0026mdash; the time of modifying metrics      The following keys are internal and protected so you can\u0026rsquo;t define them in creating/updating connector.\n connectorKey \u0026mdash; It points to the really (group, name) for the connector running in kafka. topics \u0026mdash; It points to the really topic names in kafka for the connector running in kafka.  create the settings of connector POST /v0/connectors\nIt is ok to lack some common settings when creating settings for a connector. However, it is illegal to start a connector with incomplete settings. For example, storing the settings consisting of only connector.name is ok. But stating a connector with above incomplete settings will introduce a error.\n  Example Request\n{ \u0026quot;name\u0026quot;:\u0026quot;perf\u0026quot;, \u0026quot;topicKeys\u0026quot;: [\u0026quot;t0\u0026quot;], \u0026quot;workerClusterKey\u0026quot;: \u0026quot;wk\u0026quot;, \u0026quot;connector.class\u0026quot;:\u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot; }    Example Response\n{ \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t0\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577282907085, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update the settings of connector PUT /v0/connectors/${name}?group=${group}\n you cannot update a non-stopped connector.     Example Request\n{ \u0026quot;topicKeys\u0026quot;: [ \u0026quot;t1\u0026quot; ] }    Example Response\n{ \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577283010533, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list information of all connectors GET /v0/connectors\nthe accepted query keys are listed below.\n group name lastModified tags tag - this field is similar to tags but it addresses the \u0026ldquo;contain\u0026rdquo; behavior. key   Example Response [ { \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577283010533, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a connector DELETE /v0/connectors/${name}?group=${group}\nDeleting the settings used by a running connector is not allowed. You should stop connector before deleting it.\n  Example Response\n204 NoContent     It is ok to delete an jar from an nonexistent connector or a running connector, and the response is 204 NoContent.   get information of connector GET /v0/connectors/${name}?group=${group}\n Example Response { \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577283010533, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    start a connector PUT /v0/connectors/${name}/start?group=${group}\nOhara will send a start request to specific worker cluster to start the connector with stored settings, and then make a response to called. The connector is executed async so the connector may be still in starting after you retrieve the response. You can send GET request to see the state of connector. This request is idempotent so it is safe to retry this command repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   stop a connector PUT /v0/connectors/${name}/stop?group=${group}\nOhara will send a stop request to specific worker cluster to stop the connector. The stopped connector will be removed from worker cluster. The settings of connector is still kept by Ohara so you can start the connector with same settings again in the future. If you want to delete the connector totally, you should stop the connector and then delete it. This request is idempotent so it is safe to send this request repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   pause a connector PUT /v0/connectors/${name}/pause?group=${group}\nPausing a connector is to disable connector to pull/push data from/to source/sink. The connector is still alive in kafka. This request is idempotent so it is safe to send this request repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   resume a connector PUT /v0/connectors/${name}/resume?group=${group}\nResuming a connector is to enable connector to pull/push data from/to source/sink. This request is idempotent so it is safe to retry this command repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"0c0228e94b8436af86f10764385538d2","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/connectors/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/connectors/","section":"docs","summary":"Connector is core of application in Ohara pipeline. Connector has two type - source and sink. Source connector pulls data from another system and then push to topic. By contrast, Sink connector pulls data from topic and then push to another system.","tags":null,"title":"Connector","type":"docs"},{"authors":null,"categories":null,"content":"Connector is core of application in Ohara pipeline. Connector has two type - source and sink. Source connector pulls data from another system and then push to topic. By contrast, Sink connector pulls data from topic and then push to another system. In order to use connector in pipeline, you have to set up a connector settings in Ohara and then add it to pipeline. Of course, the connector settings must belong to a existent connector in target worker cluster. By default, worker cluster hosts only the official connectors. If you have more custom requirement for connector, please follow custom connector guideline to write your connector.\nApart from custom settings, common settings are required by all connectors. The common settings are shown below.\n group (string) \u0026mdash; the value of group is always \u0026ldquo;default\u0026rdquo;. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; name (string) \u0026mdash; the name of this connector. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; connector.class (class) \u0026mdash; class name of connector implementation topicKeys(array(object)) \u0026mdash; the source topics or target topics for this connector columns (array(object)) \u0026mdash; the schema of data for this connector  columns[i].name (string) \u0026mdash; origin name of column columns[i].newName (string) \u0026mdash; new name of column columns[i].dataType (string) \u0026mdash; the type used to convert data columns[i].order (int) \u0026mdash; the order of this column   numberOfTasks (int) \u0026mdash; the number of tasks workerClusterKey (Object) \u0026mdash; target worker cluster.  workerClusterKey.group (option(string)) \u0026mdash; the group of cluster workerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   tags (object) - the extra description to this object  The following information are updated by Ohara:\n group (string) \u0026mdash; connector\u0026rsquo;s group name (string) \u0026mdash; connector\u0026rsquo;s name lastModified (long) \u0026mdash; the last time to update this connector state (option(string)) \u0026mdash; the state of a started connector aliveNodes (Set(string)) \u0026mdash; the nodes hosting this connector error (option(string)) \u0026mdash; the error message from a failed connector. If the connector is fine or un-started, you won\u0026rsquo;t get this field. tasksStatus (Array(object)) \u0026mdash; the tasks status of this connector  tasksStatus[i].state (string) \u0026mdash; the state of a started task. tasksStatus[i].nodeName (string) \u0026mdash; the node hosting this task tasksStatus[i].error (option(string)) \u0026mdash; the error message from a failed task. If the task is fine or un-started, you won\u0026rsquo;t get this field. tasksStatus[i].coordinator (boolean) \u0026mdash; true if this status is coordinator. otherwise, false    nodeMetrics (object) \u0026mdash; the metrics from a running connector  meters (array(object)) \u0026mdash; the metrics in meter type  meters[i].name (string) \u0026mdash; the number of this meter (normally, it is unique) meters[i].value (double) \u0026mdash; the value in double meters[i].valueInPerSec (double) \u0026mdash; the average value in per second meters[i].unit (string) \u0026mdash; the unit of value meters[i].document (string) \u0026mdash; human-readable description to this meter meters[i].queryTime (Long) \u0026mdash; the time we query this meter from remote nodes meters[i].startTime (Long) \u0026mdash; the time to start this meter (not all services offer this record) meters[i].lastModified (Long) \u0026mdash; the time of modifying metrics      The following keys are internal and protected so you can\u0026rsquo;t define them in creating/updating connector.\n connectorKey \u0026mdash; It points to the really (group, name) for the connector running in kafka. topics \u0026mdash; It points to the really topic names in kafka for the connector running in kafka.  create the settings of connector POST /v0/connectors\nIt is ok to lack some common settings when creating settings for a connector. However, it is illegal to start a connector with incomplete settings. For example, storing the settings consisting of only connector.name is ok. But stating a connector with above incomplete settings will introduce a error.\n  Example Request\n{ \u0026quot;name\u0026quot;:\u0026quot;perf\u0026quot;, \u0026quot;topicKeys\u0026quot;: [\u0026quot;t0\u0026quot;], \u0026quot;workerClusterKey\u0026quot;: \u0026quot;wk\u0026quot;, \u0026quot;connector.class\u0026quot;:\u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot; }    Example Response\n{ \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t0\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577282907085, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update the settings of connector PUT /v0/connectors/${name}?group=${group}\n you cannot update a non-stopped connector.     Example Request\n{ \u0026quot;topicKeys\u0026quot;: [ \u0026quot;t1\u0026quot; ] }    Example Response\n{ \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577283010533, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list information of all connectors GET /v0/connectors\nthe accepted query keys are listed below.\n group name lastModified tags tag - this field is similar to tags but it addresses the \u0026ldquo;contain\u0026rdquo; behavior. key   Example Response [ { \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577283010533, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a connector DELETE /v0/connectors/${name}?group=${group}\nDeleting the settings used by a running connector is not allowed. You should stop connector before deleting it.\n  Example Response\n204 NoContent     It is ok to delete an jar from an nonexistent connector or a running connector, and the response is 204 NoContent.   get information of connector GET /v0/connectors/${name}?group=${group}\n Example Response { \u0026quot;header.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;check.rule\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;key.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;lastModified\u0026quot;: 1577283010533, \u0026quot;tags\u0026quot;: {}, \u0026quot;value.converter\u0026quot;: \u0026quot;org.apache.kafka.connect.converters.ByteArrayConverter\u0026quot;, \u0026quot;perf.cell.length\u0026quot;: 10, \u0026quot;tasks.max\u0026quot;: 1, \u0026quot;perf.batch\u0026quot;: 10, \u0026quot;perf.frequency\u0026quot;: \u0026quot;1000 milliseconds\u0026quot;, \u0026quot;connector.class\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;baafe4a3d875e5e5028b686c4f74f26cfd8b1b66\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;columns\u0026quot;: [], \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;number of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;size (in bytes) of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 8.19825E+8, \u0026quot;valueInPerSec\u0026quot;: 19094561.546523817 }, { \u0026quot;document\u0026quot;: \u0026quot;size of ignored messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068827510, \u0026quot;name\u0026quot;: \u0026quot;ignored.message.size\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827510, \u0026quot;unit\u0026quot;: \u0026quot;bytes\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 }, { \u0026quot;document\u0026quot;: \u0026quot;number of messages\u0026quot;, \u0026quot;lastModified\u0026quot;: 1585068870445, \u0026quot;name\u0026quot;: \u0026quot;message.number\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585068870341, \u0026quot;startTime\u0026quot;: 1585068827508, \u0026quot;unit\u0026quot;: \u0026quot;messages\u0026quot;, \u0026quot;value\u0026quot;: 1275000.0, \u0026quot;valueInPerSec\u0026quot;: 29694.66893355381 } ] } }, \u0026quot;workerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot; }, \u0026quot;tasksStatus\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    start a connector PUT /v0/connectors/${name}/start?group=${group}\nOhara will send a start request to specific worker cluster to start the connector with stored settings, and then make a response to called. The connector is executed async so the connector may be still in starting after you retrieve the response. You can send GET request to see the state of connector. This request is idempotent so it is safe to retry this command repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   stop a connector PUT /v0/connectors/${name}/stop?group=${group}\nOhara will send a stop request to specific worker cluster to stop the connector. The stopped connector will be removed from worker cluster. The settings of connector is still kept by Ohara so you can start the connector with same settings again in the future. If you want to delete the connector totally, you should stop the connector and then delete it. This request is idempotent so it is safe to send this request repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   pause a connector PUT /v0/connectors/${name}/pause?group=${group}\nPausing a connector is to disable connector to pull/push data from/to source/sink. The connector is still alive in kafka. This request is idempotent so it is safe to send this request repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   resume a connector PUT /v0/connectors/${name}/resume?group=${group}\nResuming a connector is to enable connector to pull/push data from/to source/sink. This request is idempotent so it is safe to retry this command repeatedly.\n Example Response 202 Accepted     You should use Get Connector info to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"3fc60dde3ffb6853438ac408e5f5f002","permalink":"https://oharastream.github.io/en/docs/master/rest-api/connectors/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/connectors/","section":"docs","summary":"Connector is core of application in Ohara pipeline. Connector has two type - source and sink. Source connector pulls data from another system and then push to topic. By contrast, Sink connector pulls data from topic and then push to another system.","tags":null,"title":"Connector","type":"docs"},{"authors":null,"categories":null,"content":"Each processes managed by Ohara is based on docker container. In most cases, user don\u0026rsquo;t need to know the details of containers since the management of containers is on Ohara\u0026rsquo;s shoulder. However, Ohara understand that we all have curious brain so Ohara supports to display the container\u0026rsquo;s details of a running cluster. Noted that the context may be changed between different release of Ohara. And the distinct implementations of container manager possibly provide different context of containers.\nretrieve the container details of a running cluster GET /v0/containers/$clusterName?group=$clusterGroup\n Example Response  The cluster name may be mapped to different services (of course, it would be better to avoid using same name on different services), hence, the returned JSON is in array type. The details of elements are shown below.\n  clusterKey (Object) \u0026mdash; cluster key\n  clusterType (string) \u0026mdash; cluster type\n  containers (array(object)) \u0026mdash; the container in this cluster\n environments (object) \u0026mdash; the environment variables of container name (string) \u0026mdash; the name of container hostname (string) \u0026mdash; hostname of container size (string) \u0026mdash; the disk size used by this container state (option(string)) \u0026mdash; the state of container portMappings (array(object)) \u0026mdash; the exported ports of this container  portMappings[i].hostIp (string) \u0026mdash; the network interface of container host portMappings[i].hostPort (int) \u0026mdash; host port portMappings[i].containerPort (int) \u0026mdash; container port   nodeName (string) \u0026mdash; the node which host this container imageName (string) \u0026mdash; the image used to create this container id (string) \u0026mdash; container id kind (string) \u0026mdash; Ohara supports the DOCKER and K8S mode  [ { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot; }, \u0026quot;clusterType\u0026quot;: \u0026quot;worker\u0026quot;, \u0026quot;containers\u0026quot;: [ { \u0026quot;environments\u0026quot;: { \u0026quot;KAFKA_JMX_OPTS\u0026quot;: \u0026quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=41484 -Dcom.sun.management.jmxremote.rmi.port=41484 -Djava.rmi.server.hostname=ohara-release-test-00\u0026quot;, \u0026quot;KAFKA_HEAP_OPTS\u0026quot;: \u0026quot;-Xms1024M -Xmx1024M\u0026quot;, \u0026quot;WORKER_PLUGIN_URLS\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;WORKER_SHARED_JAR_URLS\u0026quot;: \u0026quot;\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;default-wk00-worker-3b8c71a\u0026quot;, \u0026quot;hostname\u0026quot;: \u0026quot;wk00-worker-5739cbd\u0026quot;, \u0026quot;size\u0026quot;: -1, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;portMappings\u0026quot;: [ { \u0026quot;hostIp\u0026quot;: \u0026quot;10.2.10.30\u0026quot;, \u0026quot;hostPort\u0026quot;: 36789, \u0026quot;containerPort\u0026quot;: 36789 }, { \u0026quot;hostIp\u0026quot;: \u0026quot;10.2.10.30\u0026quot;, \u0026quot;hostPort\u0026quot;: 41484, \u0026quot;containerPort\u0026quot;: 41484 } ], \u0026quot;nodeName\u0026quot;: \u0026quot;ohara-release-test-00\u0026quot;, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;2a3b3872-35ab-11ea-8a18-a29736512df3\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;K8S\u0026quot; } ] } ]    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"39c05dae853b13efcdf50a1e587f8b9f","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/containers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/containers/","section":"docs","summary":"Each processes managed by Ohara is based on docker container. In most cases, user don\u0026rsquo;t need to know the details of containers since the management of containers is on Ohara\u0026rsquo;s shoulder.","tags":null,"title":"Container","type":"docs"},{"authors":null,"categories":null,"content":"Each processes managed by Ohara is based on docker container. In most cases, user don\u0026rsquo;t need to know the details of containers since the management of containers is on Ohara\u0026rsquo;s shoulder. However, Ohara understand that we all have curious brain so Ohara supports to display the container\u0026rsquo;s details of a running cluster. Noted that the context may be changed between different release of Ohara. And the distinct implementations of container manager possibly provide different context of containers.\nretrieve the container details of a running cluster GET /v0/containers/$clusterName?group=$clusterGroup\n Example Response  The cluster name may be mapped to different services (of course, it would be better to avoid using same name on different services), hence, the returned JSON is in array type. The details of elements are shown below.\n  clusterKey (Object) \u0026mdash; cluster key\n  clusterType (string) \u0026mdash; cluster type\n  containers (array(object)) \u0026mdash; the container in this cluster\n environments (object) \u0026mdash; the environment variables of container name (string) \u0026mdash; the name of container hostname (string) \u0026mdash; hostname of container size (string) \u0026mdash; the disk size used by this container state (option(string)) \u0026mdash; the state of container portMappings (array(object)) \u0026mdash; the exported ports of this container  portMappings[i].hostIp (string) \u0026mdash; the network interface of container host portMappings[i].hostPort (int) \u0026mdash; host port portMappings[i].containerPort (int) \u0026mdash; container port   nodeName (string) \u0026mdash; the node which host this container imageName (string) \u0026mdash; the image used to create this container id (string) \u0026mdash; container id kind (string) \u0026mdash; Ohara supports the DOCKER and K8S mode  [ { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot; }, \u0026quot;clusterType\u0026quot;: \u0026quot;worker\u0026quot;, \u0026quot;containers\u0026quot;: [ { \u0026quot;environments\u0026quot;: { \u0026quot;KAFKA_JMX_OPTS\u0026quot;: \u0026quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=41484 -Dcom.sun.management.jmxremote.rmi.port=41484 -Djava.rmi.server.hostname=ohara-release-test-00\u0026quot;, \u0026quot;KAFKA_HEAP_OPTS\u0026quot;: \u0026quot;-Xms1024M -Xmx1024M\u0026quot;, \u0026quot;WORKER_PLUGIN_URLS\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;WORKER_SHARED_JAR_URLS\u0026quot;: \u0026quot;\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;default-wk00-worker-3b8c71a\u0026quot;, \u0026quot;hostname\u0026quot;: \u0026quot;wk00-worker-5739cbd\u0026quot;, \u0026quot;size\u0026quot;: -1, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;portMappings\u0026quot;: [ { \u0026quot;hostIp\u0026quot;: \u0026quot;10.2.10.30\u0026quot;, \u0026quot;hostPort\u0026quot;: 36789, \u0026quot;containerPort\u0026quot;: 36789 }, { \u0026quot;hostIp\u0026quot;: \u0026quot;10.2.10.30\u0026quot;, \u0026quot;hostPort\u0026quot;: 41484, \u0026quot;containerPort\u0026quot;: 41484 } ], \u0026quot;nodeName\u0026quot;: \u0026quot;ohara-release-test-00\u0026quot;, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;2a3b3872-35ab-11ea-8a18-a29736512df3\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;K8S\u0026quot; } ] } ]    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"f685e094f6e1ab88bed5c043573fddf8","permalink":"https://oharastream.github.io/en/docs/master/rest-api/containers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/containers/","section":"docs","summary":"Each processes managed by Ohara is based on docker container. In most cases, user don\u0026rsquo;t need to know the details of containers since the management of containers is on Ohara\u0026rsquo;s shoulder.","tags":null,"title":"Container","type":"docs"},{"authors":null,"categories":null,"content":"Ohara custom connector is based on kafka connector. It offers a platform that enables you to define some simple actions to connect topic to any other system. You don\u0026rsquo;t need to worry the application availability, data durability, or distribution anymore. All you have to do is to write your custom connector, which can have only the pull()/push() method, and then compile your code to a jar file. After uploading your jar to Ohara, you are able to deploy your connector on the [worker cluster]/en/docs/0.11.x/rest-api/workers/#rest-workers-create. By leveraging Ohara connector framework, apart from the availability, scalability, and durability, you can also monitor your connector for logs and metrics.\nThe following sections start to explain how to write a good connector on Ohara. You don\u0026rsquo;t need to read it through if you are familiar with kafka connector. However, Ohara connector has some improvements which are not in kafka connector so it has worth of taking a look at metrics and setting definitions\n Connector Overview Ohara connector is composed of source connector and sink connector . source connector is used to pull data from external system to topic. By contrast, sink connector is used to pull data from topic to external system. A complete connector consists of SourceConnector / SinkConnector and SourceTask / SinkTask. Worker cluster picks up a node to host your source/sink connector and then distributes the source/sink tasks across cluster.\nYou must include Ohara jars before starting to write your custom connector. Please include both of ohara-common and ohara-kafka in your dependencies. The ohara-common contains many helper methods and common data used in whole Ohara. The ohara-kafka offers a lot of beautiful APIs to help you to access kafka and design custom connector.\nrepositories { maven { url \u0026quot;https://dl.bintray.com/oharastream/ohara\u0026quot; } } implementation \u0026quot;oharastream.ohara:ohara-common:0.11.0-SNAPSHOT\u0026quot; implementation \u0026quot;oharastream.ohara:ohara-kafka:0.11.0-SNAPSHOT\u0026quot;   The releases page shows the available version of Ohara    Data Model Ohara has defined a table structure data in code base. We call it row. A row is comprised of multiple cells. Each cell has its name, value and tags. The value in cell is a generic type which accepts any serializable type of value. Following is an example that shows you how to convert a csv data to Ohara row.\nc0,c1,c2 v0,v1,v2  The above data can be converted to Ohara row by following code.\nimport oharastream.ohara.common.data.Row; import oharastream.ohara.common.data.Cell; class ExampleOfRow { public static void main(String[] args) { Row row = Row.of( Cell.of(\u0026quot;c0\u0026quot;, \u0026quot;v0\u0026quot;), Cell.of(\u0026quot;c1\u0026quot;, \u0026quot;v1\u0026quot;), Cell.of(\u0026quot;c2\u0026quot;, \u0026quot;v2\u0026quot;) ); } }  c0,c1,c2 v0,,v2  The above data can be converted to Ohara row by following code.\nimport oharastream.ohara.common.data.Row; import oharastream.ohara.common.data.Cell; class ExampleOfRow { public static void main(String[] args) { Row row = Row.of( Cell.of(\u0026quot;c0\u0026quot;, \u0026quot;v0\u0026quot;), Cell.of(\u0026quot;c2\u0026quot;, \u0026quot;v2\u0026quot;) ); } }  Don\u0026rsquo;t worry about the serialization. Ohara offers default serializations for following data types:\n string boolean short int long float double bytes serializable object row (a nested row is acceptable!)   The default serializer is located at Here   When you get the rows in the connector, you should follow the cell setting to generate the output. The cell setting in Ohara is called column. It shows the metadata of a cell. The metadata consists of:\n origin column name (string) \u0026mdash; you can match the cell by this name new column name \u0026mdash; the new name of output. type (DataType) \u0026mdash; the type of output value. Whatever the origin type of value, you should convert the value according this type. Don\u0026rsquo;t worry the casting error. It is up to the user who pass the wrong configuration.  string boolean short int long float double bytes serializable object row   order (int) \u0026mdash; the order of cells in output.  An example of converting data according to columns.\nimport oharastream.ohara.common.data.Cell; import oharastream.ohara.common.data.Column; class ExampleOfConverting { public static Object hello(Column column, String rawValue) { switch (column.dataType) { case DataType.BOOLEAN: return Boolean.valueOf(rawValue); case DataType.STRING: return rawValue; case DataType.SHORT: return Short.valueOf(rawValue); case DataType.INT: return Integer.valueOf(rawValue); case DataType.FLOAT: return Float.valueOf(rawValue); case DataType.DOUBLE: return Double.valueOf(rawValue); default: throw new IllegalArgumentException(\u0026quot;unsupported type:\u0026quot; + column.dataType); } } }  The type is a complicated issue since there are countless types in this world. It is impossible to define a general solution to handle all types so the final types of value is byte array or serializable object. If the type you want to pass is not in official support, you should define it as byte array or serializable object and then process it in your connectors.\n Feel free to throw an exception when your connector encounters an unknown type. Don\u0026rsquo;t swallow it and convert to a weird value, such as null or empty. Throwing exception is better than generating corrupt data!    Source Connector  Source connector is used to pull data from outside system and then push processed data to Ohara topics. A basic implementation for a source connector only includes four methods \u0026mdash; run, terminate, taskClass, and taskSetting\npublic abstract class RowSourceConnector extends SourceConnector { /** * Returns the RowSourceTask implementation for this Connector. * * @return a RowSourceTask class */ protected abstract Class\u0026lt;? extends RowSourceTask\u0026gt; taskClass(); /** * Return the settings for source task. * * @param maxTasks number of tasks for this connector * @return a seq from settings */ protected abstract List\u0026lt;TaskSetting\u0026gt; taskSetting(int maxTasks); /** * Start this Connector. This method will only be called on a clean Connector, i.e. it has either * just been instantiated and initialized or terminate() has been invoked. * * @param taskSetting configuration settings */ protected abstract void run(TaskSetting taskSetting); /** stop this connector */ protected abstract void terminate(); }  run(TaskSetting) After instantizing a connector, the first method called by worker is start(). You should initialize your connector in start method, since it has a input parameter TaskSetting carrying all settings, such as target topics, connector name and user-defined configs, from user. If you (connector developer) are a good friend of your connector user, you can get (and cast it to expected type) config, which is passed by connector user, from TaskSetting. For example, a connector user calls Connector API to store a config k0-v0 (both of them are string type) for your connector, and then you can get v0 via TaskSetting.stringValue(\u0026ldquo;k0\u0026rdquo;).\n Don\u0026rsquo;t be afraid of throwing exception when you notice that input parameters are incorrect. Throwing an exception can fail a connector quickly and stop worker to distribute connector task across cluster. It saves the time and resources.   We all hate wrong configs, right? When you design the connector, you can define the setting on your own initiative. The setting enable worker to check the input configs before starting connector. It can\u0026rsquo;t eliminate incorrect configs completely, but it save your time of fighting against wrong configs (have a great time with your family)\nterminate() This method is invoked by calling STOP API. You can release the resources allocated by connector, or email to shout at someone. It is ok to throw an exception when you fails to stop the connector. Worker cluster will mark failure on the connector, and the world keeps running.\ntaskClass() This method returns the java class of RowSourceTask implementation. It tells worker cluster which class should be created to pull data from external system. Noted that connector and task may not be created on same node (jvm) so you should NOT share any objects between them (for example, make them to access a global variable).\ntaskSetting(int maxTasks) Connector has to generate configs for each task. The value of maxTasks is configured by Connector API. If you prefer to make all tasks do identical job, you can just clone the task config passe by start. Or you can prepare different configs for each task. Noted that the number of configuration you return MUST be equal with input value - maxTasks. Otherwise, you will get an exception when running your connector.\n It would be better to do the final check to input configs in Connector rather than Task. Producing a failure quickly save your time and resources.    Source Task public abstract class RowSourceTask extends SourceTask { /** * Start the Task. This should handle any configuration parsing and one-time setup from the task. * * @param config initial configuration */ protected abstract void run(TaskSetting config); /** * Signal this SourceTask to stop. In SourceTasks, this method only needs to signal to the task * that it should stop trying to poll for new data and interrupt any outstanding poll() requests. * It is not required that the task has fully stopped. Note that this method necessarily may be * invoked from a different thread than pollRecords() and commitOffsets() */ protected abstract void terminate(); /** * Poll this SourceTask for new records. This method should block if no data is currently * available. * * @return a array from RowSourceRecord */ protected abstract List\u0026lt;RowSourceRecord\u0026gt; pollRecords(); }  RowSourceTask is the unit of executing poll. A connector can invoke multiple tasks if you set tasks.max be bigger than 1 via Connector API. RowSourceTask has the similar lifecycle to Source connector. Worker cluster call start to initialize a task and call stop to terminate a task.\npullRecords() You can ignore all methods except for pollRecords. Worker cluster call pollRecords regularly to get RowSourceRecord s and then save them to topics. Worker cluster does not care for your implementation. All you have to do is to put your data in RowSourceRecord. RowSourceRecord is a complicated object having many elements. Some elements are significant. For example, partition can impact the distribution of records. In order to be the best friend of programmer, Ohara follows the fluent pattern to allow you to create record through builder, and you can only fill the required elements.\npublic class ExampleOfRowSourceRecord { public static RowSourceRecord create(Row row, String topicName) { return RowSourceRecord.builder() .row(row) .topicName(topicName) .build(); } }   You can read the java docs of RowSourceRecord.Builder to see which default values are set for other (optional) elements.   Partition and Offsets in Source De-duplicating data is not a easy job. When you keep pulling data from external system to topics, you always need a place to record which data have not processed. Connector offers two specific objects for you to record the offset and partition of your data. You can define a partition and a offset for RowSourceRecord. The durability is on Worker\u0026rsquo;s shoulder, and you are always doable to get partition and offset back even if the connector fail or shutdown.\npublic class ExampleOfRowSourceContext { public static Map\u0026lt;String, ?\u0026gt; getOffset(Map\u0026lt;String, ?\u0026gt; partition) { return RowSourceContext.offset(partition); } }  Both of them are Map type with string key and primitive type. Using Map is a workaround to record the offsets for different connectors. You can view them as a flatten json representation. For example, one of task is handling file_a, and it has processed first line of file_a. Then the pair of partition and offset look like\n{ \u0026quot;fileName\u0026quot;: \u0026quot;file_a\u0026quot; }  { \u0026quot;offset\u0026quot;: 1 }  We can convert above json to partition and offset and then put them in RowSourceRecord.\npublic class ExampleOfPartitionAndOffset { public static RowSourceRecord addPartitionAndOffset(RowSourceRecord.Builder builder, String fileName, int offset) { Map\u0026lt;String, String\u0026gt; partition = Map.of(\u0026quot;fileName\u0026quot;, fileName); Map\u0026lt;String, Integer\u0026gt; offset = Map.of(\u0026quot;offset\u0026quot;, 1); return builder.sourcePartition(partition) .sourceOffset(offset) .build(); } }  A news of partition and offset is that they are not stored with data in RowSourceRecord. If you want to know the commit of partition and offset, you can override the commitOffsets().\npublic abstract class RowSourceTask extends SourceTask { /** * Commit the offsets, up to the offsets that have been returned by pollRecords(). This method should * block until the commit is complete. * * \u0026lt;p\u0026gt;SourceTasks are not required to implement this functionality; Kafka Connect will record * offsets automatically. This hook is provided for systems that also need to store offsets * internally in their own system. */ protected void commitOffsets() { // do nothing } }  Handle Exception in pollRecords() Throwing exception make connector in failure state, and inactivate connector until you restart it. Hence, you SHOULD catch and handle the exception as best you can. However, swallowing all exception is also a weired behavior. You SHOULD fails the connector when encountering unrecoverable exception.\nBlocking Action Is Unwelcome In pollRecords() Task is executed on a separate thread and there are many remaining processing for data after pollRecords(). Hence, you should NOT block pollRecords(). On the contrary, returning an empty list can yield the resource to remaining processing.\n Returning null results in same result. However, we all should hate null so please take away null from your code.   Data From pollRecords() Are Committed Async You don\u0026rsquo;t expect that the data you generated are commit at once, right? Committing data invokes a large latency since we need to sync data to multiple nodes and result in many disk I/O. Worker has another thread sending your data in background. If your connector needs to know the time of committing data, you can override the commitOffsetsRecord(RowSourceRecord).\npublic abstract class RowSourceTask extends SourceTask { /** * Commit an individual RowSourceRecord when the callback from the producer client is received, or * if a record is filtered by a transformation. SourceTasks are not required to implement this * functionality; Kafka Connect will record offsets automatically. This hook is provided for * systems that also need to store offsets internally in their own system. * * @param record RowSourceRecord that was successfully sent via the producer. */ protected void commitOffsetsRecord(RowSourceRecord record) { // do nothing } }   Sink Connector public abstract class RowSinkConnector extends SinkConnector { /** * Start this Connector. This method will only be called on a clean Connector, i.e. it has either * just been instantiated and initialized or terminate() has been invoked. * * @param config configuration settings */ protected abstract void run(TaskSetting config); /** stop this connector */ protected abstract void terminate(); /** * Returns the RowSinkTask implementation for this Connector. * * @return a RowSinkTask class */ protected abstract Class\u0026lt;? extends RowSinkTask\u0026gt; taskClass(); /** * Return the settings for source task. NOTED: It is illegal to assign different topics to * RowSinkTask * * @param maxTasks number of tasks for this connector * @return the settings for each tasks */ protected abstract List\u0026lt;TaskSetting\u0026gt; taskSetting(int maxTasks); }  Sink connector is similar to source connector. It also have run(TaskSetting), terminate(), taskClass(), taskSetting(int maxTasks), partition and offsets. The main difference between sink connector and source connector is that sink connector do pull data from topic and then push processed data to outside system. Hence, it does have pullRecords rather than pullRecords\n Though sink connector and source connector have many identical methods, you should NOT make a connector mixed sink and source. Because Both connector are abstract class, you can\u0026rsquo;t have a class extending both of them in java.   Sink connector also has to provide the task class to worker cluster. The sink task in Ohara is called RowSinkTask. It is also distributed across whole worker cluster when you run a sink connector.\n Sink Task public abstract class RowSinkTask extends SinkTask { /** * Start the Task. This should handle any configuration parsing and one-time setup from the task. * * @param config initial configuration */ protected abstract void run(TaskSetting config); /** * Perform any cleanup to stop this task. In SinkTasks, this method is invoked only once * outstanding calls to other methods have completed (e.g., pullRecords() has returned) and a final * flush() and offset commit has completed. Implementations from this method should only need to * perform final cleanup operations, such as closing network connections to the sink system. */ protected abstract void terminate(); /** * Put the table record in the sink. Usually this should send the records to the sink * asynchronously and immediately return. * * @param records table record */ protected abstract void pullRecords(List\u0026lt;RowSinkRecord\u0026gt; records); }  RowSinkTask is similar to RowSourceTask that both of them have run and stop phase. RowSinkTask is executed by a separate thread on worker also.\npullRecords(List records) Worker invokes a separate thread to fetch data from topic and put the data to sink task. The input data is called RowSinkRecord which carries not only row but also metadata.\n topicName (string) \u0026mdash; where the data come from Row (row) \u0026mdash; input data partition (int) \u0026mdash; index of partition offset (long) \u0026mdash; offset in topic-partition timestamp (long) \u0026mdash; data timestamp TimestampType (enum) \u0026mdash; the way of generating timestamp  NO_TIMESTAMP_TYPE - means timestamp is nothing for this data CREATE_TIME - the timestamp is provided by user or the time of sending this data LOG_APPEND_TIME - the timestamp is broker\u0026rsquo;s local time when the data is append    Partition and Offsets In Sink Sink task has a component, which is called RowSinkContext, saving the offset and partitions for input data. Commonly, it is not big news to you since kafka has responsibility to manage data offset in topic-partition to avoid losing data. However, if you have something more than data lost, such as exactly once, you can manage the data offset manually and then use RowSinkContext to change the offset of input data.\nHandle Exception In pullRecords(List) Any thrown exception will make this connector failed and stopped. You should handle the recoverable error and throw the exception which obstruct connector from running.\npublic interface RowSinkContext { /** * Reset the consumer offsets for the given topic partitions. SinkTasks should use this if they * manage offsets in the sink data store rather than using Kafka consumer offsets. For example, an * HDFS connector might record offsets in HDFS to provide exactly once delivery. When the SinkTask * is started or a rebalance occurs, the task would reload offsets from HDFS and use this method * to reset the consumer to those offsets. * * \u0026lt;p\u0026gt;SinkTasks that do not manage their own offsets do not need to use this method. * * @param offsets map from offsets for topic partitions */ void offset(Map\u0026lt;TopicPartition, Long\u0026gt; offsets); /** * Reset the consumer offsets for the given topic partition. SinkTasks should use if they manage * offsets in the sink data store rather than using Kafka consumer offsets. For example, an HDFS * connector might record offsets in HDFS to provide exactly once delivery. When the topic * partition is recovered the task would reload offsets from HDFS and use this method to reset the * consumer to the offset. * * \u0026lt;p\u0026gt;SinkTasks that do not manage their own offsets do not need to use this method. * * @param partition the topic partition to reset offset. * @param offset the offset to reset to. */ default void offset(TopicPartition partition, Long offset) { this.offset(Map.of(partition, offset)); } }   Noted that data offset is a order in topic-partition so the input of RowSinkContext.offset consists of topic name and partition.   Handle Exception In pullRecords(List) see handle exception in pollRecords()\nCommit Your Output Data When Kafka Commit Input Data While feeding data into your sink task, Kafka also tries to commit previous data that make the data disappear from you. The method preCommitOffsets is a callback of committing data offset. If you want to manage the offsets, you can change what to commit by kafka. Another use case is that you have some stuff which needs to be committed also, and you can trigger the commit in this callback.\npublic abstract class RowSinkTask extends SinkTask { /** * Pre-commit hook invoked prior to an offset commit. * * \u0026lt;p\u0026gt;The default implementation simply return the offsets and is thus able to assume all offsets * are safe to commit. * * @param offsets the current offset state as from the last call to pullRecords, provided for convenience * but could also be determined by tracking all offsets included in the RowSourceRecord's * passed to pullRecords. * @return an empty map if Connect-managed offset commit is not desired, otherwise a map from * offsets by topic-partition that are safe to commit. */ protected Map\u0026lt;TopicPartition, TopicOffset\u0026gt; preCommitOffsets(Map\u0026lt;TopicPartition, TopicOffset\u0026gt; offsets) { return offsets; } }   The offsets exceeding the latest consumed offset are discarded    Version We all love to show how good we are. If you are a connector designer, Ohara connector offers a way to show the version, revision and author for a connector.\npublic abstract class RowSourceConnector extends SourceConnector { public String version() { return VersionUtils.VERSION; } public String author() { return VersionUtils.USER; } public String revision() { return VersionUtils.REVISION; } }  The default value is version of build. You can override one of them or all of them when writing connector. The version information of a connector is showed by Worker APIs.\n Don\u0026rsquo;t return null, please!!!   Version in Ohara connector is different to kafka connector. The later only supports version and it\u0026rsquo;s APIs show only version. Hence, you can\u0026rsquo;t get revision, author or other settings through kafka APIs\n Metrics We are live in a world filled with number, and so do connectors. While a connector is running, Ohara collects many counts from the data flow for the connector in background. All of counters (and other records which will be introduced in the future) are called metrics, and it can be fetched by Connector API. Apart from official metrics, connector developers are also able to build custom metrics for custom connectors, and all custom metrics are also showed by Connector API.\nOhara leverage JMX to offer the metrics APIs to connector. It means all metrics you created are stored as Java beans and is accessible through JMX service. That is why you have to define a port via Worker APIs for creating a worker cluster. Although you can see all java mbeans via the JMX client (such as JMC), Ohara still encourage you to apply Connector API as it offers a more readable format of metrics.\nCounter Counter is a common use case for metrics that you can increment/decrement/add/ a number atomically. A counter consists of following members.\n group (string) \u0026mdash; the group of this counter name (string) \u0026mdash; the name of this counter unit (string) \u0026mdash; the unit of value document (string) \u0026mdash; the document for this metrics startTime (long) \u0026mdash; the time to start this counter value (long) \u0026mdash; current value of count  A example of creating a counter is shown below.\npublic class ExampleOfCreatingCounter { public static Counter sizeCounter(String group) { return Counter.builder() .group(group) .name(\u0026quot;row.size\u0026quot;) .unit(\u0026quot;bytes\u0026quot;) .document(\u0026quot;size (in bytes) of rows\u0026quot;) .startTime(CommonUtils.current()) .value(0) .register(); } }   Though unit and document are declared optional, making them have meaning description can help reader to understand the magic number from your counter.    The counter created by connector always has the group same to id of connector, since Ohara needs to find the counters for specific connector in Connector API   Official Metrics There are two official metrics for connector - row counter and bytes counter. The former is the number of processed rows, and the later is the number of processed data. Both of them are updated when data are pull/push from/to your connector. Normally, you don\u0026rsquo;t need to care for them when designing connectors. However, you can read the source code in ConnectorUtils.java to see how Ohara create official counters.\nCreate Your Own Counters In order to reduce your duplicate code, Ohara offers the CounterBuilder to all connectors. CounterBuilder is a wrap of Counter.Builder with some pre-defined variables, and hence the creation of CounterBuilder must be after initializing the connector/task.\npublic class ExampleOfCreatingCustomBuilder { public static Counter custom(RowSinkTask task) { return task.counterBuilder() .unit(\u0026quot;bytes\u0026quot;) .document(\u0026quot;size (in bytes) of rows\u0026quot;) .startTime(CommonUtils.current()) .value(0) .register(); } }   Ohara doesn\u0026rsquo;t obstruct you from using Counter directly. However, using CounterBuilder make sure that your custom metrics are available in Connector API.    Csv Sink Connector   Csv Sink connector inherits from Row Sink Connector. It also have run(TaskSetting), terminate(), taskClass(), taskSetting(int maxTasks), partition and offsets.\nThe main difference between csv sink connector and row sink connector is that csv sink connector already has some default definitions.\nBelow is a list of default definitions for CsvSinkConnector:\n TOPICS_DIR_DEFINITION: Read csv data from topic and then write to this folder FLUSH_SIZE_DEFINITION: Number of records write to store before invoking file commits ROTATE_INTERVAL_MS_DEFINITION: Commit file time FILE_NEED_HEADER_DEFINITION: File need header for flush data FILE_ENCODE_DEFINITION: File encode for write to file  Connector developers can override customSettingDefinitions to add other additional definitions:\npublic abstract class CsvSinkConnector extends RowSinkConnector { /** * Define the configuration for the connector. * * @return The SettingDef for this connector. */ protected List\u0026lt;SettingDef\u0026gt; customSettingDefinitions() { return List.of(); } }  Csv Sink Task Ohara has a well-incubated task class. We call it CsvSinkTask. As long as your data format is CSV type, you can use id to develop a sink connector to connect various file systems.\nWe all know that to make a strong and robust connector, you have to take care of a lot of details. In order to ensure that the connector works, we must also prepare a lot of tests. Connector developers will spend a lot of time on this.\nTherefore, we have encapsulated most of the logic in CsvSinkTask, which hides a lot of complex behaviors. Just provide a Storage implementation to complete a sink connector. You can save time to enjoy other happy things.\nThe following are the two methods you need to care about inherited CsvSinkTask:\npublic abstract class CsvSinkTask extends RowSinkTask { /** * Returns the Storage implementation for this Task. * * @param setting initial settings * @return a Storage instance */ public abstract Storage _storage(TaskSetting setting); }  _storage(TaskSetting setting) The goal of Task is to write the data to an external file system. For example, if we want to store the output files on FTP server, connector developers must provide an implementation of Storage that can access FTP.\n The input parameter TaskSetting carrying all settings. see TaskSetting   Storage This interface defines some common methods for accessing the file system, such as checking for the existence of a file, creating a new file, or reading an exiting file, etc. Connector developers can follow this interface to implement different file systems, such as FTP, HDFS, SMB, Amazon S3, etc. So, just provide the implementation of Storage to CsvSinkTask and you can implement a Sink Connector very quickly.\nBelow we list the important methods in the Storage interface:\npublic interface Storage extends Releasable { /** * Returns whether an object exists. * * @param path the path to the object. * @return true if object exists, false otherwise. */ boolean exists(String path); /** * Creates a new object in the given path. * * @param path the path of the object to be created. * @throws OharaFileAlreadyExistsException if a object of that path already exists. * @throws OharaException if the parent container does not exist. * @return an output stream associated with the new object. */ OutputStream create(String path); /** * Open for reading an object at the given path. * * @param path the path of the object to be read. * @return an input stream with the requested object. */ InputStream open(String path); /** * Move or rename a object from source path to target path. * * @param sourcePath the path to the object to move * @param targetPath the path to the target object * @return true if object have moved to target path , false otherwise. */ boolean move(String sourcePath, String targetPath); /** Stop using this storage. */ void close(); }   You can read the FtpStorage as an example to see how to implement your own Storage.   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"ee1be75e000770f9ec50ccdb064b7921","permalink":"https://oharastream.github.io/en/docs/0.11.x/custom_connector/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/custom_connector/","section":"docs","summary":"Ohara custom connector is based on kafka connector. It offers a platform that enables you to define some simple actions to connect topic to any other system. You don\u0026rsquo;t need to worry the application availability, data durability, or distribution anymore.","tags":null,"title":"Custom Connector Guideline","type":"docs"},{"authors":null,"categories":null,"content":"Ohara custom connector is based on kafka connector. It offers a platform that enables you to define some simple actions to connect topic to any other system. You don\u0026rsquo;t need to worry the application availability, data durability, or distribution anymore. All you have to do is to write your custom connector, which can have only the pull()/push() method, and then compile your code to a jar file. After uploading your jar to Ohara, you are able to deploy your connector on the [worker cluster]/en/docs/master/rest-api/workers/#rest-workers-create. By leveraging Ohara connector framework, apart from the availability, scalability, and durability, you can also monitor your connector for logs and metrics.\nThe following sections start to explain how to write a good connector on Ohara. You don\u0026rsquo;t need to read it through if you are familiar with kafka connector. However, Ohara connector has some improvements which are not in kafka connector so it has worth of taking a look at metrics and setting definitions\n Connector Overview Ohara connector is composed of source connector and sink connector . source connector is used to pull data from external system to topic. By contrast, sink connector is used to pull data from topic to external system. A complete connector consists of SourceConnector / SinkConnector and SourceTask / SinkTask. Worker cluster picks up a node to host your source/sink connector and then distributes the source/sink tasks across cluster.\nYou must include Ohara jars before starting to write your custom connector. Please include both of ohara-common and ohara-kafka in your dependencies. The ohara-common contains many helper methods and common data used in whole Ohara. The ohara-kafka offers a lot of beautiful APIs to help you to access kafka and design custom connector.\nrepositories { maven { url \u0026quot;https://dl.bintray.com/oharastream/ohara\u0026quot; } } implementation \u0026quot;oharastream.ohara:ohara-common:0.11.0-SNAPSHOT\u0026quot; implementation \u0026quot;oharastream.ohara:ohara-kafka:0.11.0-SNAPSHOT\u0026quot;   The releases page shows the available version of Ohara    Data Model Ohara has defined a table structure data in code base. We call it row. A row is comprised of multiple cells. Each cell has its name, value and tags. The value in cell is a generic type which accepts any serializable type of value. Following is an example that shows you how to convert a csv data to Ohara row.\nc0,c1,c2 v0,v1,v2  The above data can be converted to Ohara row by following code.\nimport oharastream.ohara.common.data.Row; import oharastream.ohara.common.data.Cell; class ExampleOfRow { public static void main(String[] args) { Row row = Row.of( Cell.of(\u0026quot;c0\u0026quot;, \u0026quot;v0\u0026quot;), Cell.of(\u0026quot;c1\u0026quot;, \u0026quot;v1\u0026quot;), Cell.of(\u0026quot;c2\u0026quot;, \u0026quot;v2\u0026quot;) ); } }  c0,c1,c2 v0,,v2  The above data can be converted to Ohara row by following code.\nimport oharastream.ohara.common.data.Row; import oharastream.ohara.common.data.Cell; class ExampleOfRow { public static void main(String[] args) { Row row = Row.of( Cell.of(\u0026quot;c0\u0026quot;, \u0026quot;v0\u0026quot;), Cell.of(\u0026quot;c2\u0026quot;, \u0026quot;v2\u0026quot;) ); } }  Don\u0026rsquo;t worry about the serialization. Ohara offers default serializations for following data types:\n string boolean short int long float double bytes serializable object row (a nested row is acceptable!)   The default serializer is located at Here   When you get the rows in the connector, you should follow the cell setting to generate the output. The cell setting in Ohara is called column. It shows the metadata of a cell. The metadata consists of:\n origin column name (string) \u0026mdash; you can match the cell by this name new column name \u0026mdash; the new name of output. type (DataType) \u0026mdash; the type of output value. Whatever the origin type of value, you should convert the value according this type. Don\u0026rsquo;t worry the casting error. It is up to the user who pass the wrong configuration.  string boolean short int long float double bytes serializable object row   order (int) \u0026mdash; the order of cells in output.  An example of converting data according to columns.\nimport oharastream.ohara.common.data.Cell; import oharastream.ohara.common.data.Column; class ExampleOfConverting { public static Object hello(Column column, String rawValue) { switch (column.dataType) { case DataType.BOOLEAN: return Boolean.valueOf(rawValue); case DataType.STRING: return rawValue; case DataType.SHORT: return Short.valueOf(rawValue); case DataType.INT: return Integer.valueOf(rawValue); case DataType.FLOAT: return Float.valueOf(rawValue); case DataType.DOUBLE: return Double.valueOf(rawValue); default: throw new IllegalArgumentException(\u0026quot;unsupported type:\u0026quot; + column.dataType); } } }  The type is a complicated issue since there are countless types in this world. It is impossible to define a general solution to handle all types so the final types of value is byte array or serializable object. If the type you want to pass is not in official support, you should define it as byte array or serializable object and then process it in your connectors.\n Feel free to throw an exception when your connector encounters an unknown type. Don\u0026rsquo;t swallow it and convert to a weird value, such as null or empty. Throwing exception is better than generating corrupt data!    Source Connector  Source connector is used to pull data from outside system and then push processed data to Ohara topics. A basic implementation for a source connector only includes four methods \u0026mdash; run, terminate, taskClass, and taskSetting\npublic abstract class RowSourceConnector extends SourceConnector { /** * Returns the RowSourceTask implementation for this Connector. * * @return a RowSourceTask class */ protected abstract Class\u0026lt;? extends RowSourceTask\u0026gt; taskClass(); /** * Return the settings for source task. * * @param maxTasks number of tasks for this connector * @return a seq from settings */ protected abstract List\u0026lt;TaskSetting\u0026gt; taskSetting(int maxTasks); /** * Start this Connector. This method will only be called on a clean Connector, i.e. it has either * just been instantiated and initialized or terminate() has been invoked. * * @param taskSetting configuration settings */ protected abstract void run(TaskSetting taskSetting); /** stop this connector */ protected abstract void terminate(); }  run(TaskSetting) After instantizing a connector, the first method called by worker is start(). You should initialize your connector in start method, since it has a input parameter TaskSetting carrying all settings, such as target topics, connector name and user-defined configs, from user. If you (connector developer) are a good friend of your connector user, you can get (and cast it to expected type) config, which is passed by connector user, from TaskSetting. For example, a connector user calls Connector API to store a config k0-v0 (both of them are string type) for your connector, and then you can get v0 via TaskSetting.stringValue(\u0026ldquo;k0\u0026rdquo;).\n Don\u0026rsquo;t be afraid of throwing exception when you notice that input parameters are incorrect. Throwing an exception can fail a connector quickly and stop worker to distribute connector task across cluster. It saves the time and resources.   We all hate wrong configs, right? When you design the connector, you can define the setting on your own initiative. The setting enable worker to check the input configs before starting connector. It can\u0026rsquo;t eliminate incorrect configs completely, but it save your time of fighting against wrong configs (have a great time with your family)\nterminate() This method is invoked by calling STOP API. You can release the resources allocated by connector, or email to shout at someone. It is ok to throw an exception when you fails to stop the connector. Worker cluster will mark failure on the connector, and the world keeps running.\ntaskClass() This method returns the java class of RowSourceTask implementation. It tells worker cluster which class should be created to pull data from external system. Noted that connector and task may not be created on same node (jvm) so you should NOT share any objects between them (for example, make them to access a global variable).\ntaskSetting(int maxTasks) Connector has to generate configs for each task. The value of maxTasks is configured by Connector API. If you prefer to make all tasks do identical job, you can just clone the task config passe by start. Or you can prepare different configs for each task. Noted that the number of configuration you return MUST be equal with input value - maxTasks. Otherwise, you will get an exception when running your connector.\n It would be better to do the final check to input configs in Connector rather than Task. Producing a failure quickly save your time and resources.    Source Task public abstract class RowSourceTask extends SourceTask { /** * Start the Task. This should handle any configuration parsing and one-time setup from the task. * * @param config initial configuration */ protected abstract void run(TaskSetting config); /** * Signal this SourceTask to stop. In SourceTasks, this method only needs to signal to the task * that it should stop trying to poll for new data and interrupt any outstanding poll() requests. * It is not required that the task has fully stopped. Note that this method necessarily may be * invoked from a different thread than pollRecords() and commitOffsets() */ protected abstract void terminate(); /** * Poll this SourceTask for new records. This method should block if no data is currently * available. * * @return a array from RowSourceRecord */ protected abstract List\u0026lt;RowSourceRecord\u0026gt; pollRecords(); }  RowSourceTask is the unit of executing poll. A connector can invoke multiple tasks if you set tasks.max be bigger than 1 via Connector API. RowSourceTask has the similar lifecycle to Source connector. Worker cluster call start to initialize a task and call stop to terminate a task.\npullRecords() You can ignore all methods except for pollRecords. Worker cluster call pollRecords regularly to get RowSourceRecord s and then save them to topics. Worker cluster does not care for your implementation. All you have to do is to put your data in RowSourceRecord. RowSourceRecord is a complicated object having many elements. Some elements are significant. For example, partition can impact the distribution of records. In order to be the best friend of programmer, Ohara follows the fluent pattern to allow you to create record through builder, and you can only fill the required elements.\npublic class ExampleOfRowSourceRecord { public static RowSourceRecord create(Row row, String topicName) { return RowSourceRecord.builder() .row(row) .topicName(topicName) .build(); } }   You can read the java docs of RowSourceRecord.Builder to see which default values are set for other (optional) elements.   Partition and Offsets in Source De-duplicating data is not a easy job. When you keep pulling data from external system to topics, you always need a place to record which data have not processed. Connector offers two specific objects for you to record the offset and partition of your data. You can define a partition and a offset for RowSourceRecord. The durability is on Worker\u0026rsquo;s shoulder, and you are always doable to get partition and offset back even if the connector fail or shutdown.\npublic class ExampleOfRowSourceContext { public static Map\u0026lt;String, ?\u0026gt; getOffset(Map\u0026lt;String, ?\u0026gt; partition) { return RowSourceContext.offset(partition); } }  Both of them are Map type with string key and primitive type. Using Map is a workaround to record the offsets for different connectors. You can view them as a flatten json representation. For example, one of task is handling file_a, and it has processed first line of file_a. Then the pair of partition and offset look like\n{ \u0026quot;fileName\u0026quot;: \u0026quot;file_a\u0026quot; }  { \u0026quot;offset\u0026quot;: 1 }  We can convert above json to partition and offset and then put them in RowSourceRecord.\npublic class ExampleOfPartitionAndOffset { public static RowSourceRecord addPartitionAndOffset(RowSourceRecord.Builder builder, String fileName, int offset) { Map\u0026lt;String, String\u0026gt; partition = Map.of(\u0026quot;fileName\u0026quot;, fileName); Map\u0026lt;String, Integer\u0026gt; offset = Map.of(\u0026quot;offset\u0026quot;, 1); return builder.sourcePartition(partition) .sourceOffset(offset) .build(); } }  A news of partition and offset is that they are not stored with data in RowSourceRecord. If you want to know the commit of partition and offset, you can override the commitOffsets().\npublic abstract class RowSourceTask extends SourceTask { /** * Commit the offsets, up to the offsets that have been returned by pollRecords(). This method should * block until the commit is complete. * * \u0026lt;p\u0026gt;SourceTasks are not required to implement this functionality; Kafka Connect will record * offsets automatically. This hook is provided for systems that also need to store offsets * internally in their own system. */ protected void commitOffsets() { // do nothing } }  Handle Exception in pollRecords() Throwing exception make connector in failure state, and inactivate connector until you restart it. Hence, you SHOULD catch and handle the exception as best you can. However, swallowing all exception is also a weired behavior. You SHOULD fails the connector when encountering unrecoverable exception.\nBlocking Action Is Unwelcome In pollRecords() Task is executed on a separate thread and there are many remaining processing for data after pollRecords(). Hence, you should NOT block pollRecords(). On the contrary, returning an empty list can yield the resource to remaining processing.\n Returning null results in same result. However, we all should hate null so please take away null from your code.   Data From pollRecords() Are Committed Async You don\u0026rsquo;t expect that the data you generated are commit at once, right? Committing data invokes a large latency since we need to sync data to multiple nodes and result in many disk I/O. Worker has another thread sending your data in background. If your connector needs to know the time of committing data, you can override the commitOffsetsRecord(RowSourceRecord).\npublic abstract class RowSourceTask extends SourceTask { /** * Commit an individual RowSourceRecord when the callback from the producer client is received, or * if a record is filtered by a transformation. SourceTasks are not required to implement this * functionality; Kafka Connect will record offsets automatically. This hook is provided for * systems that also need to store offsets internally in their own system. * * @param record RowSourceRecord that was successfully sent via the producer. */ protected void commitOffsetsRecord(RowSourceRecord record) { // do nothing } }   Sink Connector public abstract class RowSinkConnector extends SinkConnector { /** * Start this Connector. This method will only be called on a clean Connector, i.e. it has either * just been instantiated and initialized or terminate() has been invoked. * * @param config configuration settings */ protected abstract void run(TaskSetting config); /** stop this connector */ protected abstract void terminate(); /** * Returns the RowSinkTask implementation for this Connector. * * @return a RowSinkTask class */ protected abstract Class\u0026lt;? extends RowSinkTask\u0026gt; taskClass(); /** * Return the settings for source task. NOTED: It is illegal to assign different topics to * RowSinkTask * * @param maxTasks number of tasks for this connector * @return the settings for each tasks */ protected abstract List\u0026lt;TaskSetting\u0026gt; taskSetting(int maxTasks); }  Sink connector is similar to source connector. It also have run(TaskSetting), terminate(), taskClass(), taskSetting(int maxTasks), partition and offsets. The main difference between sink connector and source connector is that sink connector do pull data from topic and then push processed data to outside system. Hence, it does have pullRecords rather than pullRecords\n Though sink connector and source connector have many identical methods, you should NOT make a connector mixed sink and source. Because Both connector are abstract class, you can\u0026rsquo;t have a class extending both of them in java.   Sink connector also has to provide the task class to worker cluster. The sink task in Ohara is called RowSinkTask. It is also distributed across whole worker cluster when you run a sink connector.\n Sink Task public abstract class RowSinkTask extends SinkTask { /** * Start the Task. This should handle any configuration parsing and one-time setup from the task. * * @param config initial configuration */ protected abstract void run(TaskSetting config); /** * Perform any cleanup to stop this task. In SinkTasks, this method is invoked only once * outstanding calls to other methods have completed (e.g., pullRecords() has returned) and a final * flush() and offset commit has completed. Implementations from this method should only need to * perform final cleanup operations, such as closing network connections to the sink system. */ protected abstract void terminate(); /** * Put the table record in the sink. Usually this should send the records to the sink * asynchronously and immediately return. * * @param records table record */ protected abstract void pullRecords(List\u0026lt;RowSinkRecord\u0026gt; records); }  RowSinkTask is similar to RowSourceTask that both of them have run and stop phase. RowSinkTask is executed by a separate thread on worker also.\npullRecords(List records) Worker invokes a separate thread to fetch data from topic and put the data to sink task. The input data is called RowSinkRecord which carries not only row but also metadata.\n topicName (string) \u0026mdash; where the data come from Row (row) \u0026mdash; input data partition (int) \u0026mdash; index of partition offset (long) \u0026mdash; offset in topic-partition timestamp (long) \u0026mdash; data timestamp TimestampType (enum) \u0026mdash; the way of generating timestamp  NO_TIMESTAMP_TYPE - means timestamp is nothing for this data CREATE_TIME - the timestamp is provided by user or the time of sending this data LOG_APPEND_TIME - the timestamp is broker\u0026rsquo;s local time when the data is append    Partition and Offsets In Sink Sink task has a component, which is called RowSinkContext, saving the offset and partitions for input data. Commonly, it is not big news to you since kafka has responsibility to manage data offset in topic-partition to avoid losing data. However, if you have something more than data lost, such as exactly once, you can manage the data offset manually and then use RowSinkContext to change the offset of input data.\nHandle Exception In pullRecords(List) Any thrown exception will make this connector failed and stopped. You should handle the recoverable error and throw the exception which obstruct connector from running.\npublic interface RowSinkContext { /** * Reset the consumer offsets for the given topic partitions. SinkTasks should use this if they * manage offsets in the sink data store rather than using Kafka consumer offsets. For example, an * HDFS connector might record offsets in HDFS to provide exactly once delivery. When the SinkTask * is started or a rebalance occurs, the task would reload offsets from HDFS and use this method * to reset the consumer to those offsets. * * \u0026lt;p\u0026gt;SinkTasks that do not manage their own offsets do not need to use this method. * * @param offsets map from offsets for topic partitions */ void offset(Map\u0026lt;TopicPartition, Long\u0026gt; offsets); /** * Reset the consumer offsets for the given topic partition. SinkTasks should use if they manage * offsets in the sink data store rather than using Kafka consumer offsets. For example, an HDFS * connector might record offsets in HDFS to provide exactly once delivery. When the topic * partition is recovered the task would reload offsets from HDFS and use this method to reset the * consumer to the offset. * * \u0026lt;p\u0026gt;SinkTasks that do not manage their own offsets do not need to use this method. * * @param partition the topic partition to reset offset. * @param offset the offset to reset to. */ default void offset(TopicPartition partition, Long offset) { this.offset(Map.of(partition, offset)); } }   Noted that data offset is a order in topic-partition so the input of RowSinkContext.offset consists of topic name and partition.   Handle Exception In pullRecords(List) see handle exception in pollRecords()\nCommit Your Output Data When Kafka Commit Input Data While feeding data into your sink task, Kafka also tries to commit previous data that make the data disappear from you. The method preCommitOffsets is a callback of committing data offset. If you want to manage the offsets, you can change what to commit by kafka. Another use case is that you have some stuff which needs to be committed also, and you can trigger the commit in this callback.\npublic abstract class RowSinkTask extends SinkTask { /** * Pre-commit hook invoked prior to an offset commit. * * \u0026lt;p\u0026gt;The default implementation simply return the offsets and is thus able to assume all offsets * are safe to commit. * * @param offsets the current offset state as from the last call to pullRecords, provided for convenience * but could also be determined by tracking all offsets included in the RowSourceRecord's * passed to pullRecords. * @return an empty map if Connect-managed offset commit is not desired, otherwise a map from * offsets by topic-partition that are safe to commit. */ protected Map\u0026lt;TopicPartition, TopicOffset\u0026gt; preCommitOffsets(Map\u0026lt;TopicPartition, TopicOffset\u0026gt; offsets) { return offsets; } }   The offsets exceeding the latest consumed offset are discarded    Version We all love to show how good we are. If you are a connector designer, Ohara connector offers a way to show the version, revision and author for a connector.\npublic abstract class RowSourceConnector extends SourceConnector { public String version() { return VersionUtils.VERSION; } public String author() { return VersionUtils.USER; } public String revision() { return VersionUtils.REVISION; } }  The default value is version of build. You can override one of them or all of them when writing connector. The version information of a connector is showed by Worker APIs.\n Don\u0026rsquo;t return null, please!!!   Version in Ohara connector is different to kafka connector. The later only supports version and it\u0026rsquo;s APIs show only version. Hence, you can\u0026rsquo;t get revision, author or other settings through kafka APIs\n Metrics We are live in a world filled with number, and so do connectors. While a connector is running, Ohara collects many counts from the data flow for the connector in background. All of counters (and other records which will be introduced in the future) are called metrics, and it can be fetched by Connector API. Apart from official metrics, connector developers are also able to build custom metrics for custom connectors, and all custom metrics are also showed by Connector API.\nOhara leverage JMX to offer the metrics APIs to connector. It means all metrics you created are stored as Java beans and is accessible through JMX service. That is why you have to define a port via Worker APIs for creating a worker cluster. Although you can see all java mbeans via the JMX client (such as JMC), Ohara still encourage you to apply Connector API as it offers a more readable format of metrics.\nCounter Counter is a common use case for metrics that you can increment/decrement/add/ a number atomically. A counter consists of following members.\n group (string) \u0026mdash; the group of this counter name (string) \u0026mdash; the name of this counter unit (string) \u0026mdash; the unit of value document (string) \u0026mdash; the document for this metrics startTime (long) \u0026mdash; the time to start this counter value (long) \u0026mdash; current value of count  A example of creating a counter is shown below.\npublic class ExampleOfCreatingCounter { public static Counter sizeCounter(String group) { return Counter.builder() .group(group) .name(\u0026quot;row.size\u0026quot;) .unit(\u0026quot;bytes\u0026quot;) .document(\u0026quot;size (in bytes) of rows\u0026quot;) .startTime(CommonUtils.current()) .value(0) .register(); } }   Though unit and document are declared optional, making them have meaning description can help reader to understand the magic number from your counter.    The counter created by connector always has the group same to id of connector, since Ohara needs to find the counters for specific connector in Connector API   Official Metrics There are two official metrics for connector - row counter and bytes counter. The former is the number of processed rows, and the later is the number of processed data. Both of them are updated when data are pull/push from/to your connector. Normally, you don\u0026rsquo;t need to care for them when designing connectors. However, you can read the source code in ConnectorUtils.java to see how Ohara create official counters.\nCreate Your Own Counters In order to reduce your duplicate code, Ohara offers the CounterBuilder to all connectors. CounterBuilder is a wrap of Counter.Builder with some pre-defined variables, and hence the creation of CounterBuilder must be after initializing the connector/task.\npublic class ExampleOfCreatingCustomBuilder { public static Counter custom(RowSinkTask task) { return task.counterBuilder() .unit(\u0026quot;bytes\u0026quot;) .document(\u0026quot;size (in bytes) of rows\u0026quot;) .startTime(CommonUtils.current()) .value(0) .register(); } }   Ohara doesn\u0026rsquo;t obstruct you from using Counter directly. However, using CounterBuilder make sure that your custom metrics are available in Connector API.    Csv Sink Connector   Csv Sink connector inherits from Row Sink Connector. It also have run(TaskSetting), terminate(), taskClass(), taskSetting(int maxTasks), partition and offsets.\nThe main difference between csv sink connector and row sink connector is that csv sink connector already has some default definitions.\nBelow is a list of default definitions for CsvSinkConnector:\n TOPICS_DIR_DEFINITION: Read csv data from topic and then write to this folder FLUSH_SIZE_DEFINITION: Number of records write to store before invoking file commits ROTATE_INTERVAL_MS_DEFINITION: Commit file time FILE_NEED_HEADER_DEFINITION: File need header for flush data FILE_ENCODE_DEFINITION: File encode for write to file  Connector developers can override customSettingDefinitions to add other additional definitions:\npublic abstract class CsvSinkConnector extends RowSinkConnector { /** * Define the configuration for the connector. * * @return The SettingDef for this connector. */ protected List\u0026lt;SettingDef\u0026gt; customSettingDefinitions() { return List.of(); } }  Csv Sink Task Ohara has a well-incubated task class. We call it CsvSinkTask. As long as your data format is CSV type, you can use id to develop a sink connector to connect various file systems.\nWe all know that to make a strong and robust connector, you have to take care of a lot of details. In order to ensure that the connector works, we must also prepare a lot of tests. Connector developers will spend a lot of time on this.\nTherefore, we have encapsulated most of the logic in CsvSinkTask, which hides a lot of complex behaviors. Just provide a Storage implementation to complete a sink connector. You can save time to enjoy other happy things.\nThe following are the two methods you need to care about inherited CsvSinkTask:\npublic abstract class CsvSinkTask extends RowSinkTask { /** * Returns the Storage implementation for this Task. * * @param setting initial settings * @return a Storage instance */ public abstract Storage _storage(TaskSetting setting); }  _storage(TaskSetting setting) The goal of Task is to write the data to an external file system. For example, if we want to store the output files on FTP server, connector developers must provide an implementation of Storage that can access FTP.\n The input parameter TaskSetting carrying all settings. see TaskSetting   Storage This interface defines some common methods for accessing the file system, such as checking for the existence of a file, creating a new file, or reading an exiting file, etc. Connector developers can follow this interface to implement different file systems, such as FTP, HDFS, SMB, Amazon S3, etc. So, just provide the implementation of Storage to CsvSinkTask and you can implement a Sink Connector very quickly.\nBelow we list the important methods in the Storage interface:\npublic interface Storage extends Releasable { /** * Returns whether an object exists. * * @param path the path to the object. * @return true if object exists, false otherwise. */ boolean exists(String path); /** * Creates a new object in the given path. * * @param path the path of the object to be created. * @throws OharaFileAlreadyExistsException if a object of that path already exists. * @throws OharaException if the parent container does not exist. * @return an output stream associated with the new object. */ OutputStream create(String path); /** * Open for reading an object at the given path. * * @param path the path of the object to be read. * @return an input stream with the requested object. */ InputStream open(String path); /** * Move or rename a object from source path to target path. * * @param sourcePath the path to the object to move * @param targetPath the path to the target object * @return true if object have moved to target path , false otherwise. */ boolean move(String sourcePath, String targetPath); /** Stop using this storage. */ void close(); }   You can read the FtpStorage as an example to see how to implement your own Storage.   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"bad7eb477767c558968180ee68381db4","permalink":"https://oharastream.github.io/en/docs/master/custom_connector/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/custom_connector/","section":"docs","summary":"Ohara custom connector is based on kafka connector. It offers a platform that enables you to define some simple actions to connect topic to any other system. You don\u0026rsquo;t need to worry the application availability, data durability, or distribution anymore.","tags":null,"title":"Custom Connector Guideline","type":"docs"},{"authors":null,"categories":null,"content":"Ohara stream is an unparalleled wrap of kafka streams which gives you a straightforward thought to design your streaming flow. It offers a simple way to implement and define actions to process data between topics. You only have to write your logic in start() method and compile your code to a jar file. After jar file is compiled successfully, you can deploy your jar file to Ohara, run it, and monitor your stream by logs and metrics.\nThe following sections will describe how to write a stream application in Ohara.\n Ohara Stream Overview Ohara stream is a wrap of kafka streams and provided an entry of interface class Stream to define user custom streaming code. A normal stream application will use Row as data type to interactive topics in Ohara.\nBefore writing your stream, you should download the ohara dependencies first. Ohara includes many powerful tools for developer but not all tools are requisite in designing stream. The required dependencies are shown below.\nrepositories { maven { url \u0026quot;https://dl.bintray.com/oharastream/ohara\u0026quot; } } implementation \u0026quot;oharastream.ohara:ohara-stream:0.11.0-SNAPSHOT\u0026quot; implementation \u0026quot;oharastream.ohara:ohara-common:0.11.0-SNAPSHOT\u0026quot; implementation \u0026quot;oharastream.ohara:ohara-kafka:0.11.0-SNAPSHOT\u0026quot;   The releases page shows the available version of ohara    Stream Entry  We will automatically find your custom class which should be extended by oharastream.ohara.stream.Stream.\nIn Ohara environment, the required parameters are defined in Ohara UI. You only need to initial the OStream as following:\nOStream\u0026lt;Row\u0026gt; ostream = OStream.builder().toOharaEnvStream();  A base implementation for a custom stream only need to include start() method, but you could include other methods which are described below for your convenience.\nThe following example is a simple stream application which can run in Ohara. Note that this example simply starts the stream application without doing any transformation but writing data, i.e., the target topic will have same data as the source topic.\npublic class SimpleApplicationForOharaEnv extends Stream { @Override public void start(OStream\u0026lt;Row\u0026gt; ostream, StreamDefinitions streamSetting) { ostream.start(); } }   The following methods we provided belongs to Ohara Stream, which has many powerful and friendly features. Native Kafka Streams API does not have these methods.   init() method After we find the user custom class, the first method will be called by Stream is init(). This is an optional method that can be used for user to initialize some external data source connections or input parameters.\nconfig() method In a stream application, you may want to configure your own parameters. We support a method here to help you define a custom streamSetting list in stream. The details of streamSetting are list here.\nIn the following example, we want to add a custom definition which is used to define \u0026ldquo;join topic\u0026rdquo;:\n@Override public StreamDefinitions config() { return StreamDefinitions // add a definition of \u0026quot;filter name\u0026quot; in \u0026quot;default\u0026quot; group .with(SettingDef.builder().key(\u0026quot;filterName\u0026quot;).group(\u0026quot;default\u0026quot;).build()); }  After define the definition, you can use it in start() method\n This method is optional. We will append all the definitions you provide in this method to the stream default definitions. That is, the absent config() method means you only need the default definitions.   start(OStream, StreamDefinitions) method This method will be called after init(). Normally, you could only define start() method for most cases in Ohara. We encourage user to use source connector (see Source Connector section) for importing external data source to Ohara and use topic data as custom stream data source in start() method.\nWe provide two arguments in this method:\n  OStream \u0026mdash; the entry class of ohara stream\nOStream (a.k.a. ohara stream) helps you to construct your application and use all the powerful APIs in Stream.\n  StreamDefinitions \u0026mdash; the definitions of ohara stream\nfrom the definition you can use StreamDefinitions.string() to get the value from the config method.\n   The return value is wrap in a Java object Optional, you need to decide whether the value is present or not.   For example:\n@Override public void start(OStream\u0026lt;Row\u0026gt; ostream, StreamDefinitions streamSetting) { ostream .map(row -\u0026gt; Row.of(row.cell(\u0026quot;name\u0026quot;), row.cell(\u0026quot;age\u0026quot;))) // use the previous defined definition in config() .filter(row -\u0026gt; row.cell(streamSetting.string(\u0026quot;filterName\u0026quot;).get()).value() != null) .map(row -\u0026gt; Row.of(Cell.of(\u0026quot;name\u0026quot;, row.cell(\u0026quot;name\u0026quot;).value().toString().toUpperCase()))) .start(); }  The above code does the following transformations:\n  pick cell of the header: name, age from each row\n  filter out that if filterName is null \u0026mdash; here we get the value from filterName of definitions. the value you should update by Stream update api\nPUT /v0/streams/XXX\n{ \u0026quot;filterName\u0026quot;: \u0026quot;name\u0026quot; }    convert the cell of name to upperCase\n  From now on, you can use the Stream Java API to design your own application, happy coding!\nStream Java API  In Stream, we provide three different classes for developers:\n OStream: define the functions for operating streaming data (each row record one-by-one) OGroupedStream: define the functions for operating grouped streaming data OTable: define the functions for operating table data (changelog for same key of row record)  The above classes will be auto converted when you use the correspond functions; You should not worry about the usage of which class is right to use. All the starting point of development is just OStream.\nBelow we list the available functions in each class (See more information in javadoc):\nOStream   constructTable(String topicName)\nCreate a OTable with specified topicName from current OStream.\n  filter(Predicate predicate)\nCreate a new OStream that filter by the given predicate.\n  through(String topicName, int partitions)\nTransfer this OStream to specify topic and use the required partition number.\n  leftJoin(String joinTopicName, Conditions conditions, ValueJoiner joiner)\nJoin this OStream with required joinTopicName and conditions.\n  map(ValueMapper mapper)\nTransform the value of each record to a new value of the output record.\n  groupByKey(List keys)\nGroup the records by key to a OGroupedStream.\n  foreach(ForeachAction action)\nPerform an action on each record of OStream.\n  start()\nRun this stream application.\n  stop()\nStop this stream application.\n  describe()\nDescribe the topology of this stream.\n  getPoneglyph()\nGet the Ohara format Poneglyph from topology.\n  OGroupedStream   count()\nCount the number of records in this OGroupedStream and return the count value.\n  reduce(final Reducer reducer)\nCombine the values of each record in this OGroupedStream by the grouped key.\n  OTable   toOStream()\nConvert this OTable to OStream\n   Stream Examples Below we provide some examples that demonstrate how to develop your own stream applications. More description of each example could be found in javadoc.\n  WordCount: count the words in \u0026ldquo;word\u0026rdquo; column  PageViewRegion: count the views by each region  Sum: sum odd numbers in \u0026ldquo;number\u0026rdquo; column   Stream Definitions Stream stores a list of SettingDef, which is StreamDefinitions, in the data store. By default, we will keep the following definitions in the \u0026ldquo;core\u0026rdquo; group and generate the definition in stream API :\n DefaultConfigs.BROKER_DEFINITION : The broker list DefaultConfigs.IMAGE_NAME_DEFINITION : The image name DefaultConfigs.NAME_DEFINITION : The stream application name DefaultConfigs.GROUP_DEFINITION : The stream group name DefaultConfigs.FROM_TOPICS_DEFINITION : The from topic DefaultConfigs.TO_TOPICS_DEFINITION : The to topic DefaultConfigs.JMX_PORT_DEFINITION : The exposed jmx port DefaultConfigs.NODE_NAMES_DEFINITION : The node name list DefaultConfigs.VERSION_DEFINITION : The version of stream DefaultConfigs.REVISION_DEFINITION : The revision of stream DefaultConfigs.AUTHOR_DEFINITION : The author of stream DefaultConfigs.TAGS_DEFINITION : The tags of stream  Any other definition except above list will be treated as a custom definition. You can define:\nSettingDef.builder().key(joinTopic).group(\u0026quot;default\u0026quot;).build()  as a definition that is listed in \u0026ldquo;default\u0026rdquo; group, or\nSettingDef.builder().key(otherKey).group(\u0026quot;common\u0026quot;).build()  as a definition that is listed in the \u0026ldquo;common\u0026rdquo; group.\n Any group category will generate a new \u0026ldquo;tab\u0026rdquo; in Ohara-Manager.   The value of each definition will be kept in environment of stream running container, and you should set the value by stream api.\n Metrics When a stream application is running, Ohara automatically collects some metrics data from the stream in the background. The metrics data here means official metrics which contains Counters for now (other type of metrics will be introduced in the future). The metrics data could be fetched by Stream APIs. Developers will be able to implement their own custom metrics in the foreseeable future.\nOhara leverages JMX to offer the metrics data to stream. It means that all metrics you have created are stored as Java beans and accessible through JMX service. The stream will expose a port via Stream APIs for other JMX client tool used, such as JMC, but we still encourage you to use Stream APIs as it offers a more readable format of metrics.\nOfficial Metrics There are two type of official metrics for stream:\n consumed topic records (counter) produced topic records (counter)  A normal stream will connect to two topics, one is the source topic that stream will consume from, and the other is the target topic that stream will produce to. We use prefix words (TOPIC_IN, TOPIC_OUT) in the response data ( Stream APIs) in order to improve readabilities of those types. You don\u0026rsquo;t need to worry about the implementation of these official metrics, but you can still read the source code to see how Ohara creates official metrics.\n Logs Will be implemented in the near future. Also see issue: #962\n","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"e339c85d9f84196559a604c20850f9be","permalink":"https://oharastream.github.io/en/docs/0.11.x/custom_stream/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/custom_stream/","section":"docs","summary":"Ohara stream is an unparalleled wrap of kafka streams which gives you a straightforward thought to design your streaming flow. It offers a simple way to implement and define actions to process data between topics.","tags":null,"title":"Custom Stream Guideline","type":"docs"},{"authors":null,"categories":null,"content":"Ohara stream is an unparalleled wrap of kafka streams which gives you a straightforward thought to design your streaming flow. It offers a simple way to implement and define actions to process data between topics. You only have to write your logic in start() method and compile your code to a jar file. After jar file is compiled successfully, you can deploy your jar file to Ohara, run it, and monitor your stream by logs and metrics.\nThe following sections will describe how to write a stream application in Ohara.\n Ohara Stream Overview Ohara stream is a wrap of kafka streams and provided an entry of interface class Stream to define user custom streaming code. A normal stream application will use Row as data type to interactive topics in Ohara.\nBefore writing your stream, you should download the ohara dependencies first. Ohara includes many powerful tools for developer but not all tools are requisite in designing stream. The required dependencies are shown below.\nrepositories { maven { url \u0026quot;https://dl.bintray.com/oharastream/ohara\u0026quot; } } implementation \u0026quot;oharastream.ohara:ohara-stream:0.11.0-SNAPSHOT\u0026quot; implementation \u0026quot;oharastream.ohara:ohara-common:0.11.0-SNAPSHOT\u0026quot; implementation \u0026quot;oharastream.ohara:ohara-kafka:0.11.0-SNAPSHOT\u0026quot;   The releases page shows the available version of ohara    Stream Entry  We will automatically find your custom class which should be extended by oharastream.ohara.stream.Stream.\nIn Ohara environment, the required parameters are defined in Ohara UI. You only need to initial the OStream as following:\nOStream\u0026lt;Row\u0026gt; ostream = OStream.builder().toOharaEnvStream();  A base implementation for a custom stream only need to include start() method, but you could include other methods which are described below for your convenience.\nThe following example is a simple stream application which can run in Ohara. Note that this example simply starts the stream application without doing any transformation but writing data, i.e., the target topic will have same data as the source topic.\npublic class SimpleApplicationForOharaEnv extends Stream { @Override public void start(OStream\u0026lt;Row\u0026gt; ostream, StreamDefinitions streamSetting) { ostream.start(); } }   The following methods we provided belongs to Ohara Stream, which has many powerful and friendly features. Native Kafka Streams API does not have these methods.   init() method After we find the user custom class, the first method will be called by Stream is init(). This is an optional method that can be used for user to initialize some external data source connections or input parameters.\nconfig() method In a stream application, you may want to configure your own parameters. We support a method here to help you define a custom streamSetting list in stream. The details of streamSetting are list here.\nIn the following example, we want to add a custom definition which is used to define \u0026ldquo;join topic\u0026rdquo;:\n@Override public StreamDefinitions config() { return StreamDefinitions // add a definition of \u0026quot;filter name\u0026quot; in \u0026quot;default\u0026quot; group .with(SettingDef.builder().key(\u0026quot;filterName\u0026quot;).group(\u0026quot;default\u0026quot;).build()); }  After define the definition, you can use it in start() method\n This method is optional. We will append all the definitions you provide in this method to the stream default definitions. That is, the absent config() method means you only need the default definitions.   start(OStream, StreamDefinitions) method This method will be called after init(). Normally, you could only define start() method for most cases in Ohara. We encourage user to use source connector (see Source Connector section) for importing external data source to Ohara and use topic data as custom stream data source in start() method.\nWe provide two arguments in this method:\n  OStream \u0026mdash; the entry class of ohara stream\nOStream (a.k.a. ohara stream) helps you to construct your application and use all the powerful APIs in Stream.\n  StreamDefinitions \u0026mdash; the definitions of ohara stream\nfrom the definition you can use StreamDefinitions.string() to get the value from the config method.\n   The return value is wrap in a Java object Optional, you need to decide whether the value is present or not.   For example:\n@Override public void start(OStream\u0026lt;Row\u0026gt; ostream, StreamDefinitions streamSetting) { ostream .map(row -\u0026gt; Row.of(row.cell(\u0026quot;name\u0026quot;), row.cell(\u0026quot;age\u0026quot;))) // use the previous defined definition in config() .filter(row -\u0026gt; row.cell(streamSetting.string(\u0026quot;filterName\u0026quot;).get()).value() != null) .map(row -\u0026gt; Row.of(Cell.of(\u0026quot;name\u0026quot;, row.cell(\u0026quot;name\u0026quot;).value().toString().toUpperCase()))) .start(); }  The above code does the following transformations:\n  pick cell of the header: name, age from each row\n  filter out that if filterName is null \u0026mdash; here we get the value from filterName of definitions. the value you should update by Stream update api\nPUT /v0/streams/XXX\n{ \u0026quot;filterName\u0026quot;: \u0026quot;name\u0026quot; }    convert the cell of name to upperCase\n  From now on, you can use the Stream Java API to design your own application, happy coding!\nStream Java API  In Stream, we provide three different classes for developers:\n OStream: define the functions for operating streaming data (each row record one-by-one) OGroupedStream: define the functions for operating grouped streaming data OTable: define the functions for operating table data (changelog for same key of row record)  The above classes will be auto converted when you use the correspond functions; You should not worry about the usage of which class is right to use. All the starting point of development is just OStream.\nBelow we list the available functions in each class (See more information in javadoc):\nOStream   constructTable(String topicName)\nCreate a OTable with specified topicName from current OStream.\n  filter(Predicate predicate)\nCreate a new OStream that filter by the given predicate.\n  through(String topicName, int partitions)\nTransfer this OStream to specify topic and use the required partition number.\n  leftJoin(String joinTopicName, Conditions conditions, ValueJoiner joiner)\nJoin this OStream with required joinTopicName and conditions.\n  map(ValueMapper mapper)\nTransform the value of each record to a new value of the output record.\n  groupByKey(List keys)\nGroup the records by key to a OGroupedStream.\n  foreach(ForeachAction action)\nPerform an action on each record of OStream.\n  start()\nRun this stream application.\n  stop()\nStop this stream application.\n  describe()\nDescribe the topology of this stream.\n  getPoneglyph()\nGet the Ohara format Poneglyph from topology.\n  OGroupedStream   count()\nCount the number of records in this OGroupedStream and return the count value.\n  reduce(final Reducer reducer)\nCombine the values of each record in this OGroupedStream by the grouped key.\n  OTable   toOStream()\nConvert this OTable to OStream\n   Stream Examples Below we provide some examples that demonstrate how to develop your own stream applications. More description of each example could be found in javadoc.\n  WordCount: count the words in \u0026ldquo;word\u0026rdquo; column  PageViewRegion: count the views by each region  Sum: sum odd numbers in \u0026ldquo;number\u0026rdquo; column   Stream Definitions Stream stores a list of SettingDef, which is StreamDefinitions, in the data store. By default, we will keep the following definitions in the \u0026ldquo;core\u0026rdquo; group and generate the definition in stream API :\n DefaultConfigs.BROKER_DEFINITION : The broker list DefaultConfigs.IMAGE_NAME_DEFINITION : The image name DefaultConfigs.NAME_DEFINITION : The stream application name DefaultConfigs.GROUP_DEFINITION : The stream group name DefaultConfigs.FROM_TOPICS_DEFINITION : The from topic DefaultConfigs.TO_TOPICS_DEFINITION : The to topic DefaultConfigs.JMX_PORT_DEFINITION : The exposed jmx port DefaultConfigs.NODE_NAMES_DEFINITION : The node name list DefaultConfigs.VERSION_DEFINITION : The version of stream DefaultConfigs.REVISION_DEFINITION : The revision of stream DefaultConfigs.AUTHOR_DEFINITION : The author of stream DefaultConfigs.TAGS_DEFINITION : The tags of stream  Any other definition except above list will be treated as a custom definition. You can define:\nSettingDef.builder().key(joinTopic).group(\u0026quot;default\u0026quot;).build()  as a definition that is listed in \u0026ldquo;default\u0026rdquo; group, or\nSettingDef.builder().key(otherKey).group(\u0026quot;common\u0026quot;).build()  as a definition that is listed in the \u0026ldquo;common\u0026rdquo; group.\n Any group category will generate a new \u0026ldquo;tab\u0026rdquo; in Ohara-Manager.   The value of each definition will be kept in environment of stream running container, and you should set the value by stream api.\n Metrics When a stream application is running, Ohara automatically collects some metrics data from the stream in the background. The metrics data here means official metrics which contains Counters for now (other type of metrics will be introduced in the future). The metrics data could be fetched by Stream APIs. Developers will be able to implement their own custom metrics in the foreseeable future.\nOhara leverages JMX to offer the metrics data to stream. It means that all metrics you have created are stored as Java beans and accessible through JMX service. The stream will expose a port via Stream APIs for other JMX client tool used, such as JMC, but we still encourage you to use Stream APIs as it offers a more readable format of metrics.\nOfficial Metrics There are two type of official metrics for stream:\n consumed topic records (counter) produced topic records (counter)  A normal stream will connect to two topics, one is the source topic that stream will consume from, and the other is the target topic that stream will produce to. We use prefix words (TOPIC_IN, TOPIC_OUT) in the response data ( Stream APIs) in order to improve readabilities of those types. You don\u0026rsquo;t need to worry about the implementation of these official metrics, but you can still read the source code to see how Ohara creates official metrics.\n Logs Will be implemented in the near future. Also see issue: #962\n","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"31e3bb889d71b9797a334a9319c415a9","permalink":"https://oharastream.github.io/en/docs/master/custom_stream/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/custom_stream/","section":"docs","summary":"Ohara stream is an unparalleled wrap of kafka streams which gives you a straightforward thought to design your streaming flow. It offers a simple way to implement and define actions to process data between topics.","tags":null,"title":"Custom Stream Guideline","type":"docs"},{"authors":null,"categories":null,"content":"Ohara encourages user to write custom application if the official applications can satisfy requirements for your use case. Jar APIs is a useful entry of putting your jar on Ohara and then start related services with it. For example, Worker APIs accept a sharedJarKeys element which can carry the jar name pointing to an existent jar in Ohara. The worker cluster will load all connectors of the input jar, and then you are able to use the connectors on the worker cluster.\nThe File API upload jar file to use by the Worker and Stream.\n The file used by a worker or stream can\u0026rsquo;t be either updated or deleted.   The properties stored by Ohara are shown below.\n name (string) \u0026mdash; the file name without extension. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; the group name (we use this field to separate different workspaces). The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; size (long) \u0026mdash; file size url (option(string)) \u0026mdash; url to download this jar from Ohara Configurator. Noted not all jars are downloadable to user. lastModified (long) \u0026mdash; the time of uploading this file tags (object) \u0026mdash; the user defined parameters bytes (array(object)) \u0026mdash; read file content to bytes classInfos (array(object)) \u0026mdash; the information of available classes in this file  classInfos[i].className \u0026mdash; the name of this class classInfos[i].classType \u0026mdash; the type of this class. for example, topic, source connector, sink connector or stream app classInfos[i].settingDefinitions \u0026mdash; the definitions of this class     The field \u0026ldquo;classInfos\u0026rdquo; is empty if the file is NOT a valid jar.   upload a file to Ohara Upload a file to Ohara with field name : \u0026ldquo;jar\u0026rdquo; and group name : \u0026ldquo;group\u0026rdquo; the text field \u0026ldquo;group\u0026rdquo; could be empty and we will generate a random string.\nPOST /v0/files\n Example Request Content-Type: multipart/form-data file=\u0026quot;ohara-it-stream.jar\u0026quot; group=\u0026quot;default\u0026quot; tags={}     You have to specify the file name since it is a part of metadata stored by Ohara. Noted, the later uploaded file can overwrite the older one     Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:12345/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578967196525, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list all jars Get all jars from specific group of query parameter. If no query parameter, wll return all jars.\nGET /v0/files?group=default\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:5000/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578973197877, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; }, ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a file Delete a file with specific name and group. Note: the query parameter must exist.\nDELETE /v0/files/$name?group=default\n Example Response 204 NoContent     It is ok to delete a nonexistent jar, and the response is 204 NoContent. If you delete a file is used by other services, you also break the scalability of service as you can\u0026rsquo;t run the jar on any new nodes   get a file Get a file with specific name and group. Note: the query parameter must exists.\nGET /v0/files/$name?group=default\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:5000/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578973197877, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update tags of file PUT /v0/files/$name?group=default\n Example Response { \u0026quot;tags\u0026quot;: { \u0026quot;a\u0026quot;: \u0026quot;b\u0026quot; } }     it returns error code if input group/name are not associated to an existent file.    Example Response { \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:5000/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578974415307, \u0026quot;tags\u0026quot;: { \u0026quot;a\u0026quot;: \u0026quot;b\u0026quot; }, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"da1d2df1f34f723768d5b7b8bb1e1fa6","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/files/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/files/","section":"docs","summary":"Ohara encourages user to write custom application if the official applications can satisfy requirements for your use case. Jar APIs is a useful entry of putting your jar on Ohara and then start related services with it.","tags":null,"title":"Files","type":"docs"},{"authors":null,"categories":null,"content":"Ohara encourages user to write custom application if the official applications can satisfy requirements for your use case. Jar APIs is a useful entry of putting your jar on Ohara and then start related services with it. For example, Worker APIs accept a sharedJarKeys element which can carry the jar name pointing to an existent jar in Ohara. The worker cluster will load all connectors of the input jar, and then you are able to use the connectors on the worker cluster.\nThe File API upload jar file to use by the Worker and Stream.\n The file used by a worker or stream can\u0026rsquo;t be either updated or deleted.   The properties stored by Ohara are shown below.\n name (string) \u0026mdash; the file name without extension. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; the group name (we use this field to separate different workspaces). The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; size (long) \u0026mdash; file size url (option(string)) \u0026mdash; url to download this jar from Ohara Configurator. Noted not all jars are downloadable to user. lastModified (long) \u0026mdash; the time of uploading this file tags (object) \u0026mdash; the user defined parameters bytes (array(object)) \u0026mdash; read file content to bytes classInfos (array(object)) \u0026mdash; the information of available classes in this file  classInfos[i].className \u0026mdash; the name of this class classInfos[i].classType \u0026mdash; the type of this class. for example, topic, source connector, sink connector or stream app classInfos[i].settingDefinitions \u0026mdash; the definitions of this class     The field \u0026ldquo;classInfos\u0026rdquo; is empty if the file is NOT a valid jar.   upload a file to Ohara Upload a file to Ohara with field name : \u0026ldquo;jar\u0026rdquo; and group name : \u0026ldquo;group\u0026rdquo; the text field \u0026ldquo;group\u0026rdquo; could be empty and we will generate a random string.\nPOST /v0/files\n Example Request Content-Type: multipart/form-data file=\u0026quot;ohara-it-stream.jar\u0026quot; group=\u0026quot;default\u0026quot; tags={}     You have to specify the file name since it is a part of metadata stored by Ohara. Noted, the later uploaded file can overwrite the older one     Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:12345/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578967196525, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list all jars Get all jars from specific group of query parameter. If no query parameter, wll return all jars.\nGET /v0/files?group=default\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:5000/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578973197877, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; }, ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a file Delete a file with specific name and group. Note: the query parameter must exist.\nDELETE /v0/files/$name?group=default\n Example Response 204 NoContent     It is ok to delete a nonexistent jar, and the response is 204 NoContent. If you delete a file is used by other services, you also break the scalability of service as you can\u0026rsquo;t run the jar on any new nodes   get a file Get a file with specific name and group. Note: the query parameter must exists.\nGET /v0/files/$name?group=default\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:5000/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578973197877, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update tags of file PUT /v0/files/$name?group=default\n Example Response { \u0026quot;tags\u0026quot;: { \u0026quot;a\u0026quot;: \u0026quot;b\u0026quot; } }     it returns error code if input group/name are not associated to an existent file.    Example Response { \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;size\u0026quot;: 1896, \u0026quot;url\u0026quot;: \u0026quot;http://localhost:5000/v0/downloadFiles/default/ohara-it-stream.jar\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578974415307, \u0026quot;tags\u0026quot;: { \u0026quot;a\u0026quot;: \u0026quot;b\u0026quot; }, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;BROKER_CLUSTER\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;Broker cluster key\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the key of broker cluster used to transfer data for this stream\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;OBJECT_KEY\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 0, \u0026quot;key\u0026quot;: \u0026quot;brokerClusterKey\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"c309de3ac9e0896fb04bc1ef95ff4088","permalink":"https://oharastream.github.io/en/docs/master/rest-api/files/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/files/","section":"docs","summary":"Ohara encourages user to write custom application if the official applications can satisfy requirements for your use case. Jar APIs is a useful entry of putting your jar on Ohara and then start related services with it.","tags":null,"title":"Files","type":"docs"},{"authors":null,"categories":null,"content":"Inspect APIs is a powerful tool that it enable you to \u0026ldquo;see\u0026rdquo; what in the target via Configurator. For example, you can get the definitions from specific image, or you can see what connectors or stream app in the specific file.\nget Ohara Configurator info GET /v0/inspect/configurator\nthe format of response of Ohara Configurator is shown below.\n versionInfo (object) \u0026mdash; version details of Ohara Configurator  branch (string) \u0026mdash; the branch name of Ohara Configurator version (string) \u0026mdash; the release version of Ohara Configurator revision (string) \u0026mdash; commit hash of Ohara Configurator. You can trace the hash code via Github user (string) \u0026mdash; the release manager of Ohara Configurator. date (string) \u0026mdash; the date of releasing Ohara Configurator.   mode (string) \u0026mdash; the mode of this configurator. There are three modes now:  K8S: k8s mode is for the production. SSH: ssh is useful to simple env. FAKE: fake mode is used to test APIs.     Example Response { \u0026quot;versionInfo\u0026quot;: { \u0026quot;branch\u0026quot;: \u0026quot;0.10.0\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;date\u0026quot;: \u0026quot;2020-01-08 06:05:47\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;root\u0026quot; }, \u0026quot;mode\u0026quot;: \u0026quot;K8S\u0026quot; }    get zookeeper/broker/worker/stream info GET /v0/inspect/$service\nThis API used to fetch the definitions for specific cluster service. The following fields are returned.\n imageName (string) \u0026mdash; the image name of service settingDefinitions (array(object)) \u0026mdash; the available settings for this service (see setting)  the available variables for $service are shown below.\n zookeeper broker worker stream   Example Response { \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;peerPort\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the port exposed to each quorum\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;RANDOM_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;BINDING_PORT\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 10, \u0026quot;key\u0026quot;: \u0026quot;peerPort\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; }, ], \u0026quot;classInfos\u0026quot;: [] }    get running zookeeper/broker/worker/stream info GET /v0/inspect/$service/$name?group=$group\nThis API used to fetch the definitions for specific cluster service and the definitions of available classes in the service. The following fields are returned.\n imageName (string) \u0026mdash; the image name of service settingDefinitions (array(object)) \u0026mdash; the available settings for this service (see setting) classInfos (array(object)) \u0026mdash; the information available classes in this service  classInfos[i].className \u0026mdash; the name of this class classInfos[i].classType \u0026mdash; the type of this class. for example, topic, source connector, sink connector or stream app classInfos[i].settingDefinitions \u0026mdash; the definitions of this class    the available variables for $service are shown below.\n zookeeper broker worker stream   Example Response { \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;xmx\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;maximum memory allocation (in MB)\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;POSITIVE_LONG\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 8, \u0026quot;key\u0026quot;: \u0026quot;xmx\u0026quot;, \u0026quot;defaultValue\u0026quot;: 1024, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ], \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;numberOfPartitions\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the number of partitions\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;POSITIVE_INT\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 4, \u0026quot;key\u0026quot;: \u0026quot;numberOfPartitions\u0026quot;, \u0026quot;defaultValue\u0026quot;: 1, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ] }    Query Database POST /v0/inspect/rdb\nThis API returns the table details of a relational database. This API invokes a running connector on worker cluster to fetch database information and return to Ohara Configurator. You should deploy suitable jdbc driver on worker cluster before using this API. Otherwise, you will get a exception returned by Ohara Configurator. The query consists of following fields.\n url (string) \u0026mdash; jdbc url user (string) \u0026mdash; user who can access target database password (string) \u0026mdash; password which can access target database workerClusterKey (Object) \u0026mdash; target worker cluster.  workerClusterKey.group (option(string)) \u0026mdash; the group of cluster workerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   catalogPattern (option(string)) \u0026mdash; filter returned tables according to catalog schemaPattern (option(string)) \u0026mdash; filter returned tables according to schema tableName (option(string)) \u0026mdash; filter returned tables according to name    Example Request\n{ \u0026quot;url\u0026quot;: \u0026quot;jdbc:postgresql://localhost:5432/postgres\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;ohara\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;123456\u0026quot;, \u0026quot;workerClusterKey\u0026quot;: \u0026quot;wk00\u0026quot; }    Example Response\n name (string) \u0026mdash; database name tables (array(object))  tables[i].catalogPattern (option(object)) \u0026mdash; table\u0026rsquo;s catalog pattern tables[i].schemaPattern (option(object)) \u0026mdash; table\u0026rsquo;s schema pattern tables[i].name (option(object)) \u0026mdash; table\u0026rsquo;s name tables[i].columns (array(object)) \u0026mdash; table\u0026rsquo;s columns  tables[i].columns[j].name (string) \u0026mdash; column\u0026rsquo;s columns tables[i].columns[j].dataType (string) \u0026mdash; column\u0026rsquo;s data type tables[i].columns[j].pk (boolean) \u0026mdash; true if this column is pk. otherwise false      { \u0026quot;name\u0026quot;: \u0026quot;postgresql\u0026quot;, \u0026quot;tables\u0026quot;: [ { \u0026quot;schemaPattern\u0026quot;: \u0026quot;public\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;table1\u0026quot;, \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;column1\u0026quot;, \u0026quot;dataType\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;pk\u0026quot;: false }, { \u0026quot;name\u0026quot;: \u0026quot;column2\u0026quot;, \u0026quot;dataType\u0026quot;: \u0026quot;varchar\u0026quot;, \u0026quot;pk\u0026quot;: true } ] } ] }    Query Topic POST /v0/inspect/topic/$name?group=$group\u0026amp;timeout=$timeout\u0026amp;limit=$limit\nFetch the latest data from a topic. the query arguments are shown below.\n timeout (long) \u0026mdash; break the fetch if this timeout is reached limit (long) \u0026mdash; the number of messages in topic  the response includes following items.\n messages (Array(Object)) \u0026mdash; messages   messages[i].partition (int) \u0026mdash; the index of partition messages[i].offset (Long) \u0026mdash; the offset of this message messages[i].sourceClass (Option(String)) \u0026mdash; class name of the component which generate this data messages[i].sourceKey (Option(Object)) \u0026mdash; object key of the component which generate this data messages[i].value (Option(Object)) \u0026mdash; the value of this message messages[i].error (Option(String)) \u0026mdash; error message happen in failing to parse value   Example Response { \u0026quot;messages\u0026quot;: [ { \u0026quot;sourceKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot; }, \u0026quot;sourceClass\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSourceTask\u0026quot;, \u0026quot;partition\u0026quot;: 0, \u0026quot;offset\u0026quot;: 0, \u0026quot;value\u0026quot;: { \u0026quot;a\u0026quot;: \u0026quot;c54e2f3477\u0026quot;, \u0026quot;b\u0026quot;: \u0026quot;32ae422fb5\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;53e448ab80\u0026quot;, \u0026quot;tags\u0026quot;: [] } } ] }    Query File  name (string) \u0026mdash; the file name without extension group (string) \u0026mdash; the group name (we use this field to separate different workspaces) size (long) \u0026mdash; file size tags (object) \u0026mdash; the extra description to this object lastModified (long) \u0026mdash; the time of uploading this file classInfos (array(object)) \u0026mdash; the information of available classes in this file   classInfos[i].className \u0026mdash; the name of this class classInfos[i].classType \u0026mdash; the type of this class. for example, topic, source connector, sink connector or stream app classInfos[i].settingDefinitions \u0026mdash; the definitions of this class  POST /v0/inspect/files\n  Example Request\nContent-Type: multipart/form-data file=\u0026quot;ohara-it-sink.jar\u0026quot; group=\u0026quot;default\u0026quot;    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;ohara-it-sink.jar\u0026quot;, \u0026quot;size\u0026quot;: 7902, \u0026quot;lastModified\u0026quot;: 1579055900202, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;sink\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.connector.IncludeAllTypesSinkConnector\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;kind\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;READ_ONLY\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;kind of connector\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;STRING\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 13, \u0026quot;key\u0026quot;: \u0026quot;kind\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;sink\u0026quot;, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"1c935b4f731c1b645ea6b6299272f4f4","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/inspect/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/inspect/","section":"docs","summary":"Inspect APIs is a powerful tool that it enable you to \u0026ldquo;see\u0026rdquo; what in the target via Configurator. For example, you can get the definitions from specific image, or you can see what connectors or stream app in the specific file.","tags":null,"title":"Inspect","type":"docs"},{"authors":null,"categories":null,"content":"Inspect APIs is a powerful tool that it enable you to \u0026ldquo;see\u0026rdquo; what in the target via Configurator. For example, you can get the definitions from specific image, or you can see what connectors or stream app in the specific file.\nget Ohara Configurator info GET /v0/inspect/configurator\nthe format of response of Ohara Configurator is shown below.\n versionInfo (object) \u0026mdash; version details of Ohara Configurator  branch (string) \u0026mdash; the branch name of Ohara Configurator version (string) \u0026mdash; the release version of Ohara Configurator revision (string) \u0026mdash; commit hash of Ohara Configurator. You can trace the hash code via Github user (string) \u0026mdash; the release manager of Ohara Configurator. date (string) \u0026mdash; the date of releasing Ohara Configurator.   mode (string) \u0026mdash; the mode of this configurator. There are three modes now:  K8S: k8s mode is for the production. SSH: ssh is useful to simple env. FAKE: fake mode is used to test APIs.     Example Response { \u0026quot;versionInfo\u0026quot;: { \u0026quot;branch\u0026quot;: \u0026quot;0.10.0\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;date\u0026quot;: \u0026quot;2020-01-08 06:05:47\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;root\u0026quot; }, \u0026quot;mode\u0026quot;: \u0026quot;K8S\u0026quot; }    get zookeeper/broker/worker/stream info GET /v0/inspect/$service\nThis API used to fetch the definitions for specific cluster service. The following fields are returned.\n imageName (string) \u0026mdash; the image name of service settingDefinitions (array(object)) \u0026mdash; the available settings for this service (see setting)  the available variables for $service are shown below.\n zookeeper broker worker stream   Example Response { \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;peerPort\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the port exposed to each quorum\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;RANDOM_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;BINDING_PORT\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 10, \u0026quot;key\u0026quot;: \u0026quot;peerPort\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; }, ], \u0026quot;classInfos\u0026quot;: [] }    get running zookeeper/broker/worker/stream info GET /v0/inspect/$service/$name?group=$group\nThis API used to fetch the definitions for specific cluster service and the definitions of available classes in the service. The following fields are returned.\n imageName (string) \u0026mdash; the image name of service settingDefinitions (array(object)) \u0026mdash; the available settings for this service (see setting) classInfos (array(object)) \u0026mdash; the information available classes in this service  classInfos[i].className \u0026mdash; the name of this class classInfos[i].classType \u0026mdash; the type of this class. for example, topic, source connector, sink connector or stream app classInfos[i].settingDefinitions \u0026mdash; the definitions of this class    the available variables for $service are shown below.\n zookeeper broker worker stream   Example Response { \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/broker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;xmx\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;maximum memory allocation (in MB)\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;POSITIVE_LONG\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 8, \u0026quot;key\u0026quot;: \u0026quot;xmx\u0026quot;, \u0026quot;defaultValue\u0026quot;: 1024, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ], \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;numberOfPartitions\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;the number of partitions\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;POSITIVE_INT\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 4, \u0026quot;key\u0026quot;: \u0026quot;numberOfPartitions\u0026quot;, \u0026quot;defaultValue\u0026quot;: 1, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ] }    Query Database POST /v0/inspect/rdb\nThis API returns the table details of a relational database. This API invokes a running connector on worker cluster to fetch database information and return to Ohara Configurator. You should deploy suitable jdbc driver on worker cluster before using this API. Otherwise, you will get a exception returned by Ohara Configurator. The query consists of following fields.\n url (string) \u0026mdash; jdbc url user (string) \u0026mdash; user who can access target database password (string) \u0026mdash; password which can access target database workerClusterKey (Object) \u0026mdash; target worker cluster.  workerClusterKey.group (option(string)) \u0026mdash; the group of cluster workerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   catalogPattern (option(string)) \u0026mdash; filter returned tables according to catalog schemaPattern (option(string)) \u0026mdash; filter returned tables according to schema tableName (option(string)) \u0026mdash; filter returned tables according to name    Example Request\n{ \u0026quot;url\u0026quot;: \u0026quot;jdbc:postgresql://localhost:5432/postgres\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;ohara\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;123456\u0026quot;, \u0026quot;workerClusterKey\u0026quot;: \u0026quot;wk00\u0026quot; }    Example Response\n name (string) \u0026mdash; database name tables (array(object))  tables[i].catalogPattern (option(object)) \u0026mdash; table\u0026rsquo;s catalog pattern tables[i].schemaPattern (option(object)) \u0026mdash; table\u0026rsquo;s schema pattern tables[i].name (option(object)) \u0026mdash; table\u0026rsquo;s name tables[i].columns (array(object)) \u0026mdash; table\u0026rsquo;s columns  tables[i].columns[j].name (string) \u0026mdash; column\u0026rsquo;s columns tables[i].columns[j].dataType (string) \u0026mdash; column\u0026rsquo;s data type tables[i].columns[j].pk (boolean) \u0026mdash; true if this column is pk. otherwise false      { \u0026quot;name\u0026quot;: \u0026quot;postgresql\u0026quot;, \u0026quot;tables\u0026quot;: [ { \u0026quot;schemaPattern\u0026quot;: \u0026quot;public\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;table1\u0026quot;, \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;column1\u0026quot;, \u0026quot;dataType\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;pk\u0026quot;: false }, { \u0026quot;name\u0026quot;: \u0026quot;column2\u0026quot;, \u0026quot;dataType\u0026quot;: \u0026quot;varchar\u0026quot;, \u0026quot;pk\u0026quot;: true } ] } ] }    Query Topic POST /v0/inspect/topic/$name?group=$group\u0026amp;timeout=$timeout\u0026amp;limit=$limit\nFetch the latest data from a topic. the query arguments are shown below.\n timeout (long) \u0026mdash; break the fetch if this timeout is reached limit (long) \u0026mdash; the number of messages in topic  the response includes following items.\n messages (Array(Object)) \u0026mdash; messages   messages[i].partition (int) \u0026mdash; the index of partition messages[i].offset (Long) \u0026mdash; the offset of this message messages[i].sourceClass (Option(String)) \u0026mdash; class name of the component which generate this data messages[i].sourceKey (Option(Object)) \u0026mdash; object key of the component which generate this data messages[i].value (Option(Object)) \u0026mdash; the value of this message messages[i].error (Option(String)) \u0026mdash; error message happen in failing to parse value   Example Response { \u0026quot;messages\u0026quot;: [ { \u0026quot;sourceKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot; }, \u0026quot;sourceClass\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSourceTask\u0026quot;, \u0026quot;partition\u0026quot;: 0, \u0026quot;offset\u0026quot;: 0, \u0026quot;value\u0026quot;: { \u0026quot;a\u0026quot;: \u0026quot;c54e2f3477\u0026quot;, \u0026quot;b\u0026quot;: \u0026quot;32ae422fb5\u0026quot;, \u0026quot;c\u0026quot;: \u0026quot;53e448ab80\u0026quot;, \u0026quot;tags\u0026quot;: [] } } ] }    Query File  name (string) \u0026mdash; the file name without extension group (string) \u0026mdash; the group name (we use this field to separate different workspaces) size (long) \u0026mdash; file size tags (object) \u0026mdash; the extra description to this object lastModified (long) \u0026mdash; the time of uploading this file classInfos (array(object)) \u0026mdash; the information of available classes in this file   classInfos[i].className \u0026mdash; the name of this class classInfos[i].classType \u0026mdash; the type of this class. for example, topic, source connector, sink connector or stream app classInfos[i].settingDefinitions \u0026mdash; the definitions of this class  POST /v0/inspect/files\n  Example Request\nContent-Type: multipart/form-data file=\u0026quot;ohara-it-sink.jar\u0026quot; group=\u0026quot;default\u0026quot;    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;ohara-it-sink.jar\u0026quot;, \u0026quot;size\u0026quot;: 7902, \u0026quot;lastModified\u0026quot;: 1579055900202, \u0026quot;tags\u0026quot;: {}, \u0026quot;classInfos\u0026quot;: [ { \u0026quot;classType\u0026quot;: \u0026quot;sink\u0026quot;, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.it.connector.IncludeAllTypesSinkConnector\u0026quot;, \u0026quot;settingDefinitions\u0026quot;: [ { \u0026quot;blacklist\u0026quot;: [], \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;kind\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;READ_ONLY\u0026quot;, \u0026quot;documentation\u0026quot;: \u0026quot;kind of connector\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;STRING\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;orderInGroup\u0026quot;: 13, \u0026quot;key\u0026quot;: \u0026quot;kind\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;sink\u0026quot;, \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot; } ] } ], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"9d42a137824e4dd692b99f1c3496a6e5","permalink":"https://oharastream.github.io/en/docs/master/rest-api/inspect/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/inspect/","section":"docs","summary":"Inspect APIs is a powerful tool that it enable you to \u0026ldquo;see\u0026rdquo; what in the target via Configurator. For example, you can get the definitions from specific image, or you can see what connectors or stream app in the specific file.","tags":null,"title":"Inspect","type":"docs"},{"authors":null,"categories":null,"content":"This world is beautiful but not safe. Even though Ohara shoulders the blame for simplifying your life, there is a slim chance that something don\u0026rsquo;t work well in Ohara. The Logs APIs, which are engineers\u0026rsquo; best friend, open a door to observe the logs of running cluster.\nThe available query parameters are shown below.\n sinceSeconds (long) \u0026mdash; show the logs since relative time  It collects output from all containers\u0026rsquo; of a cluster and then format them to JSON representation which has following elements.\n clusterKey (object) \u0026mdash; cluster key  clusterKey.group (string) \u0026mdash; cluster group clusterKey.name (string) \u0026mdash; cluster name   logs (array(object)) \u0026mdash; log of each container  logs[i].hostname \u0026mdash; hostname logs[i].value \u0026mdash; total log of a container    get the log of a running cluster GET /v0/logs/$clusterType/$clusterName?group=$group\n clusterType (string)  zookeepers brokers workers streams shabondis     Example Response { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot; }, \u0026quot;logs\u0026quot;: [ { \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2020-01-14 10:15:42,146 [myid:] - INFO [main:QuorumPeerConfig@136\u0026quot; } ] }    get the since seconds log GET /v0/logs/$clusterType/$clusterName?group=$group\u0026amp;sinceSeconds=$relativeSeconds\n Example Response { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot; }, \u0026quot;logs\u0026quot;: [ { \u0026quot;hostname\u0026quot;: \u0026quot;ohara-release-test-00\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2020-01-15 02:00:43,090 [myid:] - INFO [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100000761180000 type:setData cxid:0x11a zxid:0x9e txntype:-1 reqpath:n/a Error Path:/config/topics/default-topic0 Error:KeeperErrorCode = NoNode for /config/topics/default-topic0\\n\u0026quot; } ] }    get the log of Configurator GET /v0/logs/configurator\n Example Response { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; }, \u0026quot;logs\u0026quot;: [ { \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2020-01-10 09:43:02,669 INFO [main] configurator.Configurator$(391): start a configurator built on hostname:ohara-release-test-00 and port:5000\\n2020-01-10 09:43:02,676 INFO [main] configurator.Configurator$(393): enter ctrl+c to terminate the configurator\u0026quot; } ] }     the Configurator MUST run on docker container and the node hosting Configurator MUST be added to Configurator via Node APIs   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"ebcd0eb499e395c5bc68a051d3d74088","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/logs/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/logs/","section":"docs","summary":"This world is beautiful but not safe. Even though Ohara shoulders the blame for simplifying your life, there is a slim chance that something don\u0026rsquo;t work well in Ohara. The Logs APIs, which are engineers\u0026rsquo; best friend, open a door to observe the logs of running cluster.","tags":null,"title":"Logs","type":"docs"},{"authors":null,"categories":null,"content":"This world is beautiful but not safe. Even though Ohara shoulders the blame for simplifying your life, there is a slim chance that something don\u0026rsquo;t work well in Ohara. The Logs APIs, which are engineers\u0026rsquo; best friend, open a door to observe the logs of running cluster.\nThe available query parameters are shown below.\n sinceSeconds (long) \u0026mdash; show the logs since relative time  It collects output from all containers\u0026rsquo; of a cluster and then format them to JSON representation which has following elements.\n clusterKey (object) \u0026mdash; cluster key  clusterKey.group (string) \u0026mdash; cluster group clusterKey.name (string) \u0026mdash; cluster name   logs (array(object)) \u0026mdash; log of each container  logs[i].hostname \u0026mdash; hostname logs[i].value \u0026mdash; total log of a container    get the log of a running cluster GET /v0/logs/$clusterType/$clusterName?group=$group\n clusterType (string)  zookeepers brokers workers streams shabondis     Example Response { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot; }, \u0026quot;logs\u0026quot;: [ { \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2020-01-14 10:15:42,146 [myid:] - INFO [main:QuorumPeerConfig@136\u0026quot; } ] }    get the since seconds log GET /v0/logs/$clusterType/$clusterName?group=$group\u0026amp;sinceSeconds=$relativeSeconds\n Example Response { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot; }, \u0026quot;logs\u0026quot;: [ { \u0026quot;hostname\u0026quot;: \u0026quot;ohara-release-test-00\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2020-01-15 02:00:43,090 [myid:] - INFO [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100000761180000 type:setData cxid:0x11a zxid:0x9e txntype:-1 reqpath:n/a Error Path:/config/topics/default-topic0 Error:KeeperErrorCode = NoNode for /config/topics/default-topic0\\n\u0026quot; } ] }    get the log of Configurator GET /v0/logs/configurator\n Example Response { \u0026quot;clusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; }, \u0026quot;logs\u0026quot;: [ { \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2020-01-10 09:43:02,669 INFO [main] configurator.Configurator$(391): start a configurator built on hostname:ohara-release-test-00 and port:5000\\n2020-01-10 09:43:02,676 INFO [main] configurator.Configurator$(393): enter ctrl+c to terminate the configurator\u0026quot; } ] }     the Configurator MUST run on docker container and the node hosting Configurator MUST be added to Configurator via Node APIs   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"85c0925d8d9f80a80545c91498931a96","permalink":"https://oharastream.github.io/en/docs/master/rest-api/logs/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/logs/","section":"docs","summary":"This world is beautiful but not safe. Even though Ohara shoulders the blame for simplifying your life, there is a slim chance that something don\u0026rsquo;t work well in Ohara. The Logs APIs, which are engineers\u0026rsquo; best friend, open a door to observe the logs of running cluster.","tags":null,"title":"Logs","type":"docs"},{"authors":null,"categories":null,"content":"Node is the basic unit of running service. It can be either physical machine or vm. In section zookeeper, Broker and Worker, you will see many requests demanding you to fill the node name to build the services. Currently, Ohara requires the node added to Ohara should pre-install following services.\n docker (18.09+) ssh server k8s (only if you want to k8s to host containers) official Ohara images   oharastream/zookeeper  oharastream/broker  oharastream/connect-worker  oharastream/stream    The version (tag) depends on which Ohara you used. It would be better to use the same version to Ohara. For example, the version of Ohara configurator you are running is 0.11.0-SNAPSHOT, then the official images you should download is oharastream/xxxx:0.11.0-SNAPSHOT.\nThe properties used by describing a node are shown below.\n  hostname (string) \u0026mdash; hostname of node. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo;\nThis hostname must be available on you DNS. It will cause a lot of troubles if Ohara Configurator is unable to connect to remote node via this hostname.\n  port (int) \u0026mdash; ssh port of node\n  user (string) \u0026mdash; ssh account\n  password (string) \u0026mdash; ssh password\n  tags (object) \u0026mdash; the extra description to this object\n  The following information are updated at run-time.\n services (array(object)) \u0026mdash; the services hosted by this node  services[i].name (string) \u0026mdash; service name (configurator, zookeeper, broker, connect-worker and stream) services[i].clusterKeys (array(object)) \u0026mdash; the keys of this service   resources (array(object)) \u0026mdash; the available resources of this node  resources[i].name (string) \u0026mdash; the resource name resources[i].value (number) \u0026mdash; the \u0026ldquo;pure\u0026rdquo; number of resource resources[i].used (option(double)) \u0026mdash; the used \u0026ldquo;value\u0026rdquo; in percentage. Noted: this value may be null if the impl is unable to calculate the used resource. resources[i].unit (string) \u0026mdash; the description of the \u0026ldquo;value\u0026rdquo; unit   state (String) \u0026mdash; \u0026ldquo;available\u0026rdquo; means this node works well. otherwise, \u0026ldquo;unavailable\u0026rdquo; is returned error (option(String)) \u0026mdash; the description to the unavailable node lastModified (long) \u0026mdash; the last time to update this node  store a node POST /v0/nodes\n hostname (string) \u0026mdash; hostname of node port (int) \u0026mdash; ssh port of node user (string) \u0026mdash; ssh account password (string) \u0026mdash; ssh password    Example Request\n{ \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;port\u0026quot;: 22, \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    Example Response\n{ \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 22, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    update a node PUT /v0/nodes/${name}\n hostname (string) \u0026mdash; hostname of node port (int) \u0026mdash; ssh port of node user (string) \u0026mdash; ssh account password (string) \u0026mdash; ssh password   Example Request { \u0026quot;port\u0026quot;: 9999 }     An new node will be created if your input name does not exist    the update request will clear the validation report attached to this node    Example Response { \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 9999, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    list all nodes stored in Ohara GET /v0/nodes\n Example Response [ { \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 22, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; } ]    delete a node DELETE /v0/nodes/${name}\n Example Response 204 NoContent     It is ok to delete a nonexistent pipeline, and the response is 204 NoContent. However, it is disallowed to remove a node which is running service. If you do want to delete the node from Ohara, please stop all services from the node.   get a node GET /v0/nodes/${name}\n Example Response { \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 22, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"f3113be8911dc9813bec6e0176061a92","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/nodes/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/nodes/","section":"docs","summary":"Node is the basic unit of running service. It can be either physical machine or vm. In section zookeeper, Broker and Worker, you will see many requests demanding you to fill the node name to build the services.","tags":null,"title":"Node","type":"docs"},{"authors":null,"categories":null,"content":"Node is the basic unit of running service. It can be either physical machine or vm. In section zookeeper, Broker and Worker, you will see many requests demanding you to fill the node name to build the services. Currently, Ohara requires the node added to Ohara should pre-install following services.\n docker (18.09+) ssh server k8s (only if you want to k8s to host containers) official Ohara images   oharastream/zookeeper  oharastream/broker  oharastream/connect-worker  oharastream/stream    The version (tag) depends on which Ohara you used. It would be better to use the same version to Ohara. For example, the version of Ohara configurator you are running is 0.11.0-SNAPSHOT, then the official images you should download is oharastream/xxxx:0.11.0-SNAPSHOT.\nThe properties used by describing a node are shown below.\n  hostname (string) \u0026mdash; hostname of node. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo;\nThis hostname must be available on you DNS. It will cause a lot of troubles if Ohara Configurator is unable to connect to remote node via this hostname.\n  port (int) \u0026mdash; ssh port of node\n  user (string) \u0026mdash; ssh account\n  password (string) \u0026mdash; ssh password\n  tags (object) \u0026mdash; the extra description to this object\n  The following information are updated at run-time.\n services (array(object)) \u0026mdash; the services hosted by this node  services[i].name (string) \u0026mdash; service name (configurator, zookeeper, broker, connect-worker and stream) services[i].clusterKeys (array(object)) \u0026mdash; the keys of this service   resources (array(object)) \u0026mdash; the available resources of this node  resources[i].name (string) \u0026mdash; the resource name resources[i].value (number) \u0026mdash; the \u0026ldquo;pure\u0026rdquo; number of resource resources[i].used (option(double)) \u0026mdash; the used \u0026ldquo;value\u0026rdquo; in percentage. Noted: this value may be null if the impl is unable to calculate the used resource. resources[i].unit (string) \u0026mdash; the description of the \u0026ldquo;value\u0026rdquo; unit   state (String) \u0026mdash; \u0026ldquo;available\u0026rdquo; means this node works well. otherwise, \u0026ldquo;unavailable\u0026rdquo; is returned error (option(String)) \u0026mdash; the description to the unavailable node lastModified (long) \u0026mdash; the last time to update this node  store a node POST /v0/nodes\n hostname (string) \u0026mdash; hostname of node port (int) \u0026mdash; ssh port of node user (string) \u0026mdash; ssh account password (string) \u0026mdash; ssh password    Example Request\n{ \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;port\u0026quot;: 22, \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    Example Response\n{ \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 22, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    update a node PUT /v0/nodes/${name}\n hostname (string) \u0026mdash; hostname of node port (int) \u0026mdash; ssh port of node user (string) \u0026mdash; ssh account password (string) \u0026mdash; ssh password   Example Request { \u0026quot;port\u0026quot;: 9999 }     An new node will be created if your input name does not exist    the update request will clear the validation report attached to this node    Example Response { \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 9999, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    list all nodes stored in Ohara GET /v0/nodes\n Example Response [ { \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 22, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; } ]    delete a node DELETE /v0/nodes/${name}\n Example Response 204 NoContent     It is ok to delete a nonexistent pipeline, and the response is 204 NoContent. However, it is disallowed to remove a node which is running service. If you do want to delete the node from Ohara, please stop all services from the node.   get a node GET /v0/nodes/${name}\n Example Response { \u0026quot;services\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;zookeeper\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;broker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;connect-worker\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [] }, { \u0026quot;name\u0026quot;: \u0026quot;configurator\u0026quot;, \u0026quot;clusterKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;N/A\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;node00\u0026quot; } ] } ], \u0026quot;hostname\u0026quot;: \u0026quot;node00\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;AVAILABLE\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578627668686, \u0026quot;tags\u0026quot;: {}, \u0026quot;port\u0026quot;: 22, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;CPU\u0026quot;, \u0026quot;value\u0026quot;: 6.0, \u0026quot;unit\u0026quot;: \u0026quot;cores\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;Memory\u0026quot;, \u0026quot;value\u0026quot;: 10.496479034423828, \u0026quot;unit\u0026quot;: \u0026quot;GB\u0026quot; } ], \u0026quot;user\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;pwd\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"9267619c4a76b634556b5a8294ddf145","permalink":"https://oharastream.github.io/en/docs/master/rest-api/nodes/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/nodes/","section":"docs","summary":"Node is the basic unit of running service. It can be either physical machine or vm. In section zookeeper, Broker and Worker, you will see many requests demanding you to fill the node name to build the services.","tags":null,"title":"Node","type":"docs"},{"authors":null,"categories":null,"content":"Object APIs offer a way to store anything to Configurator. It is useful when you have something temporary and you have no other way to store them.\nSimilar to other APIs, the required fields are \u0026ldquo;name\u0026rdquo; and \u0026ldquo;group\u0026rdquo;.\n name (string) \u0026mdash; name of object. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; group of object. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; tags (option(object)) \u0026mdash; the extra description to this object  The following information are updated at run-time.\n lastModified (long) \u0026mdash; the last time to update this node  store a object POST /v0/objects\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot; }    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579071742763, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update a object PUT /v0/objects/${name}\n  Example Request\n{ \u0026quot;k0\u0026quot;: \u0026quot;v0\u0026quot; }    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k0\u0026quot;: \u0026quot;v0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579072298657, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list all objects GET /v0/objects\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k0\u0026quot;: \u0026quot;v1000000\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579072345437, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a node DELETE /v0/objects/${name}\n Example Response 204 NoContent    get a object GET /v0/objects/${name}\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k0\u0026quot;: \u0026quot;v0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579072345437, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"2cbfe80f2568c95c683b604df46119aa","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/objects/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/objects/","section":"docs","summary":"Object APIs offer a way to store anything to Configurator. It is useful when you have something temporary and you have no other way to store them.\nSimilar to other APIs, the required fields are \u0026ldquo;name\u0026rdquo; and \u0026ldquo;group\u0026rdquo;.","tags":null,"title":"Object","type":"docs"},{"authors":null,"categories":null,"content":"Object APIs offer a way to store anything to Configurator. It is useful when you have something temporary and you have no other way to store them.\nSimilar to other APIs, the required fields are \u0026ldquo;name\u0026rdquo; and \u0026ldquo;group\u0026rdquo;.\n name (string) \u0026mdash; name of object. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; group of object. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; tags (option(object)) \u0026mdash; the extra description to this object  The following information are updated at run-time.\n lastModified (long) \u0026mdash; the last time to update this node  store a object POST /v0/objects\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot; }    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579071742763, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update a object PUT /v0/objects/${name}\n  Example Request\n{ \u0026quot;k0\u0026quot;: \u0026quot;v0\u0026quot; }    Example Response\n{ \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k0\u0026quot;: \u0026quot;v0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579072298657, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list all objects GET /v0/objects\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k0\u0026quot;: \u0026quot;v1000000\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579072345437, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a node DELETE /v0/objects/${name}\n Example Response 204 NoContent    get a object GET /v0/objects/${name}\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;n0\u0026quot;, \u0026quot;k0\u0026quot;: \u0026quot;v0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1579072345437, \u0026quot;tags\u0026quot;: {}, \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"6b9b63199aea90d3c4d84d4e23de6473","permalink":"https://oharastream.github.io/en/docs/master/rest-api/objects/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/objects/","section":"docs","summary":"Object APIs offer a way to store anything to Configurator. It is useful when you have something temporary and you have no other way to store them.\nSimilar to other APIs, the required fields are \u0026ldquo;name\u0026rdquo; and \u0026ldquo;group\u0026rdquo;.","tags":null,"title":"Object","type":"docs"},{"authors":null,"categories":null,"content":"Pipeline APIs are born of Ohara manager which needs a way to store the relationship of components in streaming. The relationship in pipeline is made up of multi endpoints. Each endpoint describe a component. For example, you have a topic as source so you can describe the relationship via following endpoints.\n{ \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ] }  The objects grouped by pipeline should be existent. Otherwise, pipeline will ignore them in generating object abstracts.\nThe objects grouped by pipeline don\u0026rsquo;t need to located on the same cluster hierarchy. Grouping a topic, which is placed at broker_0, and a topic, which is located at broker_1, is valid. However, the object based on a dead cluster will get an abstract with error state.\nThe properties used in generating pipeline are shown below.\n group (string) \u0026mdash; pipeline\u0026rsquo;s group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; name (string) \u0026mdash; pipeline\u0026rsquo;s name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; endpoints (array(object)) \u0026mdash; the relationship between objects  endpoints[i].group (String) \u0026mdash; the group of this endpoint endpoints[i].name (String) \u0026mdash; the name of this endpoint endpoints[i].kind (String) \u0026mdash; the kind of this endpoint   tags (object) \u0026mdash; the extra description to this object  Following information are written by Ohara.\n lastModified (long) \u0026mdash; the last time to update this pipeline objects (array(object)) \u0026mdash; the abstract of all objects mentioned by pipeline  objects[i].name (string) \u0026mdash; object\u0026rsquo;s name objects[i].kind (string) \u0026mdash; the type of this object. for instance, topic, connector, and stream objects[i].className (string) \u0026mdash; object\u0026rsquo;s implementation. Normally, it shows the full name of a java class objects[i].state (option(string)) \u0026mdash; the state of object. If the object can\u0026rsquo;t have state (eg, topic), you won\u0026rsquo;t see this field objects[i].error (option(string)) \u0026mdash; the error message of this object objects[i].lastModified (long) \u0026mdash; the last time to update this object  nodeMetrics (object) \u0026mdash; the metrics from this object. Not all objects in pipeline have metrics! meters (array(object)) \u0026mdash; the metrics in meter type meters[i].value (double) \u0026mdash; the number stored in meter meters[i].unit (string) \u0026mdash; unit for value meters[i].document (string) \u0026mdash; document of this meter meters[i].queryTime (long) \u0026mdash; the time of query metrics from remote machine meters[i].startTime (option(long)) \u0026mdash; the time of record generated in remote machine   jarKeys (array(object)) \u0026mdash; the jars used by the objects in pipeline.  create a pipeline POST /v0/pipelines\nThe following example creates a pipeline with a topic and connector. The topic is created on broker cluster but the connector isn\u0026rsquo;t. Hence, the response from server shows that it fails to find the status of the connector. That is to say, it is ok to add un-running connector to pipeline.\nAllow setting the not exists object for the endpoint. The object resposne value is empty.\n  Example Request 1 - Running single topic example\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ] }    Example Response 1\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578639344607, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578635914746, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    Example Request 2 - Running topic and perf connector example\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline1\u0026quot;, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; }, { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;connector\u0026quot; } ] }    Example Response 2\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline1\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578649709850, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; }, { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;connector\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578649564486, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578649620960, \u0026quot;tags\u0026quot;: {}, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update a pipeline PUT /v0/pipelines/$name\n Example Request { \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ] }     This API creates a new pipeline for you if the input name does not exist!    Example Response { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641282237, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641231579, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list all pipelines GET /v0/pipelines\nListing all pipelines is a expensive operation as it invokes a iteration to all objects stored in pipeline. The loop will do a lot of checks and fetch status, metrics and log from backend clusters. If you have the name of pipeline, please use GET to fetch details of single pipeline.\nthe accepted query keys are listed below.\n group name jarKeys lastModified tags   Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641282237, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641231579, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    GET /v0/pipelines?name=${pipelineName}\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578647223700, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a pipeline DELETE /v0/pipelines/$name\nDeleting a pipeline does not delete the objects related to the pipeline.\n Example Response 204 NoContent     It is ok to delete a nonexistent pipeline, and the response is 204 NoContent. However, it is illegal to remove a pipeline having any running objects   get a pipeline GET /v0/pipelines/$name\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578647223700, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }     The field \u0026ldquo;objects\u0026rdquo; displays only existent endpoints.   refresh a pipeline PUT /v0/pipelines/$name/refresh\nRequires Ohara Configurator to cleanup nonexistent objects of pipeline. Pipeline is a group of objects and it contains, sometimes, some nonexistent objects. Those nonexistent objects won\u0026rsquo;t hurt our services but it may be ugly and weird to read. Hence, the (helper) API do a background cleanup for your pipeline. The cleanup rules are shown below.\n the endpoint having nonexistent \u0026ldquo;from\u0026rdquo; is removed the objects in \u0026ldquo;to\u0026rdquo; get removed   Example Response 202 Accepted     You should use Get pipeline to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"bae771487936211e470769e08c5df5d7","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/pipelines/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/pipelines/","section":"docs","summary":"Pipeline APIs are born of Ohara manager which needs a way to store the relationship of components in streaming. The relationship in pipeline is made up of multi endpoints. Each endpoint describe a component.","tags":null,"title":"Pipeline","type":"docs"},{"authors":null,"categories":null,"content":"Pipeline APIs are born of Ohara manager which needs a way to store the relationship of components in streaming. The relationship in pipeline is made up of multi endpoints. Each endpoint describe a component. For example, you have a topic as source so you can describe the relationship via following endpoints.\n{ \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ] }  The objects grouped by pipeline should be existent. Otherwise, pipeline will ignore them in generating object abstracts.\nThe objects grouped by pipeline don\u0026rsquo;t need to located on the same cluster hierarchy. Grouping a topic, which is placed at broker_0, and a topic, which is located at broker_1, is valid. However, the object based on a dead cluster will get an abstract with error state.\nThe properties used in generating pipeline are shown below.\n group (string) \u0026mdash; pipeline\u0026rsquo;s group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; name (string) \u0026mdash; pipeline\u0026rsquo;s name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; endpoints (array(object)) \u0026mdash; the relationship between objects  endpoints[i].group (String) \u0026mdash; the group of this endpoint endpoints[i].name (String) \u0026mdash; the name of this endpoint endpoints[i].kind (String) \u0026mdash; the kind of this endpoint   tags (object) \u0026mdash; the extra description to this object  Following information are written by Ohara.\n lastModified (long) \u0026mdash; the last time to update this pipeline objects (array(object)) \u0026mdash; the abstract of all objects mentioned by pipeline  objects[i].name (string) \u0026mdash; object\u0026rsquo;s name objects[i].kind (string) \u0026mdash; the type of this object. for instance, topic, connector, and stream objects[i].className (string) \u0026mdash; object\u0026rsquo;s implementation. Normally, it shows the full name of a java class objects[i].state (option(string)) \u0026mdash; the state of object. If the object can\u0026rsquo;t have state (eg, topic), you won\u0026rsquo;t see this field objects[i].error (option(string)) \u0026mdash; the error message of this object objects[i].lastModified (long) \u0026mdash; the last time to update this object  nodeMetrics (object) \u0026mdash; the metrics from this object. Not all objects in pipeline have metrics! meters (array(object)) \u0026mdash; the metrics in meter type meters[i].value (double) \u0026mdash; the number stored in meter meters[i].unit (string) \u0026mdash; unit for value meters[i].document (string) \u0026mdash; document of this meter meters[i].queryTime (long) \u0026mdash; the time of query metrics from remote machine meters[i].startTime (option(long)) \u0026mdash; the time of record generated in remote machine   jarKeys (array(object)) \u0026mdash; the jars used by the objects in pipeline.  create a pipeline POST /v0/pipelines\nThe following example creates a pipeline with a topic and connector. The topic is created on broker cluster but the connector isn\u0026rsquo;t. Hence, the response from server shows that it fails to find the status of the connector. That is to say, it is ok to add un-running connector to pipeline.\nAllow setting the not exists object for the endpoint. The object resposne value is empty.\n  Example Request 1 - Running single topic example\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ] }    Example Response 1\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578639344607, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578635914746, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    Example Request 2 - Running topic and perf connector example\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline1\u0026quot;, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; }, { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;connector\u0026quot; } ] }    Example Response 2\n{ \u0026quot;name\u0026quot;: \u0026quot;pipeline1\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578649709850, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; }, { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;connector\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578649564486, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;perf\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578649620960, \u0026quot;tags\u0026quot;: {}, \u0026quot;className\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    update a pipeline PUT /v0/pipelines/$name\n Example Request { \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ] }     This API creates a new pipeline for you if the input name does not exist!    Example Response { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641282237, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641231579, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }    list all pipelines GET /v0/pipelines\nListing all pipelines is a expensive operation as it invokes a iteration to all objects stored in pipeline. The loop will do a lot of checks and fetch status, metrics and log from backend clusters. If you have the name of pipeline, please use GET to fetch details of single pipeline.\nthe accepted query keys are listed below.\n group name jarKeys lastModified tags   Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641282237, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578641231579, \u0026quot;tags\u0026quot;: {}, \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    GET /v0/pipelines?name=${pipelineName}\n Example Response [ { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578647223700, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; } ]    delete a pipeline DELETE /v0/pipelines/$name\nDeleting a pipeline does not delete the objects related to the pipeline.\n Example Response 204 NoContent     It is ok to delete a nonexistent pipeline, and the response is 204 NoContent. However, it is illegal to remove a pipeline having any running objects   get a pipeline GET /v0/pipelines/$name\n Example Response { \u0026quot;name\u0026quot;: \u0026quot;pipeline0\u0026quot;, \u0026quot;lastModified\u0026quot;: 1578647223700, \u0026quot;endpoints\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;topic\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;objects\u0026quot;: [], \u0026quot;jarKeys\u0026quot;: [], \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot; }     The field \u0026ldquo;objects\u0026rdquo; displays only existent endpoints.   refresh a pipeline PUT /v0/pipelines/$name/refresh\nRequires Ohara Configurator to cleanup nonexistent objects of pipeline. Pipeline is a group of objects and it contains, sometimes, some nonexistent objects. Those nonexistent objects won\u0026rsquo;t hurt our services but it may be ugly and weird to read. Hence, the (helper) API do a background cleanup for your pipeline. The cleanup rules are shown below.\n the endpoint having nonexistent \u0026ldquo;from\u0026rdquo; is removed the objects in \u0026ldquo;to\u0026rdquo; get removed   Example Response 202 Accepted     You should use Get pipeline to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"d48687fd2705d3f96a5035979a4286f9","permalink":"https://oharastream.github.io/en/docs/master/rest-api/pipelines/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/pipelines/","section":"docs","summary":"Pipeline APIs are born of Ohara manager which needs a way to store the relationship of components in streaming. The relationship in pipeline is made up of multi endpoints. Each endpoint describe a component.","tags":null,"title":"Pipeline","type":"docs"},{"authors":null,"categories":null,"content":"Shabondi service play the role of a http proxy service in the Pipeline. Just like connector, there have two kinds of Shabondi service:\n Source: Shabondi source service receives data from http request and then writes the data into the linked topic. Sink: Users can use http request to read topic data through Shabondi sink service  The following are common setting both Shabondi source and sink:\n name (string) \u0026mdash; the value of group is always \u0026ldquo;default\u0026rdquo;. The legal character is number, lowercase alphanumeric characters, or . group (string) \u0026mdash; the name of this connector. The legal character is number, lowercase alphanumeric characters, or . brokerClusterKey (object) \u0026mdash; the broker cluster used to store data generated by this worker cluster  brokerClusterKey.group (option(string)) \u0026mdash; the group of cluster brokerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   shabondi.class (string) \u0026mdash; class name of Shabondi service, only the following two class names are legal:  Shabondi source: oharastream.ohara.shabondi.ShabondiSource Shabondi sink: oharastream.ohara.shabondi.ShabondiSink   shabondi.client.port (int) - The Shabondi service client port imageName (string) \u0026mdash; docker image nodeNames array(string) - The nodes that running this Shabondi. Currently, Shabondi not support multiple nodes deployment. So nodeNames only can be contained one node when start shabondi service, otherwise you\u0026rsquo;ll get an error. tags (object) \u0026mdash; the extra parameter for this object routes (object) - jmxPort (int) - the JVM jmx port for Shabondi service xms (int) - the initial memory allocation pool for JVM xmx (int) - the maximum memory allocation pool for JVM author (string) \u0026mdash; revision (string) \u0026mdash; version (string) \u0026mdash;  Shabondi Source only settings\n shabondi.source.toTopics (array(object)) - The topic which Shabondi source service would write the data to.  Shabondi Sink only settings\n shabondi.sink.fromTopics (array(object)) - The topic which Shabondi sink service would read the data from. shabondi.sink.group.idletime (duration) - The maximum idle time of each sink data group. shabondi.sink.poll.timeout(duration) - The maximum time of each consumer poll.  The following information are updated by Ohara\n lastModified (long) \u0026mdash; the last time to update this shabondi service state (option(string)) \u0026mdash; the state of a started shabondi service aliveNodes (Set(string)) \u0026mdash; the nodes hosting this shabondi service error (option(string)) \u0026mdash; the error message from a failed shabondi service. If the a shabondi service is fine or un-started, you won\u0026rsquo;t get this field.  nodeMetrics (object) \u0026mdash; the metrics from a running connector  meters (array(object)) \u0026mdash; the metrics in meter type  meters[i].name (string) \u0026mdash; the number of this meter (normally, it is unique) meters[i].value (double) \u0026mdash; the value in double meters[i].valueInPerSec (double) \u0026mdash; the average value in per second meters[i].unit (string) \u0026mdash; the unit of value meters[i].document (string) \u0026mdash; human-readable description to this meter meters[i].queryTime (Long) \u0026mdash; the time we query this meter from remote nodes meters[i].startTime (Long) \u0026mdash; the time to start this meter (not all services offer this record) meters[i].lastModified (Long) \u0026mdash; the time of modifying metrics      List settings of all Shabondi services GET /v0/shabondis\n Example Response [ { \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1587100361274, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;The number of received rows\u0026quot;, \u0026quot;lastModified\u0026quot;: 1587100347637, \u0026quot;name\u0026quot;: \u0026quot;total-rows\u0026quot;, \u0026quot;queryTime\u0026quot;: 1587100360577, \u0026quot;startTime\u0026quot;: 1587100347637, \u0026quot;unit\u0026quot;: \u0026quot;row\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 } ] } }, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;jmxPort\u0026quot;: 56586, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    Create the settings of a Shabondi service POST /v0/shabondis\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;,\u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;} ], \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Example Response\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1587101035977, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: {}, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 56726, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Get the settings of a Shabondi service GET /v0/shabondis/${name}?group=${group}\n Example Response { \u0026quot;author\u0026quot;: \u0026quot;vitojeng\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1587101035977, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: {}, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 56726, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Update the settings of a Shabondi service PUT /v0/shabondis/${name}?group=${group}\n  Example Request\n{ \u0026quot;shabondi.client.port\u0026quot;: 96456 }    Example Response\n{ \u0026quot;author\u0026quot;: \u0026quot;vitojeng\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1587106367767, \u0026quot;shabondi.client.port\u0026quot;: 38400, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: {}, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 56726, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Delete the settings of Shabondi DELETE /v0/shabondis/${name}?group=${group}\n Example Response 202 Accepted     It is ok to delete an nonexistent properties, and the response is 204 NoContent.   Start a Shabondi service PUT /v0/shabondis/${name}/start?group=${group}\n Example Response 202 Accepted     You should use get shabondi to fetch up-to-date status   Stop a Shabondi service PUT /v0/shabondis/${name}/stop?group=${group}\n Example Response 202 Accepted     You should use get shabondi to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"168b0646aae2aea50e8d1377f8bc9979","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/shabondis/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/shabondis/","section":"docs","summary":"Shabondi service play the role of a http proxy service in the Pipeline. Just like connector, there have two kinds of Shabondi service:\n Source: Shabondi source service receives data from http request and then writes the data into the linked topic.","tags":null,"title":"Shabondi","type":"docs"},{"authors":null,"categories":null,"content":"Shabondi service play the role of a http proxy service in the Pipeline. Just like connector, there have two kinds of Shabondi service:\n Source: Shabondi source service receives data from http request and then writes the data into the linked topic. Sink: Users can use http request to read topic data through Shabondi sink service  The following are common setting both Shabondi source and sink:\n name (string) \u0026mdash; the value of group is always \u0026ldquo;default\u0026rdquo;. The legal character is number, lowercase alphanumeric characters, or . group (string) \u0026mdash; the name of this connector. The legal character is number, lowercase alphanumeric characters, or . brokerClusterKey (object) \u0026mdash; the broker cluster used to store data generated by this worker cluster  brokerClusterKey.group (option(string)) \u0026mdash; the group of cluster brokerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   shabondi.class (string) \u0026mdash; class name of Shabondi service, only the following two class names are legal:  Shabondi source: oharastream.ohara.shabondi.ShabondiSource Shabondi sink: oharastream.ohara.shabondi.ShabondiSink   shabondi.client.port (int) - The Shabondi service client port imageName (string) \u0026mdash; docker image nodeNames array(string) - The nodes that running this Shabondi. Currently, Shabondi not support multiple nodes deployment. So nodeNames only can be contained one node when start shabondi service, otherwise you\u0026rsquo;ll get an error. tags (object) \u0026mdash; the extra parameter for this object routes (object) - jmxPort (int) - the JVM jmx port for Shabondi service xms (int) - the initial memory allocation pool for JVM xmx (int) - the maximum memory allocation pool for JVM author (string) \u0026mdash; revision (string) \u0026mdash; version (string) \u0026mdash;  Shabondi Source only settings\n shabondi.source.toTopics (array(object)) - The topic which Shabondi source service would write the data to.  Shabondi Sink only settings\n shabondi.sink.fromTopics (array(object)) - The topic which Shabondi sink service would read the data from. shabondi.sink.group.idletime (duration) - The maximum idle time of each sink data group. shabondi.sink.poll.timeout(duration) - The maximum time of each consumer poll.  The following information are updated by Ohara\n lastModified (long) \u0026mdash; the last time to update this shabondi service state (option(string)) \u0026mdash; the state of a started shabondi service aliveNodes (Set(string)) \u0026mdash; the nodes hosting this shabondi service error (option(string)) \u0026mdash; the error message from a failed shabondi service. If the a shabondi service is fine or un-started, you won\u0026rsquo;t get this field.  nodeMetrics (object) \u0026mdash; the metrics from a running connector  meters (array(object)) \u0026mdash; the metrics in meter type  meters[i].name (string) \u0026mdash; the number of this meter (normally, it is unique) meters[i].value (double) \u0026mdash; the value in double meters[i].valueInPerSec (double) \u0026mdash; the average value in per second meters[i].unit (string) \u0026mdash; the unit of value meters[i].document (string) \u0026mdash; human-readable description to this meter meters[i].queryTime (Long) \u0026mdash; the time we query this meter from remote nodes meters[i].startTime (Long) \u0026mdash; the time to start this meter (not all services offer this record) meters[i].lastModified (Long) \u0026mdash; the time of modifying metrics      List settings of all Shabondi services GET /v0/shabondis\n Example Response [ { \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;lastModified\u0026quot;: 1587100361274, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;The number of received rows\u0026quot;, \u0026quot;lastModified\u0026quot;: 1587100347637, \u0026quot;name\u0026quot;: \u0026quot;total-rows\u0026quot;, \u0026quot;queryTime\u0026quot;: 1587100360577, \u0026quot;startTime\u0026quot;: 1587100347637, \u0026quot;unit\u0026quot;: \u0026quot;row\u0026quot;, \u0026quot;value\u0026quot;: 0.0, \u0026quot;valueInPerSec\u0026quot;: 0.0 } ] } }, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;jmxPort\u0026quot;: 56586, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    Create the settings of a Shabondi service POST /v0/shabondis\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;,\u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;} ], \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Example Response\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1587101035977, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: {}, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 56726, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Get the settings of a Shabondi service GET /v0/shabondis/${name}?group=${group}\n Example Response { \u0026quot;author\u0026quot;: \u0026quot;vitojeng\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1587101035977, \u0026quot;shabondi.client.port\u0026quot;: 58456, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: {}, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 56726, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Update the settings of a Shabondi service PUT /v0/shabondis/${name}?group=${group}\n  Example Request\n{ \u0026quot;shabondi.client.port\u0026quot;: 96456 }    Example Response\n{ \u0026quot;author\u0026quot;: \u0026quot;vitojeng\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;shabondi0\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1587106367767, \u0026quot;shabondi.client.port\u0026quot;: 38400, \u0026quot;shabondi.source.toTopics\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;shabondi.class\u0026quot;: \u0026quot;oharastream.ohara.shabondi.ShabondiSource\u0026quot;, \u0026quot;nodeMetrics\u0026quot;: {}, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/shabondi:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;revision\u0026quot;: \u0026quot;7cb25202c5308095546e5a6a2b96480d9d3104e1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 56726, \u0026quot;kind\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group0\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Delete the settings of Shabondi DELETE /v0/shabondis/${name}?group=${group}\n Example Response 202 Accepted     It is ok to delete an nonexistent properties, and the response is 204 NoContent.   Start a Shabondi service PUT /v0/shabondis/${name}/start?group=${group}\n Example Response 202 Accepted     You should use get shabondi to fetch up-to-date status   Stop a Shabondi service PUT /v0/shabondis/${name}/stop?group=${group}\n Example Response 202 Accepted     You should use get shabondi to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"6d02e8abce1a952096eebf61288523ba","permalink":"https://oharastream.github.io/en/docs/master/rest-api/shabondis/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/shabondis/","section":"docs","summary":"Shabondi service play the role of a http proxy service in the Pipeline. Just like connector, there have two kinds of Shabondi service:\n Source: Shabondi source service receives data from http request and then writes the data into the linked topic.","tags":null,"title":"Shabondi","type":"docs"},{"authors":null,"categories":null,"content":"Ohara Stream is a unparalleled wrap of kafka streaming. It leverages and enhances Kafka Streams to make developer easily design and implement the streaming application. More details of developing streaming application is in custom stream guideline.\nAssume that you have completed a streaming application via Ohara Java APIs, and you have generated a jar including your streaming code. By Ohara Restful APIs, you are enable to control, deploy, and monitor your streaming application. As with cluster APIs, Ohara leverages docker container to host streaming application. Of course, you can apply your favor container management tool including simple (based on ssh) and k8s when you are starting Ohara.\nBefore stating to use restful APIs, please ensure that all nodes have downloaded the Stream image. The jar you uploaded to run streaming application will be included in the image and then executes as a docker container. The Stream image is kept in each node so don\u0026rsquo;t worry about the network. We all hate re-download everything when running services.\n If you implement the Ohara stream application, you must pack to jar file and use the File API upload jar file then setting the jarKey to create the Stream API.   The following information of Stream are updated by Ohara.\nstream stored data The following are common settings to a stream app.\n name (string) \u0026mdash; cluster name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; cluster group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; jarKey (object) \u0026mdash; the used jar key jmxPort (int) \u0026mdash; expose port for jmx className (string) \u0026mdash; the class to be executed. This field is optional and Configurator will pick up a class from the input jar. However, it throw exception if there are many available classes in the jar file. from (array(TopicKey)) \u0026mdash; source topic to (array(TopicKey)) \u0026mdash; target topic nodeNames (array(string)) \u0026mdash; the nodes running the stream process brokerClusterKey (object) \u0026mdash; the broker cluster key used for stream running  brokerClusterKey.group (option(string)) \u0026mdash; the group of broker cluster brokerClusterKey.name (string) \u0026mdash; the name of broker cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   tags (object) \u0026mdash; the user defined parameters aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of stream cluster state (option(string)) \u0026mdash; only started/failed stream has state (DEAD if all containers are not running, else RUNNING) error (option(string)) \u0026mdash; the error message from a failed stream. If the stream is fine or un-started, you won\u0026rsquo;t get this field.  nodeMetrics (object) \u0026mdash; the metrics from this stream.  meters (array(object)) \u0026mdash; the metrics in meter type  meters[i].value (double) \u0026mdash; the number stored in meter meters[i].unit (string) \u0026mdash; unit for value meters[i].document (string) \u0026mdash; document of this meter meters[i].queryTime (long) \u0026mdash; the time of query metrics from remote machine meters[i].startTime (option(long)) \u0026mdash; the time of record generated in remote machine     lastModified (long) \u0026mdash; last modified this jar time  create properties of specific stream Create the properties of a stream.\nPOST /v0/streams\n name (string) \u0026mdash; new stream name. This is the object unique name ; default is random string. group (string) \u0026mdash; group name for current stream ; default value is \u0026ldquo;default\u0026rdquo; imageName (string) \u0026mdash; image name of stream used to ; default is oharastream/stream:0.11.0-SNAPSHOT nodeNames (array(string)) \u0026mdash; node name list of stream used to ; default is empty tags (object) \u0026mdash; a key-value map of user defined data ; default is empty jarKey (object) \u0026mdash; the used jar key  group (string) \u0026mdash; the group name of this jar name (string) \u0026mdash; the name of this jar   brokerClusterKey (option(object)) \u0026mdash; the broker cluster used for stream running ; default we will auto fill this parameter for you if you don\u0026rsquo;t specify it and there only exists one broker cluster. jmxPort (int) \u0026mdash; expose port for jmx ; default is random port from (array(TopicKey)) \u0026mdash; source topic ; default is empty array   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nWe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n  to (array(TopicKey)) \u0026mdash; target topic ; default is empty array   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nWe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n    Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: \u0026quot;bk\u0026quot;, \u0026quot;jarKey\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;], \u0026quot;from\u0026quot;: [\u0026quot;topic0\u0026quot;], \u0026quot;to\u0026quot;: [\u0026quot;topic1\u0026quot;] }    Example Response\nResponse format is as stream stored format.\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579145546218, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }     The stream, which is just created, does not have any metrics.   get information from a specific stream cluster GET /v0/streams/${name}?group=$group\n We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.     Example Response\nResponse format is as stream stored format.\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579145546218, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    list information of stream cluster GET /v0/streams\nthe accepted query keys are listed below\n author group name lastModified tags state aliveNodes key version    Example Response\nResponse format is as stream stored format.\n[ { \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579145546218, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    update properties of specific stream Update the properties of a non-started stream.\nPUT /v0/streams/${name}?group=$group\n If the required stream (group, name) was not exists, we will try to use this request as create stream    imageName (option(string)) \u0026mdash; image name of stream used to. nodeNames (option(array(string))) \u0026mdash; node name list of stream used to. tags (option(object)) \u0026mdash; a key-value map of user defined data. jarKey (option(option(object))) \u0026mdash; the used jar key  group (option(string)) \u0026mdash; the group name of this jar name (option(string)) \u0026mdash; the name without extension of this jar   jmxPort (option(int)) \u0026mdash; expose port for jmx. from (option(array(string))) \u0026mdash; source topic.   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nwe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n  to (option(array(string))) \u0026mdash; target topic.   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nwe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n    Example Request\n{ \u0026quot;from\u0026quot;: [\u0026quot;topic2\u0026quot;], \u0026quot;to\u0026quot;: [\u0026quot;topic3\u0026quot;], \u0026quot;jarKey\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node01\u0026quot;] }    Example Response\nResponse format is as stream stored format.\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579153777586, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic3\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic2\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node01\u0026quot; ] }    delete properties of specific stream Delete the properties of a non-started stream. This api only remove the stream component which is stored in pipeline.\nDELETE /v0/streams/${name}?group=$group\n We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.    Example Response 204 NoContent     It is ok to delete an nonexistent properties, and the response is 204 NoContent.   start a Stream PUT /v0/streams/${name}/start?group=$group\n We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.    Example Response 202 Accepted     You should use get stream to fetch up-to-date status   stop a Stream This action will graceful stop and remove all docker containers belong to this stream. Note: successful stop stream will have no status.\nPUT /v0/streams/${name}/stop?group=$group[\u0026amp;force=true]\n Query Parameters  force (boolean) \u0026mdash; true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).     We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.    Example Response 202 Accepted     You should use get stream to fetch up-to-date status   get topology tree graph from specific stream [TODO] This is not implemented yet !\nGET /v0/streams/view/${name}\n  Example Response\n jarInfo (string) \u0026mdash; the upload jar information name (string) \u0026mdash; the stream name poneglyph (object) \u0026mdash; the stream topology tree graph  steles (array(object)) \u0026mdash; the topology collection  steles[i].kind (string) \u0026mdash; this component kind (SOURCE, PROCESSOR, or SINK) steles[i].key (string) \u0026mdash; this component kind with order steles[i].name (string) \u0026mdash; depend on kind, the name is  SOURCE \u0026mdash; source topic name PROCESSOR \u0026mdash; the function name SINK \u0026mdash; target topic name   steles[i].from (string) \u0026mdash; the prior component key (could be empty if this is the first component) steles[i].to (string) \u0026mdash; the posterior component key (could be empty if this is the final component)      { \u0026quot;jarInfo\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;stream-app\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;wk01\u0026quot;, \u0026quot;size\u0026quot;: 1234, \u0026quot;lastModified\u0026quot;: 1542102595892 }, \u0026quot;name\u0026quot;: \u0026quot;my-app\u0026quot;, \u0026quot;poneglyph\u0026quot;: { \u0026quot;steles\u0026quot;: [ { \u0026quot;kind\u0026quot;: \u0026quot;SOURCE\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;SOURCE-0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;stream-in\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;PROCESSOR-1\u0026quot; }, { \u0026quot;kind\u0026quot;: \u0026quot;PROCESSOR\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;PROCESSOR-1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;filter\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;SOURCE-0\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;PROCESSOR-2\u0026quot; }, { \u0026quot;kind\u0026quot;: \u0026quot;PROCESSOR\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;PROCESSOR-2\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;mapvalues\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;PROCESSOR-1\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;SINK-3\u0026quot; }, { \u0026quot;kind\u0026quot;: \u0026quot;SINK\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;SINK-3\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;stream-out\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;PROCESSOR-2\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;\u0026quot; } ] } }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"e46c3cb6fbcd0f5211b8c09be97ff3b6","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/streams/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/streams/","section":"docs","summary":"Ohara Stream is a unparalleled wrap of kafka streaming. It leverages and enhances Kafka Streams to make developer easily design and implement the streaming application. More details of developing streaming application is in custom stream guideline.","tags":null,"title":"Stream","type":"docs"},{"authors":null,"categories":null,"content":"Ohara Stream is a unparalleled wrap of kafka streaming. It leverages and enhances Kafka Streams to make developer easily design and implement the streaming application. More details of developing streaming application is in custom stream guideline.\nAssume that you have completed a streaming application via Ohara Java APIs, and you have generated a jar including your streaming code. By Ohara Restful APIs, you are enable to control, deploy, and monitor your streaming application. As with cluster APIs, Ohara leverages docker container to host streaming application. Of course, you can apply your favor container management tool including simple (based on ssh) and k8s when you are starting Ohara.\nBefore stating to use restful APIs, please ensure that all nodes have downloaded the Stream image. The jar you uploaded to run streaming application will be included in the image and then executes as a docker container. The Stream image is kept in each node so don\u0026rsquo;t worry about the network. We all hate re-download everything when running services.\n If you implement the Ohara stream application, you must pack to jar file and use the File API upload jar file then setting the jarKey to create the Stream API.   The following information of Stream are updated by Ohara.\nstream stored data The following are common settings to a stream app.\n name (string) \u0026mdash; cluster name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; cluster group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; jarKey (object) \u0026mdash; the used jar key jmxPort (int) \u0026mdash; expose port for jmx className (string) \u0026mdash; the class to be executed. This field is optional and Configurator will pick up a class from the input jar. However, it throw exception if there are many available classes in the jar file. from (array(TopicKey)) \u0026mdash; source topic to (array(TopicKey)) \u0026mdash; target topic nodeNames (array(string)) \u0026mdash; the nodes running the stream process brokerClusterKey (object) \u0026mdash; the broker cluster key used for stream running  brokerClusterKey.group (option(string)) \u0026mdash; the group of broker cluster brokerClusterKey.name (string) \u0026mdash; the name of broker cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   tags (object) \u0026mdash; the user defined parameters aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of stream cluster state (option(string)) \u0026mdash; only started/failed stream has state (DEAD if all containers are not running, else RUNNING) error (option(string)) \u0026mdash; the error message from a failed stream. If the stream is fine or un-started, you won\u0026rsquo;t get this field.  nodeMetrics (object) \u0026mdash; the metrics from this stream.  meters (array(object)) \u0026mdash; the metrics in meter type  meters[i].value (double) \u0026mdash; the number stored in meter meters[i].unit (string) \u0026mdash; unit for value meters[i].document (string) \u0026mdash; document of this meter meters[i].queryTime (long) \u0026mdash; the time of query metrics from remote machine meters[i].startTime (option(long)) \u0026mdash; the time of record generated in remote machine     lastModified (long) \u0026mdash; last modified this jar time  create properties of specific stream Create the properties of a stream.\nPOST /v0/streams\n name (string) \u0026mdash; new stream name. This is the object unique name ; default is random string. group (string) \u0026mdash; group name for current stream ; default value is \u0026ldquo;default\u0026rdquo; imageName (string) \u0026mdash; image name of stream used to ; default is oharastream/stream:0.11.0-SNAPSHOT nodeNames (array(string)) \u0026mdash; node name list of stream used to ; default is empty tags (object) \u0026mdash; a key-value map of user defined data ; default is empty jarKey (object) \u0026mdash; the used jar key  group (string) \u0026mdash; the group name of this jar name (string) \u0026mdash; the name of this jar   brokerClusterKey (option(object)) \u0026mdash; the broker cluster used for stream running ; default we will auto fill this parameter for you if you don\u0026rsquo;t specify it and there only exists one broker cluster. jmxPort (int) \u0026mdash; expose port for jmx ; default is random port from (array(TopicKey)) \u0026mdash; source topic ; default is empty array   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nWe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n  to (array(TopicKey)) \u0026mdash; target topic ; default is empty array   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nWe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n    Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: \u0026quot;bk\u0026quot;, \u0026quot;jarKey\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;], \u0026quot;from\u0026quot;: [\u0026quot;topic0\u0026quot;], \u0026quot;to\u0026quot;: [\u0026quot;topic1\u0026quot;] }    Example Response\nResponse format is as stream stored format.\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579145546218, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }     The stream, which is just created, does not have any metrics.   get information from a specific stream cluster GET /v0/streams/${name}?group=$group\n We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.     Example Response\nResponse format is as stream stored format.\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579145546218, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    list information of stream cluster GET /v0/streams\nthe accepted query keys are listed below\n author group name lastModified tags state aliveNodes key version    Example Response\nResponse format is as stream stored format.\n[ { \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579145546218, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    update properties of specific stream Update the properties of a non-started stream.\nPUT /v0/streams/${name}?group=$group\n If the required stream (group, name) was not exists, we will try to use this request as create stream    imageName (option(string)) \u0026mdash; image name of stream used to. nodeNames (option(array(string))) \u0026mdash; node name list of stream used to. tags (option(object)) \u0026mdash; a key-value map of user defined data. jarKey (option(option(object))) \u0026mdash; the used jar key  group (option(string)) \u0026mdash; the group name of this jar name (option(string)) \u0026mdash; the name without extension of this jar   jmxPort (option(int)) \u0026mdash; expose port for jmx. from (option(array(string))) \u0026mdash; source topic.   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nwe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n  to (option(array(string))) \u0026mdash; target topic.   Currently, stream uses a \u0026ldquo;single flow\u0026rdquo; for users to define their own data flow, we need a better structure to support multiple topics.\nwe only support one topic for current version. We will throw exception in start api if you assign more than 1 topic. We will support multiple topics on issue [#688](https://github.com/oharastream/ohara/issues/688\n)\n    Example Request\n{ \u0026quot;from\u0026quot;: [\u0026quot;topic2\u0026quot;], \u0026quot;to\u0026quot;: [\u0026quot;topic3\u0026quot;], \u0026quot;jarKey\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node01\u0026quot;] }    Example Response\nResponse format is as stream stored format.\n{ \u0026quot;author\u0026quot;: \u0026quot;root\u0026quot;, \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;streamtest1\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1579153777586, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/stream:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;jarKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ohara-it-stream.jar\u0026quot; }, \u0026quot;to\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic3\u0026quot; } ], \u0026quot;revision\u0026quot;: \u0026quot;b303f3c2e52647ee5e79e55f9d74a5e51238a92c\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;stream.class\u0026quot;: \u0026quot;oharastream.ohara.it.stream.DumbStream\u0026quot;, \u0026quot;from\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic2\u0026quot; } ], \u0026quot;nodeMetrics\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 44914, \u0026quot;kind\u0026quot;: \u0026quot;stream\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node01\u0026quot; ] }    delete properties of specific stream Delete the properties of a non-started stream. This api only remove the stream component which is stored in pipeline.\nDELETE /v0/streams/${name}?group=$group\n We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.    Example Response 204 NoContent     It is ok to delete an nonexistent properties, and the response is 204 NoContent.   start a Stream PUT /v0/streams/${name}/start?group=$group\n We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.    Example Response 202 Accepted     You should use get stream to fetch up-to-date status   stop a Stream This action will graceful stop and remove all docker containers belong to this stream. Note: successful stop stream will have no status.\nPUT /v0/streams/${name}/stop?group=$group[\u0026amp;force=true]\n Query Parameters  force (boolean) \u0026mdash; true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).     We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.    Example Response 202 Accepted     You should use get stream to fetch up-to-date status   get topology tree graph from specific stream [TODO] This is not implemented yet !\nGET /v0/streams/view/${name}\n  Example Response\n jarInfo (string) \u0026mdash; the upload jar information name (string) \u0026mdash; the stream name poneglyph (object) \u0026mdash; the stream topology tree graph  steles (array(object)) \u0026mdash; the topology collection  steles[i].kind (string) \u0026mdash; this component kind (SOURCE, PROCESSOR, or SINK) steles[i].key (string) \u0026mdash; this component kind with order steles[i].name (string) \u0026mdash; depend on kind, the name is  SOURCE \u0026mdash; source topic name PROCESSOR \u0026mdash; the function name SINK \u0026mdash; target topic name   steles[i].from (string) \u0026mdash; the prior component key (could be empty if this is the first component) steles[i].to (string) \u0026mdash; the posterior component key (could be empty if this is the final component)      { \u0026quot;jarInfo\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;stream-app\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;wk01\u0026quot;, \u0026quot;size\u0026quot;: 1234, \u0026quot;lastModified\u0026quot;: 1542102595892 }, \u0026quot;name\u0026quot;: \u0026quot;my-app\u0026quot;, \u0026quot;poneglyph\u0026quot;: { \u0026quot;steles\u0026quot;: [ { \u0026quot;kind\u0026quot;: \u0026quot;SOURCE\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;SOURCE-0\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;stream-in\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;PROCESSOR-1\u0026quot; }, { \u0026quot;kind\u0026quot;: \u0026quot;PROCESSOR\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;PROCESSOR-1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;filter\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;SOURCE-0\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;PROCESSOR-2\u0026quot; }, { \u0026quot;kind\u0026quot;: \u0026quot;PROCESSOR\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;PROCESSOR-2\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;mapvalues\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;PROCESSOR-1\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;SINK-3\u0026quot; }, { \u0026quot;kind\u0026quot;: \u0026quot;SINK\u0026quot;, \u0026quot;key\u0026quot; : \u0026quot;SINK-3\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;stream-out\u0026quot;, \u0026quot;from\u0026quot;: \u0026quot;PROCESSOR-2\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;\u0026quot; } ] } }    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"53bc007b50b8c61dc6b57232f90acecc","permalink":"https://oharastream.github.io/en/docs/master/rest-api/streams/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/streams/","section":"docs","summary":"Ohara Stream is a unparalleled wrap of kafka streaming. It leverages and enhances Kafka Streams to make developer easily design and implement the streaming application. More details of developing streaming application is in custom stream guideline.","tags":null,"title":"Stream","type":"docs"},{"authors":null,"categories":null,"content":"Ohara topic is based on kafka topic. It means the creation of topic on ohara will invoke a creation of kafka also. Also, the delete to Ohara topic also invoke a delete request to kafka. The common properties in topic are shown below.\n group (string) - topic group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; name (string) - topic name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; brokerClusterKey (Object) - target broker cluster.  brokerClusterKey.group (option(string)) - the group of cluster brokerClusterKey.name (string) - the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   numberOfReplications (option(int)) - the number of replications numberOfPartitions (option(int))- the number of partitions for this topic tags (option(object)) - the extra description to this object    The name must be unique in a broker cluster. There are many other available configs which are useful in creating topic. Please ref broker clusters to see how to retrieve the available configs for specific broker cluster.    The following information are tagged by Ohara.\n state (option(string)) - state of a running topic. nothing if the topic is not running. partitionInfos (Array(object)) - the details of partitions.  index (int) - the index of partition leaderNode (String) - the leader (node) of this partition replicaNodes (Array(String)) - the nodes hosting the replica for this partition inSyncReplicaNodes (Array(String)) - the nodes which have fetched the newest data from leader beginningOffset (long) - the beginning offset endOffset (endOffset) - the latest offset (Normally, it is the latest commit data)   group (string) - the group value is always \u0026ldquo;default\u0026rdquo;  nodeMetrics (object) - the metrics number of a running topic lastModified (long) - the last time to update this ftp information  store a topic properties POST /v0/topics\n  the name you pass to Ohara is used to build topic on kafka, and it is restricted by Kafka ([a-zA-Z0-9._-]) the ignored fields will be auto-completed by Ohara Configurator. Also, you could update/replace it by UPDATE request later. this API does NOT create a topic on broker cluster. Instead, you should send START request to run a topic on broker cluster actually There are many other available configs which are useful in creating topic. Please ref broker clusters to see how to retrieve the available configs for specific broker cluster.      Example Request\n{ \u0026quot;brokerClusterKey\u0026quot;: \u0026quot;bk\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;numberOfPartitions\u0026quot;: 1 }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537142950, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;:\u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 1 }     The topic, which is just created, does not have any metrics.   update a topic properties PUT /v0/topics/${name}?group=${group}\n  Example Request\n{ \u0026quot;numberOfPartitions\u0026quot;: 3 }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537915735, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 3 }    list all topics properties GET /v0/topics?${key}=${value}\nthe accepted query keys are listed below.\n group name state lastModified tags tag: this field is similar to tags but it addresses the \u0026ldquo;contain\u0026rdquo; behavior. key   Using \u0026ldquo;NONE\u0026rdquo; represents the nonexistence of state.    Example Response [ { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537915735, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 3 } ]    delete a topic properties DELETE /v0/topics/${name}?group=${group}\n  Example Response\n204 NoContent     It is ok to delete a nonexistent topic, and the response is 204 NoContent. You must be stopped the delete topic.   get a topic properties GET /v0/topics/${name}\n Example Response { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537915735, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 3 }    start a topic on remote broker cluster PUT /v0/topics/${name}/start\n Example Response 202 Accepted     You should use Get Topic info to fetch up-to-date status   stop a topic from remote broker cluster PUT /v0/topics/${name}/stop\n the topic will lose all data after stopping.    Example Response 202 Accepted     You should use Get Topic info to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"3db742c122153f0b7a0719c16a2da054","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/topics/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/topics/","section":"docs","summary":"Ohara topic is based on kafka topic. It means the creation of topic on ohara will invoke a creation of kafka also. Also, the delete to Ohara topic also invoke a delete request to kafka.","tags":null,"title":"Topic","type":"docs"},{"authors":null,"categories":null,"content":"Ohara topic is based on kafka topic. It means the creation of topic on ohara will invoke a creation of kafka also. Also, the delete to Ohara topic also invoke a delete request to kafka. The common properties in topic are shown below.\n group (string) - topic group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; name (string) - topic name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; brokerClusterKey (Object) - target broker cluster.  brokerClusterKey.group (option(string)) - the group of cluster brokerClusterKey.name (string) - the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}   numberOfReplications (option(int)) - the number of replications numberOfPartitions (option(int))- the number of partitions for this topic tags (option(object)) - the extra description to this object    The name must be unique in a broker cluster. There are many other available configs which are useful in creating topic. Please ref broker clusters to see how to retrieve the available configs for specific broker cluster.    The following information are tagged by Ohara.\n state (option(string)) - state of a running topic. nothing if the topic is not running. partitionInfos (Array(object)) - the details of partitions.  index (int) - the index of partition leaderNode (String) - the leader (node) of this partition replicaNodes (Array(String)) - the nodes hosting the replica for this partition inSyncReplicaNodes (Array(String)) - the nodes which have fetched the newest data from leader beginningOffset (long) - the beginning offset endOffset (endOffset) - the latest offset (Normally, it is the latest commit data)   group (string) - the group value is always \u0026ldquo;default\u0026rdquo;  nodeMetrics (object) - the metrics number of a running topic lastModified (long) - the last time to update this ftp information  store a topic properties POST /v0/topics\n  the name you pass to Ohara is used to build topic on kafka, and it is restricted by Kafka ([a-zA-Z0-9._-]) the ignored fields will be auto-completed by Ohara Configurator. Also, you could update/replace it by UPDATE request later. this API does NOT create a topic on broker cluster. Instead, you should send START request to run a topic on broker cluster actually There are many other available configs which are useful in creating topic. Please ref broker clusters to see how to retrieve the available configs for specific broker cluster.      Example Request\n{ \u0026quot;brokerClusterKey\u0026quot;: \u0026quot;bk\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;numberOfPartitions\u0026quot;: 1 }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537142950, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;:\u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 1 }     The topic, which is just created, does not have any metrics.   update a topic properties PUT /v0/topics/${name}?group=${group}\n  Example Request\n{ \u0026quot;numberOfPartitions\u0026quot;: 3 }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic0\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537915735, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 3 }    list all topics properties GET /v0/topics?${key}=${value}\nthe accepted query keys are listed below.\n group name state lastModified tags tag: this field is similar to tags but it addresses the \u0026ldquo;contain\u0026rdquo; behavior. key   Using \u0026ldquo;NONE\u0026rdquo; represents the nonexistence of state.    Example Response [ { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537915735, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 3 } ]    delete a topic properties DELETE /v0/topics/${name}?group=${group}\n  Example Response\n204 NoContent     It is ok to delete a nonexistent topic, and the response is 204 NoContent. You must be stopped the delete topic.   get a topic properties GET /v0/topics/${name}\n Example Response { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;topic1\u0026quot;, \u0026quot;partitionInfos\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578537915735, \u0026quot;tags\u0026quot;: {}, \u0026quot;numberOfReplications\u0026quot;: 1, \u0026quot;nodeMetrics\u0026quot;: { \u0026quot;node00\u0026quot;: { \u0026quot;meters\u0026quot;: [ { \u0026quot;document\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;BytesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;bytes / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2143210885 }, { \u0026quot;document\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;MessagesInPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;messages / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 2810000.0 }, { \u0026quot;document\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;TotalProduceRequestsPerSec\u0026quot;, \u0026quot;queryTime\u0026quot;: 1585069111069, \u0026quot;unit\u0026quot;: \u0026quot;requests / SECONDS\u0026quot;, \u0026quot;value\u0026quot;: 137416.0 } ] } }, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;numberOfPartitions\u0026quot;: 3 }    start a topic on remote broker cluster PUT /v0/topics/${name}/start\n Example Response 202 Accepted     You should use Get Topic info to fetch up-to-date status   stop a topic from remote broker cluster PUT /v0/topics/${name}/stop\n the topic will lose all data after stopping.    Example Response 202 Accepted     You should use Get Topic info to fetch up-to-date status   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"2c53be05bfcb872cbeb14630d751e5b3","permalink":"https://oharastream.github.io/en/docs/master/rest-api/topics/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/topics/","section":"docs","summary":"Ohara topic is based on kafka topic. It means the creation of topic on ohara will invoke a creation of kafka also. Also, the delete to Ohara topic also invoke a delete request to kafka.","tags":null,"title":"Topic","type":"docs"},{"authors":null,"categories":null,"content":"Notwithstanding we have read a lot of document and guideline, there is a chance to input incorrect request or settings when operating Ohara. Hence, Ohara provides a serial APIs used to validate request/settings before you do use them to start service. Noted that not all request/settings are validated by Ohara configurator. If the request/settings is used by other system (for example, kafka), Ohara automatically bypass the validation request to target system and then wrap the result to JSON representation.\nValidate the connector settings PUT /v0/validate/connector\nBefore starting a connector, you can send the settings to test whether all settings are available for the specific connector. Ohara is not in charge of settings validation. Connector MUST define its setting via setting definitions. Ohara configurator only repackage the request to kafka format and then collect the validation result from kafka.\n  Example Request\nThe request format is same as connector request\n  Example Response\nIf target connector has defined the settings correctly, kafka is doable to validate each setting of request. Ohara configurator collect the result and then generate the following report.\n{ \u0026quot;errorCount\u0026quot;: 0, \u0026quot;settings\u0026quot;: [ { \u0026quot;definition\u0026quot;: { \u0026quot;displayName\u0026quot;: \u0026quot;Connector class\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot;, \u0026quot;orderInGroup\u0026quot;: 3, \u0026quot;key\u0026quot;: \u0026quot;connector.class\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;CLASS\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;documentation\u0026quot;: \u0026quot;the class name of connector\u0026quot;, \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;blacklist\u0026quot;: [] }, \u0026quot;value\u0026quot;: { \u0026quot;key\u0026quot;: \u0026quot;connector.class\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;errors\u0026quot;: [] } } ] }  The above example only show a part of report. The element definition is equal to connector’s setting definition. The definition is what connector must define. If you don\u0026rsquo;t write any definitions for you connector, the validation will do nothing for you. The element value is what you request to validate.\n key (string) \u0026mdash; the property key. It is equal to key in definition value (string) \u0026mdash; the value you request to validate errors (array(string)) \u0026mdash; error message when the input value is illegal to connector    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"72f5d119ddadbe7e7794c246d1556763","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/validate/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/validate/","section":"docs","summary":"Notwithstanding we have read a lot of document and guideline, there is a chance to input incorrect request or settings when operating Ohara. Hence, Ohara provides a serial APIs used to validate request/settings before you do use them to start service.","tags":null,"title":"Validation","type":"docs"},{"authors":null,"categories":null,"content":"Notwithstanding we have read a lot of document and guideline, there is a chance to input incorrect request or settings when operating Ohara. Hence, Ohara provides a serial APIs used to validate request/settings before you do use them to start service. Noted that not all request/settings are validated by Ohara configurator. If the request/settings is used by other system (for example, kafka), Ohara automatically bypass the validation request to target system and then wrap the result to JSON representation.\nValidate the connector settings PUT /v0/validate/connector\nBefore starting a connector, you can send the settings to test whether all settings are available for the specific connector. Ohara is not in charge of settings validation. Connector MUST define its setting via setting definitions. Ohara configurator only repackage the request to kafka format and then collect the validation result from kafka.\n  Example Request\nThe request format is same as connector request\n  Example Response\nIf target connector has defined the settings correctly, kafka is doable to validate each setting of request. Ohara configurator collect the result and then generate the following report.\n{ \u0026quot;errorCount\u0026quot;: 0, \u0026quot;settings\u0026quot;: [ { \u0026quot;definition\u0026quot;: { \u0026quot;displayName\u0026quot;: \u0026quot;Connector class\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot;, \u0026quot;orderInGroup\u0026quot;: 3, \u0026quot;key\u0026quot;: \u0026quot;connector.class\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;CLASS\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;documentation\u0026quot;: \u0026quot;the class name of connector\u0026quot;, \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;regex\u0026quot;: null, \u0026quot;internal\u0026quot;: false, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot;, \u0026quot;tableKeys\u0026quot;: [], \u0026quot;recommendedValues\u0026quot;: [], \u0026quot;blacklist\u0026quot;: [] }, \u0026quot;value\u0026quot;: { \u0026quot;key\u0026quot;: \u0026quot;connector.class\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;oharastream.ohara.connector.perf.PerfSource\u0026quot;, \u0026quot;errors\u0026quot;: [] } } ] }  The above example only show a part of report. The element definition is equal to connector’s setting definition. The definition is what connector must define. If you don\u0026rsquo;t write any definitions for you connector, the validation will do nothing for you. The element value is what you request to validate.\n key (string) \u0026mdash; the property key. It is equal to key in definition value (string) \u0026mdash; the value you request to validate errors (array(string)) \u0026mdash; error message when the input value is illegal to connector    ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"09a7a65aac2e88c40b2e8da4cf1a087e","permalink":"https://oharastream.github.io/en/docs/master/rest-api/validate/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/validate/","section":"docs","summary":"Notwithstanding we have read a lot of document and guideline, there is a chance to input incorrect request or settings when operating Ohara. Hence, Ohara provides a serial APIs used to validate request/settings before you do use them to start service.","tags":null,"title":"Validation","type":"docs"},{"authors":null,"categories":null,"content":" Worker is core of running connectors for Ohara. It provides a simple but powerful system to distribute and execute connectors on different nodes. The performance of connectors depends on the scale of worker cluster. For example, you can assign the number of task when creating connector. If there is only 3 nodes within your worker cluster and you specify 6 tasks for your connector, the tasks of you connectors still be deployed on 3 nodes. That is to say, the connector can\u0026rsquo;t get more resources to execute.\nWorker is based on Broker, hence you have to create broker cluster first. Noted that a broker cluster can be used by multi worker clusters. BTW, worker cluster will pre-allocate a lot of topics on broker cluster, and the pre-created topics CAN\u0026rsquo;T be reused by different worker clusters.\nThe properties which can be set by user are shown below.\n name (string) \u0026mdash; cluster name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; cluster group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; imageName (string) \u0026mdash; docker image brokerClusterKey (Object) \u0026mdash; the broker cluster used to store data generated by this worker cluster  brokerClusterKey.group (option(string)) \u0026mdash; the group of cluster brokerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}    clientPort (int) \u0026mdash; worker client port\n  jmxPort (int) \u0026mdash; worker jmx port\n  freePorts (Array(int)) \u0026mdash; the ports you want to pre-bind for the connectors.\nIf your connectors want to build a service on a port which is available to external nodes, you have to define the free ports for your worker cluster so as to make Configurator pre-bind the ports on your worker cluster. Otherwise, your connectors is disable to build service on the port of worker cluster and be connected by external node.\n  group.id (string) \u0026mdash; the id of worker stored in broker cluster\n  config.storage.topic (string) \u0026mdash; a internal topic used to store connector configuration\n  config.storage.replication.factor (int) \u0026mdash; number of replications for config topic\n  offset.storage.topic (string) \u0026mdash; a internal topic used to store connector offset\n  offset.storage.partitions (int) \u0026mdash; number of partitions for offset topic\n  offset.storage.replication.factor (int) \u0026mdash; number of replications for offset topic\n  status.storage.topic (string) \u0026mdash; an internal topic used to store connector status\n  status.storage.partitions (int) \u0026mdash; number of partitions for status topic\n  status.storage.replication.factor (int) \u0026mdash; number of replications for status topic\n  pluginKeys (array(object)) \u0026mdash; the \u0026ldquo;primary key\u0026rdquo; of jars which contain your connectors\nYou can require worker cluster to load the jars stored in ohara if you want to run custom connectors on the worker cluster. see Files APIs for uploading jars to ohara. The files which are deployed to worker must be uber jars - it must include all dependencies exclude for ohara stuff.\n  sharedJarKeys (array(object)) \u0026mdash; those jars is deployed on the root classpath so all connectors are able to load them.\n     When you implement the Ohara connector, you must use the File API upload connector jar file to worker.\n  If your jdbc source connector need to use the third party jar file (such oracle jdbc jar file), you must use the File API upload jar file then setting sharedJarKeys to create the worker API.\n     nodeNames (array(string)) \u0026mdash; the nodes running the worker process\nThe following information are updated by Ohara.\n  aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of worker\n   The group.id, config.storage.topic, offset.storage.topic and status.storage.topic must be unique in broker cluster. Don\u0026rsquo;t reuse them in same broker cluster. Dispatching above unique resources to two worker cluster will pollute the data. Of course, Ohara do a quick failure for this dumb case. However, it is not a quick failure when you are using raw kafka rather than Ohara. Please double check what you configure!   create a worker properties POST /v0/workers\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;], \u0026quot;brokerClusterKey\u0026quot;: \u0026quot;bk\u0026quot; }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982566359, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 33333, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; }    list all workers clusters GET /v0/workers\n Example Response [ { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982566359, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 33333, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; } ]    update broker cluster properties PUT /v0/workers/$name?group=$group\n If the required worker (group, name) was not exists, we will try to use this request as POST     Example Request\n{ \u0026quot;jmxPort\u0026quot;: 7777 }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982765738, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 7777, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; }    delete a worker properties DELETE /v0/workers/$name?group=$group\nYou cannot delete properties of an non-stopped worker cluster. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 204 NoContent     It is ok to delete an nonexistent worker cluster, and the response is 204 NoContent.   get a worker cluster GET /v0/workers/$name?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982765738, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 7777, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; }    start a worker cluster PUT /v0/workers/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     You should use Get worker cluster to fetch up-to-date status   stop a worker cluster Gracefully stopping a running worker cluster.\nPUT /v0/workers/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) \u0026mdash; true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted     You should use Get worker cluster to fetch up-to-date status   add a new node to a running worker cluster PUT /v0/workers/$name/$nodeName?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\nIf you want to extend a running worker cluster, you can add a node to share the heavy loading of a running worker cluster. However, the balance is not triggered at once. By the way, moving a task to another idle node needs to stop task first. Don\u0026rsquo;t worry about the temporary lower throughput when balancer is running.\nremove a node from a running worker cluster DELETE /v0/workers/$name/$nodeName?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\nIf your budget is limited, you can decrease the number of nodes running worker cluster. BUT, removing a node from a running worker cluster invoke a lot of task move, and it will decrease the throughput of your connector.\n Example Response 204 NoContent     It is ok to delete an nonexistent worker node, and the response is 204 NoContent.   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"03aa96cc53a47e8eec9149c2bcc53545","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/workers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/workers/","section":"docs","summary":"Worker is core of running connectors for Ohara. It provides a simple but powerful system to distribute and execute connectors on different nodes. The performance of connectors depends on the scale of worker cluster.","tags":null,"title":"Worker","type":"docs"},{"authors":null,"categories":null,"content":" Worker is core of running connectors for Ohara. It provides a simple but powerful system to distribute and execute connectors on different nodes. The performance of connectors depends on the scale of worker cluster. For example, you can assign the number of task when creating connector. If there is only 3 nodes within your worker cluster and you specify 6 tasks for your connector, the tasks of you connectors still be deployed on 3 nodes. That is to say, the connector can\u0026rsquo;t get more resources to execute.\nWorker is based on Broker, hence you have to create broker cluster first. Noted that a broker cluster can be used by multi worker clusters. BTW, worker cluster will pre-allocate a lot of topics on broker cluster, and the pre-created topics CAN\u0026rsquo;T be reused by different worker clusters.\nThe properties which can be set by user are shown below.\n name (string) \u0026mdash; cluster name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; cluster group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; imageName (string) \u0026mdash; docker image brokerClusterKey (Object) \u0026mdash; the broker cluster used to store data generated by this worker cluster  brokerClusterKey.group (option(string)) \u0026mdash; the group of cluster brokerClusterKey.name (string) \u0026mdash; the name of cluster     the following forms are legal as well: (1) {\u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}, (2) \u0026quot;n\u0026quot;. Both forms are converted to {\u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;n\u0026quot;}    clientPort (int) \u0026mdash; worker client port\n  jmxPort (int) \u0026mdash; worker jmx port\n  freePorts (Array(int)) \u0026mdash; the ports you want to pre-bind for the connectors.\nIf your connectors want to build a service on a port which is available to external nodes, you have to define the free ports for your worker cluster so as to make Configurator pre-bind the ports on your worker cluster. Otherwise, your connectors is disable to build service on the port of worker cluster and be connected by external node.\n  group.id (string) \u0026mdash; the id of worker stored in broker cluster\n  config.storage.topic (string) \u0026mdash; a internal topic used to store connector configuration\n  config.storage.replication.factor (int) \u0026mdash; number of replications for config topic\n  offset.storage.topic (string) \u0026mdash; a internal topic used to store connector offset\n  offset.storage.partitions (int) \u0026mdash; number of partitions for offset topic\n  offset.storage.replication.factor (int) \u0026mdash; number of replications for offset topic\n  status.storage.topic (string) \u0026mdash; an internal topic used to store connector status\n  status.storage.partitions (int) \u0026mdash; number of partitions for status topic\n  status.storage.replication.factor (int) \u0026mdash; number of replications for status topic\n  pluginKeys (array(object)) \u0026mdash; the \u0026ldquo;primary key\u0026rdquo; of jars which contain your connectors\nYou can require worker cluster to load the jars stored in ohara if you want to run custom connectors on the worker cluster. see Files APIs for uploading jars to ohara. The files which are deployed to worker must be uber jars - it must include all dependencies exclude for ohara stuff.\n  sharedJarKeys (array(object)) \u0026mdash; those jars is deployed on the root classpath so all connectors are able to load them.\n     When you implement the Ohara connector, you must use the File API upload connector jar file to worker.\n  If your jdbc source connector need to use the third party jar file (such oracle jdbc jar file), you must use the File API upload jar file then setting sharedJarKeys to create the worker API.\n     nodeNames (array(string)) \u0026mdash; the nodes running the worker process\nThe following information are updated by Ohara.\n  aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of worker\n   The group.id, config.storage.topic, offset.storage.topic and status.storage.topic must be unique in broker cluster. Don\u0026rsquo;t reuse them in same broker cluster. Dispatching above unique resources to two worker cluster will pollute the data. Of course, Ohara do a quick failure for this dumb case. However, it is not a quick failure when you are using raw kafka rather than Ohara. Please double check what you configure!   create a worker properties POST /v0/workers\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;wk\u0026quot;, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;], \u0026quot;brokerClusterKey\u0026quot;: \u0026quot;bk\u0026quot; }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982566359, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 33333, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; }    list all workers clusters GET /v0/workers\n Example Response [ { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982566359, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 33333, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; } ]    update broker cluster properties PUT /v0/workers/$name?group=$group\n If the required worker (group, name) was not exists, we will try to use this request as POST     Example Request\n{ \u0026quot;jmxPort\u0026quot;: 7777 }    Example Response\n{ \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982765738, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 7777, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; }    delete a worker properties DELETE /v0/workers/$name?group=$group\nYou cannot delete properties of an non-stopped worker cluster. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 204 NoContent     It is ok to delete an nonexistent worker cluster, and the response is 204 NoContent.   get a worker cluster GET /v0/workers/$name?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;brokerClusterKey\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;bk00\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;wk00\u0026quot;, \u0026quot;offset.storage.partitions\u0026quot;: 1, \u0026quot;xms\u0026quot;: 2048, \u0026quot;routes\u0026quot;: {}, \u0026quot;config.storage.topic\u0026quot;: \u0026quot;b8dadc3de21048fa927335b8f\u0026quot;, \u0026quot;sharedJarKeys\u0026quot;: [], \u0026quot;lastModified\u0026quot;: 1578982765738, \u0026quot;tags\u0026quot;: {}, \u0026quot;xmx\u0026quot;: 2048, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/connect-worker:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;offset.storage.topic\u0026quot;: \u0026quot;346b839ea3e74387ab1eea409\u0026quot;, \u0026quot;status.storage.replication.factor\u0026quot;: 1, \u0026quot;group.id\u0026quot;: \u0026quot;af4b4d49234a4848bb90fb452\u0026quot;, \u0026quot;offset.storage.replication.factor\u0026quot;: 1, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;pluginKeys\u0026quot;: [], \u0026quot;status.storage.partitions\u0026quot;: 1, \u0026quot;freePorts\u0026quot;: [], \u0026quot;jmxPort\u0026quot;: 7777, \u0026quot;config.storage.partitions\u0026quot;: 1, \u0026quot;clientPort\u0026quot;: 45127, \u0026quot;config.storage.replication.factor\u0026quot;: 1, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;status.storage.topic\u0026quot;: \u0026quot;1cdca943f0b945bc892ebe9a7\u0026quot; }    start a worker cluster PUT /v0/workers/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     You should use Get worker cluster to fetch up-to-date status   stop a worker cluster Gracefully stopping a running worker cluster.\nPUT /v0/workers/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) \u0026mdash; true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted     You should use Get worker cluster to fetch up-to-date status   add a new node to a running worker cluster PUT /v0/workers/$name/$nodeName?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\nIf you want to extend a running worker cluster, you can add a node to share the heavy loading of a running worker cluster. However, the balance is not triggered at once. By the way, moving a task to another idle node needs to stop task first. Don\u0026rsquo;t worry about the temporary lower throughput when balancer is running.\nremove a node from a running worker cluster DELETE /v0/workers/$name/$nodeName?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\nIf your budget is limited, you can decrease the number of nodes running worker cluster. BUT, removing a node from a running worker cluster invoke a lot of task move, and it will decrease the throughput of your connector.\n Example Response 204 NoContent     It is ok to delete an nonexistent worker node, and the response is 204 NoContent.   ","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"0407c5f3c8467d62ca6afd8c4d4f650d","permalink":"https://oharastream.github.io/en/docs/master/rest-api/workers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/workers/","section":"docs","summary":"Worker is core of running connectors for Ohara. It provides a simple but powerful system to distribute and execute connectors on different nodes. The performance of connectors depends on the scale of worker cluster.","tags":null,"title":"Worker","type":"docs"},{"authors":null,"categories":null,"content":" Zookeeper service is the base of all other services. It is also the fist service you should set up. At the beginning, you can deploy zookeeper cluster in single node. However, it may be unstable since single node can\u0026rsquo;t guarantee the data durability when node crash. In production you should set up zookeeper cluster on 3 nodes at least.\nZookeeper service has many configs which make you spend a lot of time to read and set. Ohara provides default values to all configs but open a room to enable you to overwrite somethings you do care.\nThe properties which can be set by user are shown below.\n name (string) \u0026mdash; cluster name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; cluster group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; imageName (string) \u0026mdash; docker image jmxPort (int) \u0026mdash; zookeeper jmx port clientPort (int) \u0026mdash; zookeeper client port electionPort (int) \u0026mdash; used to select the zk node leader peerPort (int) \u0026mdash; port used by internal communication nodeNames (array(string)) \u0026mdash; the nodes running the zookeeper process tags (object) \u0026mdash; the user defined parameters  The following information are updated by Ohara.\n aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of zookeeper state (option(string)) \u0026mdash; only started/failed zookeeper has state (RUNNING or DEAD) error (option(string)) \u0026mdash; the error message from a failed zookeeper. If zookeeper is fine or un-started, you won\u0026rsquo;t get this field. lastModified (long) \u0026mdash; last modified this jar time  create a zookeeper properties POST /v0/zookeepers\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Example Response\n{ \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642569693, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 33915, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }     All ports have default value so you can ignore them when creating zookeeper cluster. However, the port conflict detect does not allow you to reuse port on different purpose (a dangerous behavior, right?).   list all zookeeper clusters GET /v0/zookeepers\nthe accepted query keys are listed below.\n group name lastModified tags tag - this field is similar to tags but it addresses the \u0026ldquo;contain\u0026rdquo; behavior. state aliveNodes key   Example Response [ { \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642569693, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 33915, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    update zookeeper cluster properties PUT /v0/zookeepers/$name?group=$group\n If the required zookeeper (group, name) was not exists, we will try to use this request as POST     Example Request\n{ \u0026quot;jmxPort\u0026quot;: 12345 }    Example Response\n{ \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642751122, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 12345, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    delete a zookeeper properties DELETE /v0/zookeepers/$name?group=$group\nYou cannot delete properties of an non-stopped zookeeper cluster. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Example Response\n204 NoContent     It is ok to delete an nonexistent zookeeper cluster, and the response is 204 NoContent.   get a zookeeper cluster GET /v0/zookeepers/$name?group=$group\nGet zookeeper information by name and group. This API could fetch all information of a zookeeper (include state). We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642569693, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 33915, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    start a zookeeper cluster PUT /v0/zookeepers/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     You should use Get zookeeper cluster to fetch up-to-date status   stop a zookeeper cluster Gracefully stopping a running zookeeper cluster. It is disallowed to stop a zookeeper cluster used by a running broker cluster.\nPUT /v0/zookeepers/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) - true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted     You should use Get zookeeper cluster to fetch up-to-date status   delete a node from a running zookeeper cluster Unfortunately, it is a litter dangerous to remove a node from a running zookeeper cluster so we don\u0026rsquo;t support it yet.\nadd a node to a running zookeeper cluster Unfortunately, it is a litter hard to add a node to a running zookeeper cluster so we don\u0026rsquo;t support it yet.\n","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"b2fe77703c1127416594d71e88867f16","permalink":"https://oharastream.github.io/en/docs/0.11.x/rest-api/zookeepers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/rest-api/zookeepers/","section":"docs","summary":"Zookeeper service is the base of all other services. It is also the fist service you should set up. At the beginning, you can deploy zookeeper cluster in single node.","tags":null,"title":"Zookeeper","type":"docs"},{"authors":null,"categories":null,"content":" Zookeeper service is the base of all other services. It is also the fist service you should set up. At the beginning, you can deploy zookeeper cluster in single node. However, it may be unstable since single node can\u0026rsquo;t guarantee the data durability when node crash. In production you should set up zookeeper cluster on 3 nodes at least.\nZookeeper service has many configs which make you spend a lot of time to read and set. Ohara provides default values to all configs but open a room to enable you to overwrite somethings you do care.\nThe properties which can be set by user are shown below.\n name (string) \u0026mdash; cluster name. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; group (string) \u0026mdash; cluster group. The legal character is number, lowercase alphanumeric characters, or \u0026lsquo;.\u0026rsquo; imageName (string) \u0026mdash; docker image jmxPort (int) \u0026mdash; zookeeper jmx port clientPort (int) \u0026mdash; zookeeper client port electionPort (int) \u0026mdash; used to select the zk node leader peerPort (int) \u0026mdash; port used by internal communication nodeNames (array(string)) \u0026mdash; the nodes running the zookeeper process tags (object) \u0026mdash; the user defined parameters  The following information are updated by Ohara.\n aliveNodes (array(string)) \u0026mdash; the nodes that host the running containers of zookeeper state (option(string)) \u0026mdash; only started/failed zookeeper has state (RUNNING or DEAD) error (option(string)) \u0026mdash; the error message from a failed zookeeper. If zookeeper is fine or un-started, you won\u0026rsquo;t get this field. lastModified (long) \u0026mdash; last modified this jar time  create a zookeeper properties POST /v0/zookeepers\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    Example Response\n{ \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642569693, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 33915, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }     All ports have default value so you can ignore them when creating zookeeper cluster. However, the port conflict detect does not allow you to reuse port on different purpose (a dangerous behavior, right?).   list all zookeeper clusters GET /v0/zookeepers\nthe accepted query keys are listed below.\n group name lastModified tags tag - this field is similar to tags but it addresses the \u0026ldquo;contain\u0026rdquo; behavior. state aliveNodes key   Example Response [ { \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642569693, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 33915, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] } ]    update zookeeper cluster properties PUT /v0/zookeepers/$name?group=$group\n If the required zookeeper (group, name) was not exists, we will try to use this request as POST     Example Request\n{ \u0026quot;jmxPort\u0026quot;: 12345 }    Example Response\n{ \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642751122, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 12345, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    delete a zookeeper properties DELETE /v0/zookeepers/$name?group=$group\nYou cannot delete properties of an non-stopped zookeeper cluster. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Example Response\n204 NoContent     It is ok to delete an nonexistent zookeeper cluster, and the response is 204 NoContent.   get a zookeeper cluster GET /v0/zookeepers/$name?group=$group\nGet zookeeper information by name and group. This API could fetch all information of a zookeeper (include state). We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;syncLimit\u0026quot;: 5, \u0026quot;name\u0026quot;: \u0026quot;zk00\u0026quot;, \u0026quot;xms\u0026quot;: 1024, \u0026quot;routes\u0026quot;: {}, \u0026quot;lastModified\u0026quot;: 1578642569693, \u0026quot;dataDir\u0026quot;: \u0026quot;/home/ohara/default/data\u0026quot;, \u0026quot;tags\u0026quot;: {}, \u0026quot;electionPort\u0026quot;: 44371, \u0026quot;xmx\u0026quot;: 1024, \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:0.11.0-SNAPSHOT\u0026quot;, \u0026quot;aliveNodes\u0026quot;: [], \u0026quot;initLimit\u0026quot;: 10, \u0026quot;jmxPort\u0026quot;: 33915, \u0026quot;clientPort\u0026quot;: 42006, \u0026quot;peerPort\u0026quot;: 46559, \u0026quot;tickTime\u0026quot;: 2000, \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ] }    start a zookeeper cluster PUT /v0/zookeepers/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response 202 Accepted     You should use Get zookeeper cluster to fetch up-to-date status   stop a zookeeper cluster Gracefully stopping a running zookeeper cluster. It is disallowed to stop a zookeeper cluster used by a running broker cluster.\nPUT /v0/zookeepers/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) - true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted     You should use Get zookeeper cluster to fetch up-to-date status   delete a node from a running zookeeper cluster Unfortunately, it is a litter dangerous to remove a node from a running zookeeper cluster so we don\u0026rsquo;t support it yet.\nadd a node to a running zookeeper cluster Unfortunately, it is a litter hard to add a node to a running zookeeper cluster so we don\u0026rsquo;t support it yet.\n","date":1592434800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"c22e79f48493efd74fccc7847f7f760a","permalink":"https://oharastream.github.io/en/docs/master/rest-api/zookeepers/","publishdate":"2020-06-18T00:00:00+01:00","relpermalink":"/en/docs/master/rest-api/zookeepers/","section":"docs","summary":"Zookeeper service is the base of all other services. It is also the fist service you should set up. At the beginning, you can deploy zookeeper cluster in single node.","tags":null,"title":"Zookeeper","type":"docs"},{"authors":null,"categories":null,"content":"A powerful application always has a complicated configuration. In order to be a best friend of Ohara users, Ohara provides a method which can return the details of setting definitions, and ohara suggests that all developers ought to implement the method so as to guide users through the complicated settings of your applications.\n If you have no interest in settings or your application is too simple to have any settings, you can just skip this section.   SettingDef is a class used to describe the details of a setting. It consists of following arguments.\n  reference(string) \u0026mdash; works for ohara manager. It represents the reference of value. group (string) \u0026mdash; the group of this setting (all core setting are in core group) orderInGroup (int) \u0026mdash; the order in group displayName (string) \u0026mdash; the readable name of this setting permission (string) \u0026mdash; the way to \u0026ldquo;touch\u0026rdquo; value. It consists of  READ_ONLY \u0026mdash; you can\u0026rsquo;t define an new value for it CREATE_ONLY \u0026mdash; you can\u0026rsquo;t update the value to an new one EDITABLE \u0026mdash; feel free to modify the value as you wish :)   key (string) \u0026mdash; the key of configuration  valueType(string) \u0026mdash; the type of value necessary (string)  REQUIRED \u0026mdash; this field has no default and user MUST define something for it. OPTIONAL \u0026mdash; this field has no default and user does NOT need to define something for it. RANDOM_DEFAULT \u0026mdash; this field has a \u0026ldquo;random\u0026rdquo; default value   defaultValue (object) \u0026mdash; the default value. the type is equal to what valueType defines but we only allow string, number and boolean type to have default value currently. documentation (string) \u0026mdash; the explanation of this definition internal (boolean) \u0026mdash; true if this setting is assigned by system automatically. tableKeys (array(object)) \u0026mdash; the description to Type.TABLE  tableKeys[i].name \u0026mdash; the column name tableKeys[i].type \u0026mdash; acceptable type (string, number and boolean) tableKeys[i].recommendedValues \u0026mdash; recommended values (it is legal to enter other values you prefer)     You can call Worker APIs to get all connectors\u0026rsquo; setting definitions, and use Stream APIs to get all stream setting definitions.   Although a SettingDef can include many elements, you can simply build a SettingDef with only what you need. An extreme example is that you can create a SettingDef with only key.\nSettingDef.builder() .key(key) .build();  Notwithstanding it is flexible to build a SettingDef, we encourage developers to create a description-rich SettingDef. More description to your setting produces more document in calling ohara rest APIs. We all hate write documentation, so it would be better to make your code readable.\nReference, Internal and TableKeys Are NOT Public Ohara offers a great UI, which is located at ohara-manager. The UI requires some private information to generate forms for custom applications. The such private information is specific-purpose and is meaningless to non-ohara developers. Hence, all of them are declared as package-private and ohara does not encourage developers to stop at nothing to use them.\nOptional, Required And Default Value We know a great application must have countless settings and only The Chosen One can control it. In order to shorten the gap between your application and human begin, ohara encourage developers to offer the default values to most of settings as much as possible. Assigning a default value to a SettingDef is a piece of cake.\nSettingDef.builder() .key(key) .optional(defaultValue) .build();   the default value is declared as string type as it must be readable in Restful APIs.   After calling the optional(String) method, the response, created by Worker APIs for example, will display the following information.\n{ \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;ur_default_value\u0026quot; }   The default value will be added to TaskSetting automatically if the specified key is not already associated with a value.   A Readonly Setting Definition You can declare a readonly setting that not only exposes something of your application to user but also remind user the setting can\u0026rsquo;t be changed at runtime. For instance, the information of version is fixed after you have completed your connector so it is not an editable setting. Hence, ohara define a setting for version with a readonly label. By the way, you should assign a default value to a readonly setting since a readonly setting without default value is really weird. There is an example of creating a readonly setting.\nSettingDef.builder() .key(key) .optional(defaultValue) .permission(SettingDef.Permission.READ_ONLY) .build();   The input value will be removed automatically if the associated setting is declared readonly.   Setting Reference This element is a specific purpose. It is used by Ohara manager (UI) only. If you don\u0026rsquo;t have interest in UI, you can just ignore this element. However, we still list the available values here.\n NODE TOPIC ZOOKEEPER BROKER WORKER   For each reference value, it may have different type and will produce different behavior.   Topic String\nSettingDef.builder().key(\u0026quot;topic\u0026quot;).reference(Reference.TOPIC).required(Type.STRING).build();  which means the request should \u0026ldquo;accept one topic of string type\u0026rdquo;\n{ \u0026quot;topic\u0026quot;: \u0026quot;t1\u0026quot; }   TopicKey List\nSettingDef.builder() .key(\u0026quot;topicKeys\u0026quot;) .reference(Reference.TOPIC) .required(Type.OBJECT_KEYS) .build();  which means the request should \u0026ldquo;accept topic list of TopicKey type\u0026rdquo;\n{ \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; }, { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t2\u0026quot; } ] }   Topic String List\nSettingDef.builder() .key(\u0026quot;topics\u0026quot;) .reference(Reference.TOPIC) .required(Type.ARRAY) .build();  which means the request should \u0026ldquo;accept topic list of string type\u0026rdquo;\n{ \u0026quot;topics\u0026quot;: [\u0026quot;t1\u0026quot;, \u0026quot;t2\u0026quot;, \u0026quot;t3\u0026quot;] }  Value Type In a custom application, the settings could have various data type. In order to display correct data type in ohara manager and leverage the benefit of type checker, we strongly suggest you to define the correct data type for each setting.\nThe following data types are supported currently.\nType.BOOLEAN Boolean type represents that the data should have only two possible value: true or false. The value must be able cast to java.lang.Boolean\nType.STRING String type represents that the data should be a string. The value must be able cast to java.lang.String\nSettingDef.builder() .key(key) .required(Type.STRING) .build();  Type.POSITIVE_SHORT Short type represents that the data should be a 2-bytes integer. The value must be able cast to java.lang.Short. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_SHORT) .build();  Type.SHORT Short type represents that the data should be a 2-bytes integer. The value must be able cast to java.lang.Short\nSettingDef.builder() .key(key) .required(Type.SHORT) .build();  Type.POSITIVE_INT Int type represents that the data should be a 4-bytes integer. The value must be able cast to java.lang.Integer. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_INT) .build();  Type.INT Int type represents that the data should be a 4-bytes integer. The value must be able cast to java.lang.Integer\nSettingDef.builder() .key(key) .required(Type.INT) .build();  Type.POSITIVE_LONG Long type represents that the data should be a 8-bytes integer. The value must be able cast to java.lang.Long. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_LONG) .build();  Type.LONG Long type represents that the data should be a 8-bytes integer. The value must be able cast to java.lang.Long\nSettingDef.builder() .key(key) .required(Type.LONG) .build();  Type.POSITIVE_DOUBLE Double type represents that the data should be a 8-bytes floating-point. The value must be able cast to java.lang.Double. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_DOUBLE) .build();  Type.DOUBLE Double type represents that the data should be a 8-bytes floating point. The value must be able cast to java.lang.Double\nSettingDef.builder() .key(key) .required(Type.DOUBLE) .build();  Type.ARRAY Array type represents that the data should be a collection of data. We don\u0026rsquo;t check the element data type in the collection, that is, the following request is legal in SettingDef but will produce a weird behavior in ohara manager. We suggest you use the same data type of element in an array.\n{ \u0026quot;key\u0026quot;: [\u0026quot;abc\u0026quot;, 123, 2.0] }  SettingDef.builder() .key(key) .required(Type.ARRAY) .build();   An empty array is ok and will pass the checker:\n{ \u0026quot;key\u0026quot;: [] }     the default value to array value is empty   Type.CLASS Class type represents that the data is a class. This data type is used to display a value that is a class. The value must be able cast to java.lang.String.\nSettingDef.builder() .key(key) .required(Type.CLASS) .build();  Type.PASSWORD Password type represents that the data is a password. We will replace the value by hidden symbol in APIs. if the data type is used as password. The value must be able cast to java.lang.String.\nSettingDef.builder() .key(key) .required(Type.PASSWORD) .build();  Type.JDBC_TABLE JDBC_TABLE is a specific string type used to reminder Ohara Manager that this field requires a magic button to show available tables of remote database via Query APIs. Except for the magic in UI, there is no other stuff for this JDBC_TYPE since kafka can\u0026rsquo;t verify the input arguments according to other arguments. It means we can\u0026rsquo;t connect to remote database to check the existence of input table.\nIt is ok to replace this field by Type.STRING if you don\u0026rsquo;t use Ohara Manager. Nevertheless, we still encourage the developer to choose the fitting type for your setting if you demand your user to input a database table.\nType.TABLE Table type enable you to define a setting having table structure value. Apart from assigning Type.Table to your setting definition, you also have to define which keys are in your table. The following example show a case that declares a table having two columns called c0 and c1.\nSettingDef.builder() .key(key) .tableKeys(Arrays.asList(\u0026quot;c0\u0026quot;, \u0026quot;c1\u0026quot;)) .required(Type.TABLE) .build();  The legal value for above setting definition is shown below.\n{ \u0026quot;key\u0026quot;: [ { \u0026quot;c0\u0026quot;: \u0026quot;v0\u0026quot;, \u0026quot;c1\u0026quot;: \u0026quot;v1\u0026quot; }, { \u0026quot;c0\u0026quot;: \u0026quot;v2\u0026quot;, \u0026quot;c1\u0026quot;: \u0026quot;v3\u0026quot; } ] }  The above example implies there is a table having two columns called c0 and c1. Also, you assign two values to c0 that first is v0 and another is v2. Ohara offers a check for Type.Table that the input value must match all keys in.\nHow to get the description of above keys ? If the setting type is table, the setting must have tableKeys. It is an array of string which shows the keys used in the table type. For instance, a setting having table type is shown below.\n{ \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;columns\u0026quot;, \u0026quot;internal\u0026quot;: false, \u0026quot;documentation\u0026quot;: \u0026quot;output schema\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;TABLE\u0026quot;, \u0026quot;tableKeys\u0026quot;: [ \u0026quot;order\u0026quot;, \u0026quot;dataType\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;newName\u0026quot; ], \u0026quot;orderInGroup\u0026quot;: 6, \u0026quot;key\u0026quot;: \u0026quot;columns\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot;, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot; }   If you ignore the table keys for Type.Table, the check to your input value is also ignored. By contrast, the table keys are useless for other types.    the default value to table value is empty   Type.DURATION The time-based amount of time is a common setting in our world. However, it is also hard to reach the consensus about the string representation for a duration. For instance, the java.time.Duration prefers ISO-8601, such as PT10S. The scala.concurrent.duration.Duration prefers simple format, such as 10 seconds. Ohara offers a official support to Duration type so as to ease the pain of using string in connector. When you declare a setting with duration type, ohara provides the default check which casts input value to java Duration and scala Duration. Also, your connector can get the Duration from TaskSetting easily without worrying about the conversion between java and scala. Furthermore, connector users can input both java.Duration and scala.Duration when starting connector.\nThe value must be castable to java.time.Duration and it is based on the ISO-860 duration format PnDTnHnMn.nS\nType.REMOTE_PORT Remote port is a common property to connector. For example, the ftp connector needs port used to connect to source/target ftp server in remote . Inputting an illegal port can destroy connector easily. Declaring your type of value to Port involve a check that only the port which is smaller than 65536 and bigger than zero can be accepted. Other port value will be rejected in starting connector.\nType.BINDING_PORT This type is similar to Type.PORT except that the value mapped to BINDING_PORT has an extra check to the availability on the target nodes. For example, you define value 5555 as a BINDING_PORT, and you will get a exception when you try to deploy your code on the node which is using port 5555 as well. The legal value of binding port is between [0, 65535].\nType.OBJECT_KEY object key represents a format of oharastream.ohara.common.setting.ObjectKey for a specific object. It consists \u0026ldquo;group\u0026rdquo; and \u0026ldquo;name\u0026rdquo; fields. In a custom application, you should check the request contains both fields.\n{ \u0026quot;key\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;abc\u0026quot; } }  Type.OBJECT_KEYS OBJECT_KEYS represents a list of oharastream.ohara.common.setting.Obj. Note the type of the plural char \u0026ldquo;s\u0026rdquo;. It means the request value should pass an array.\n{ \u0026quot;objectKeys\u0026quot;: [{ \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; }] }   the default value to table value is empty   Type.TAGS Tags is a flexible type that accept a json object. It could use in some circumstances that user needs to define additional values which type is not list above.\n{ \u0026quot;tags\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;anArray\u0026quot;: [\u0026quot;bar\u0026quot;, \u0026quot;foo\u0026quot;], \u0026quot;count\u0026quot;: 10, \u0026quot;params\u0026quot;: { \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot; } } }   the default value to table value is empty   Necessary In Ohara world, most components have a lot of configs to offers various usage in production. In order to simplify the settings, most configs have default value and you can trace Necessary field to know that.\nNecessary field has three values.\n REQUIRED \u0026mdash; this value has no default value, and it must be defined. You may get error if you don\u0026rsquo;t give any value to it. OPTIONAL \u0026mdash; this value has no default value, but it is ok to leave nothing. RANDOM_DEFAULT \u0026mdash; the default value assigned to this value is random. For example, all objects\u0026rsquo; name has a random string by default; The binding port field has a random free port by default.  Checker We all love quick failure, right? A quick failure can save our resource and time. Ohara offers many checks for your setting according to the expected type. For example, a setting declared Duration type has a checker which validate whether the input value is able to be cast to either java.time.Duration or scala.duration.Duration. However, you are going to design a complicated connector which has specific limit for input value.\nDenyList The denyList is useful information that it offers following checks.\n The restful APIs will reject the values in the denyList Ohara UI disable user to input the illegal words  Currently, denyList is used by Array type only.\nRecommended values Recommended values is used by Ohara UI that it able to pop a list to users when they are using UI.\nCurrently, recommended values is used by String type only.\n","date":1592348400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"6efbd017b9b34afd33bf509d38ebb0b7","permalink":"https://oharastream.github.io/en/docs/0.11.x/setting_definition/","publishdate":"2020-06-17T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/setting_definition/","section":"docs","summary":"A powerful application always has a complicated configuration. In order to be a best friend of Ohara users, Ohara provides a method which can return the details of setting definitions, and ohara suggests that all developers ought to implement the method so as to guide users through the complicated settings of your applications.","tags":null,"title":"Setting Definition Guide","type":"docs"},{"authors":null,"categories":null,"content":"A powerful application always has a complicated configuration. In order to be a best friend of Ohara users, Ohara provides a method which can return the details of setting definitions, and ohara suggests that all developers ought to implement the method so as to guide users through the complicated settings of your applications.\n If you have no interest in settings or your application is too simple to have any settings, you can just skip this section.   SettingDef is a class used to describe the details of a setting. It consists of following arguments.\n  reference(string) \u0026mdash; works for ohara manager. It represents the reference of value. group (string) \u0026mdash; the group of this setting (all core setting are in core group) orderInGroup (int) \u0026mdash; the order in group displayName (string) \u0026mdash; the readable name of this setting permission (string) \u0026mdash; the way to \u0026ldquo;touch\u0026rdquo; value. It consists of  READ_ONLY \u0026mdash; you can\u0026rsquo;t define an new value for it CREATE_ONLY \u0026mdash; you can\u0026rsquo;t update the value to an new one EDITABLE \u0026mdash; feel free to modify the value as you wish :)   key (string) \u0026mdash; the key of configuration  valueType(string) \u0026mdash; the type of value necessary (string)  REQUIRED \u0026mdash; this field has no default and user MUST define something for it. OPTIONAL \u0026mdash; this field has no default and user does NOT need to define something for it. RANDOM_DEFAULT \u0026mdash; this field has a \u0026ldquo;random\u0026rdquo; default value   defaultValue (object) \u0026mdash; the default value. the type is equal to what valueType defines but we only allow string, number and boolean type to have default value currently. documentation (string) \u0026mdash; the explanation of this definition internal (boolean) \u0026mdash; true if this setting is assigned by system automatically. tableKeys (array(object)) \u0026mdash; the description to Type.TABLE  tableKeys[i].name \u0026mdash; the column name tableKeys[i].type \u0026mdash; acceptable type (string, number and boolean) tableKeys[i].recommendedValues \u0026mdash; recommended values (it is legal to enter other values you prefer)     You can call Worker APIs to get all connectors\u0026rsquo; setting definitions, and use Stream APIs to get all stream setting definitions.   Although a SettingDef can include many elements, you can simply build a SettingDef with only what you need. An extreme example is that you can create a SettingDef with only key.\nSettingDef.builder() .key(key) .build();  Notwithstanding it is flexible to build a SettingDef, we encourage developers to create a description-rich SettingDef. More description to your setting produces more document in calling ohara rest APIs. We all hate write documentation, so it would be better to make your code readable.\nReference, Internal and TableKeys Are NOT Public Ohara offers a great UI, which is located at ohara-manager. The UI requires some private information to generate forms for custom applications. The such private information is specific-purpose and is meaningless to non-ohara developers. Hence, all of them are declared as package-private and ohara does not encourage developers to stop at nothing to use them.\nOptional, Required And Default Value We know a great application must have countless settings and only The Chosen One can control it. In order to shorten the gap between your application and human begin, ohara encourage developers to offer the default values to most of settings as much as possible. Assigning a default value to a SettingDef is a piece of cake.\nSettingDef.builder() .key(key) .optional(defaultValue) .build();   the default value is declared as string type as it must be readable in Restful APIs.   After calling the optional(String) method, the response, created by Worker APIs for example, will display the following information.\n{ \u0026quot;necessary\u0026quot;: \u0026quot;OPTIONAL_WITH_DEFAULT\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;ur_default_value\u0026quot; }   The default value will be added to TaskSetting automatically if the specified key is not already associated with a value.   A Readonly Setting Definition You can declare a readonly setting that not only exposes something of your application to user but also remind user the setting can\u0026rsquo;t be changed at runtime. For instance, the information of version is fixed after you have completed your connector so it is not an editable setting. Hence, ohara define a setting for version with a readonly label. By the way, you should assign a default value to a readonly setting since a readonly setting without default value is really weird. There is an example of creating a readonly setting.\nSettingDef.builder() .key(key) .optional(defaultValue) .permission(SettingDef.Permission.READ_ONLY) .build();   The input value will be removed automatically if the associated setting is declared readonly.   Setting Reference This element is a specific purpose. It is used by Ohara manager (UI) only. If you don\u0026rsquo;t have interest in UI, you can just ignore this element. However, we still list the available values here.\n NODE TOPIC ZOOKEEPER BROKER WORKER   For each reference value, it may have different type and will produce different behavior.   Topic String\nSettingDef.builder().key(\u0026quot;topic\u0026quot;).reference(Reference.TOPIC).required(Type.STRING).build();  which means the request should \u0026ldquo;accept one topic of string type\u0026rdquo;\n{ \u0026quot;topic\u0026quot;: \u0026quot;t1\u0026quot; }   TopicKey List\nSettingDef.builder() .key(\u0026quot;topicKeys\u0026quot;) .reference(Reference.TOPIC) .required(Type.OBJECT_KEYS) .build();  which means the request should \u0026ldquo;accept topic list of TopicKey type\u0026rdquo;\n{ \u0026quot;topicKeys\u0026quot;: [ { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; }, { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t2\u0026quot; } ] }   Topic String List\nSettingDef.builder() .key(\u0026quot;topics\u0026quot;) .reference(Reference.TOPIC) .required(Type.ARRAY) .build();  which means the request should \u0026ldquo;accept topic list of string type\u0026rdquo;\n{ \u0026quot;topics\u0026quot;: [\u0026quot;t1\u0026quot;, \u0026quot;t2\u0026quot;, \u0026quot;t3\u0026quot;] }  Value Type In a custom application, the settings could have various data type. In order to display correct data type in ohara manager and leverage the benefit of type checker, we strongly suggest you to define the correct data type for each setting.\nThe following data types are supported currently.\nType.BOOLEAN Boolean type represents that the data should have only two possible value: true or false. The value must be able cast to java.lang.Boolean\nType.STRING String type represents that the data should be a string. The value must be able cast to java.lang.String\nSettingDef.builder() .key(key) .required(Type.STRING) .build();  Type.POSITIVE_SHORT Short type represents that the data should be a 2-bytes integer. The value must be able cast to java.lang.Short. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_SHORT) .build();  Type.SHORT Short type represents that the data should be a 2-bytes integer. The value must be able cast to java.lang.Short\nSettingDef.builder() .key(key) .required(Type.SHORT) .build();  Type.POSITIVE_INT Int type represents that the data should be a 4-bytes integer. The value must be able cast to java.lang.Integer. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_INT) .build();  Type.INT Int type represents that the data should be a 4-bytes integer. The value must be able cast to java.lang.Integer\nSettingDef.builder() .key(key) .required(Type.INT) .build();  Type.POSITIVE_LONG Long type represents that the data should be a 8-bytes integer. The value must be able cast to java.lang.Long. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_LONG) .build();  Type.LONG Long type represents that the data should be a 8-bytes integer. The value must be able cast to java.lang.Long\nSettingDef.builder() .key(key) .required(Type.LONG) .build();  Type.POSITIVE_DOUBLE Double type represents that the data should be a 8-bytes floating-point. The value must be able cast to java.lang.Double. Noted: only positive number is acceptable\nSettingDef.builder() .key(key) .required(Type.POSITIVE_DOUBLE) .build();  Type.DOUBLE Double type represents that the data should be a 8-bytes floating point. The value must be able cast to java.lang.Double\nSettingDef.builder() .key(key) .required(Type.DOUBLE) .build();  Type.ARRAY Array type represents that the data should be a collection of data. We don\u0026rsquo;t check the element data type in the collection, that is, the following request is legal in SettingDef but will produce a weird behavior in ohara manager. We suggest you use the same data type of element in an array.\n{ \u0026quot;key\u0026quot;: [\u0026quot;abc\u0026quot;, 123, 2.0] }  SettingDef.builder() .key(key) .required(Type.ARRAY) .build();   An empty array is ok and will pass the checker:\n{ \u0026quot;key\u0026quot;: [] }     the default value to array value is empty   Type.CLASS Class type represents that the data is a class. This data type is used to display a value that is a class. The value must be able cast to java.lang.String.\nSettingDef.builder() .key(key) .required(Type.CLASS) .build();  Type.PASSWORD Password type represents that the data is a password. We will replace the value by hidden symbol in APIs. if the data type is used as password. The value must be able cast to java.lang.String.\nSettingDef.builder() .key(key) .required(Type.PASSWORD) .build();  Type.JDBC_TABLE JDBC_TABLE is a specific string type used to reminder Ohara Manager that this field requires a magic button to show available tables of remote database via Query APIs. Except for the magic in UI, there is no other stuff for this JDBC_TYPE since kafka can\u0026rsquo;t verify the input arguments according to other arguments. It means we can\u0026rsquo;t connect to remote database to check the existence of input table.\nIt is ok to replace this field by Type.STRING if you don\u0026rsquo;t use Ohara Manager. Nevertheless, we still encourage the developer to choose the fitting type for your setting if you demand your user to input a database table.\nType.TABLE Table type enable you to define a setting having table structure value. Apart from assigning Type.Table to your setting definition, you also have to define which keys are in your table. The following example show a case that declares a table having two columns called c0 and c1.\nSettingDef.builder() .key(key) .tableKeys(Arrays.asList(\u0026quot;c0\u0026quot;, \u0026quot;c1\u0026quot;)) .required(Type.TABLE) .build();  The legal value for above setting definition is shown below.\n{ \u0026quot;key\u0026quot;: [ { \u0026quot;c0\u0026quot;: \u0026quot;v0\u0026quot;, \u0026quot;c1\u0026quot;: \u0026quot;v1\u0026quot; }, { \u0026quot;c0\u0026quot;: \u0026quot;v2\u0026quot;, \u0026quot;c1\u0026quot;: \u0026quot;v3\u0026quot; } ] }  The above example implies there is a table having two columns called c0 and c1. Also, you assign two values to c0 that first is v0 and another is v2. Ohara offers a check for Type.Table that the input value must match all keys in.\nHow to get the description of above keys ? If the setting type is table, the setting must have tableKeys. It is an array of string which shows the keys used in the table type. For instance, a setting having table type is shown below.\n{ \u0026quot;reference\u0026quot;: \u0026quot;NONE\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;columns\u0026quot;, \u0026quot;internal\u0026quot;: false, \u0026quot;documentation\u0026quot;: \u0026quot;output schema\u0026quot;, \u0026quot;valueType\u0026quot;: \u0026quot;TABLE\u0026quot;, \u0026quot;tableKeys\u0026quot;: [ \u0026quot;order\u0026quot;, \u0026quot;dataType\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;newName\u0026quot; ], \u0026quot;orderInGroup\u0026quot;: 6, \u0026quot;key\u0026quot;: \u0026quot;columns\u0026quot;, \u0026quot;necessary\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;defaultValue\u0026quot;: null, \u0026quot;group\u0026quot;: \u0026quot;core\u0026quot;, \u0026quot;permission\u0026quot;: \u0026quot;EDITABLE\u0026quot; }   If you ignore the table keys for Type.Table, the check to your input value is also ignored. By contrast, the table keys are useless for other types.    the default value to table value is empty   Type.DURATION The time-based amount of time is a common setting in our world. However, it is also hard to reach the consensus about the string representation for a duration. For instance, the java.time.Duration prefers ISO-8601, such as PT10S. The scala.concurrent.duration.Duration prefers simple format, such as 10 seconds. Ohara offers a official support to Duration type so as to ease the pain of using string in connector. When you declare a setting with duration type, ohara provides the default check which casts input value to java Duration and scala Duration. Also, your connector can get the Duration from TaskSetting easily without worrying about the conversion between java and scala. Furthermore, connector users can input both java.Duration and scala.Duration when starting connector.\nThe value must be castable to java.time.Duration and it is based on the ISO-860 duration format PnDTnHnMn.nS\nType.REMOTE_PORT Remote port is a common property to connector. For example, the ftp connector needs port used to connect to source/target ftp server in remote . Inputting an illegal port can destroy connector easily. Declaring your type of value to Port involve a check that only the port which is smaller than 65536 and bigger than zero can be accepted. Other port value will be rejected in starting connector.\nType.BINDING_PORT This type is similar to Type.PORT except that the value mapped to BINDING_PORT has an extra check to the availability on the target nodes. For example, you define value 5555 as a BINDING_PORT, and you will get a exception when you try to deploy your code on the node which is using port 5555 as well. The legal value of binding port is between [0, 65535].\nType.OBJECT_KEY object key represents a format of oharastream.ohara.common.setting.ObjectKey for a specific object. It consists \u0026ldquo;group\u0026rdquo; and \u0026ldquo;name\u0026rdquo; fields. In a custom application, you should check the request contains both fields.\n{ \u0026quot;key\u0026quot;: { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;abc\u0026quot; } }  Type.OBJECT_KEYS OBJECT_KEYS represents a list of oharastream.ohara.common.setting.Obj. Note the type of the plural char \u0026ldquo;s\u0026rdquo;. It means the request value should pass an array.\n{ \u0026quot;objectKeys\u0026quot;: [{ \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;t1\u0026quot; }] }   the default value to table value is empty   Type.TAGS Tags is a flexible type that accept a json object. It could use in some circumstances that user needs to define additional values which type is not list above.\n{ \u0026quot;tags\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;anArray\u0026quot;: [\u0026quot;bar\u0026quot;, \u0026quot;foo\u0026quot;], \u0026quot;count\u0026quot;: 10, \u0026quot;params\u0026quot;: { \u0026quot;k\u0026quot;: \u0026quot;v\u0026quot; } } }   the default value to table value is empty   Necessary In Ohara world, most components have a lot of configs to offers various usage in production. In order to simplify the settings, most configs have default value and you can trace Necessary field to know that.\nNecessary field has three values.\n REQUIRED \u0026mdash; this value has no default value, and it must be defined. You may get error if you don\u0026rsquo;t give any value to it. OPTIONAL \u0026mdash; this value has no default value, but it is ok to leave nothing. RANDOM_DEFAULT \u0026mdash; the default value assigned to this value is random. For example, all objects\u0026rsquo; name has a random string by default; The binding port field has a random free port by default.  Checker We all love quick failure, right? A quick failure can save our resource and time. Ohara offers many checks for your setting according to the expected type. For example, a setting declared Duration type has a checker which validate whether the input value is able to be cast to either java.time.Duration or scala.duration.Duration. However, you are going to design a complicated connector which has specific limit for input value.\nDenyList The denyList is useful information that it offers following checks.\n The restful APIs will reject the values in the denyList Ohara UI disable user to input the illegal words  Currently, denyList is used by Array type only.\nRecommended values Recommended values is used by Ohara UI that it able to pop a list to users when they are using UI.\nCurrently, recommended values is used by String type only.\n","date":1592348400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"cd0bfc14bcb6773a233ee28d4122e877","permalink":"https://oharastream.github.io/en/docs/master/setting_definition/","publishdate":"2020-06-17T00:00:00+01:00","relpermalink":"/en/docs/master/setting_definition/","section":"docs","summary":"A powerful application always has a complicated configuration. In order to be a best friend of Ohara users, Ohara provides a method which can return the details of setting definitions, and ohara suggests that all developers ought to implement the method so as to guide users through the complicated settings of your applications.","tags":null,"title":"Setting Definition Guide","type":"docs"},{"authors":null,"categories":null,"content":"This documentation is for Ohara users who try to exercise or test Ohara without writing any code. Ohara team design and implement Ohara to provide a unparalleled platform which enable everyone to build streaming easily and quickly. For normal users, you can manipulate Ohara through UI interface even if you have no idea about the magic infra of Ohara. For advanced users trying to build custom streaming, they are encouraged to design and write application based on Ohara\u0026rsquo;s various and powerful APIs (see Custom Connector and Custom Stream).\nOhara consists of many services, such as\n  Ohara Configurator \u0026mdash; the controller of Ohara. It cooperates other services and provides the Restful APIs  Ohara Manager \u0026mdash; the UI service of Ohara. It offers a streaming flow called pipeline  Zookeeper \u0026mdash; works for Broker. It has charge of managing the configuration of topics and health of node  Broker \u0026mdash; It provides the access of topics, topics\u0026rsquo; data durability and balance.  Worker \u0026mdash; It hosts and execute Custom Connector  Docker \u0026mdash; It packages the configs, dependencies and binary required by services and execute them in a isolated environments  Kubernetes \u0026mdash; a management tool of docker instances  Ohara has a complicated software stack, but most services are almost transparent to you. For example, before creating a topic on Ohara, you ought to set up a zookeeper cluster and a broker cluster. There are , unfortunately, a bunch of configs which you have to design and write for both cluster. Ohara auto-generates most configs for you as best as it can, and Ohara offers the the readable Restful APIs and friendly UI to you. All complicated configs are replaced by some simple steps showed on UI. The Quick Start section teach you to exercise Ohara easily and quickly.\n Quick Start The core component of Ohara is Configurator. After installing related tools, you can set up a Configurator via following docker command.\n$ docker run --rm -p 12345:12345 oharastream/configurator:0.11.0-SNAPSHOT --port 12345 --hostname ${host}   click here to see more options for configurator   And then you can also create a manager to provide a beautiful UI based on above Ohara Configurator.\n$ docker run --rm -p 5050:5050 oharastream/manager:0.11.0-SNAPSHOT --port 5050 --configurator http://$ip:12345/v0   Please replace the ip by your host\u0026rsquo;s address   Open your browser (we recommend Google Chrome) and link to http://localhost:5050/\n Installation We all love docker, right? All Ohara services are executed by docker container. However, it is ok to run Ohara services through assembly file if you really really really hate docker.\nNetwork Configurations We are trying to do everything for you. However, your network your problem (reference to Hadoop\u0026rsquo;s motto. A bad network configurations can bring any kind of exception in any time, and it is hard to diagnose your network problems. In order to make each container be able to find each other, please ensure following common problems (reference to Hadoop again) don\u0026rsquo;t happen on your nodes.\n DNS and reverse DNS broken/non-existent. Host tables in the machines invalid. Firewalls in the hosts blocking connections. Routers blocking traffic. Hosts with multiple network cards listening/talking on the wrong NIC. Difference between the hadoop configuration files\u0026rsquo; definition of the cluster (especially hostnames and ports) from that of the actual cluster setup.  After validating your network configurations layer by layer, you could try filing issue on github if you still can\u0026rsquo;t get Ohara to work.\nWe often encounter problems with network problems\nAfter install Docker-ce package in CentOS,the network default policy is block docker\u0026rsquo;s bridge to host network, You must add a rule on the firewall:\n$ sudo firewall-cmd --zone=trusted --permanent --add-interface=docker0  Install Docker-ce on CentOS Docker has provided a great docs about installing docker-ce. Please click this link.\nDownload Ohara Images Ohara deploys docker images on docker hub. You can download images via docker pull command. All images are list below.\n oharastream/broker:0.11.0-SNAPSHOT oharastream/zookeeper:0.11.0-SNAPSHOT oharastream/connect-worker:0.11.0-SNAPSHOT oharastream/configurator:0.11.0-SNAPSHOT oharastream/manager:0.11.0-SNAPSHOT oharastream/stream:0.11.0-SNAPSHOT oharastream/shabondi:0.11.0-SNAPSHOT  Execute Configurator $ docker run --rm -p ${port}:${port} --add-host ${nodeHostName}:${nodeHostIP} oharastream/configurator:0.11.0-SNAPSHOT --port ${port} --hostname ${host}   --folder: the folder used to store data (default is random). Mount the volume if you want to keep your data after restarting Configurator --port: bound by Configurator (default is random) --add-host: add a host mapping to /etc/hosts in Ohara Configurator (nodeHostName:nodeHostIP). If you have DNS server, you can just ignore parameter of add-host. --hostname: hostname to run Ohara Configurator (defaults to 0.0.0.0)   You can enable the jmx reporter via inputting two env variables - \u0026ldquo;JMX_HOSTNAME\u0026rdquo; and \u0026ldquo;JMX_PORT\u0026rdquo;.\n \u0026ldquo;JMX_HOSTNAME\u0026rdquo; should be same as the host running Ohara Configurator container so as to access the jmx service in docker from outside. \u0026ldquo;JMX_PORT\u0026rdquo; should be opened by docker (for example, add \u0026ldquo;-p $JMX_PORT:$JMX_PORT\u0026rdquo;)    All services host by Ohara Configurator are based on docker technique. By default Ohara Configurator use ssh to control the docker containers from remote nodes (see Docker section). In this mode, please make sure the ssh account added by Node APIs should have sudo permission to run docker command (see here for related steps).\nKeep the data of Configurator Ohara Configurator demand a folder to store data and jars. As Ohara Configurator is running in docker container, you have to mount the volume, which is located on container host, on the home folder of Ohara Configurator if you want to keep all data of Ohara Configurator. The following example is to mount a local folder (/tmp/configurator) on /home/ohara/configurator of Ohara Configurator\u0026rsquo;s container.\n$ mkdir /tmp/configurator $ docker run -v /tmp/configurator:/home/ohara/configurator \\ -p 12345:12345 \\ oharastream/configurator:0.11.0-SNAPSHOT \\ --port 12345 \\ --hostname ${host} \\ --folder /home/ohara/configurator  The user account in docker container is ohara, and hence it would be better to set the folder under the /home/ohara. Otherwise, you will encounter the permission error. Noted that you have tell Ohara Configurator to save data in the folder referencing to the outside folder. Otherwise, Ohara Configurator flush all data to a random folder.\nHow to solve the start configurator container permission denied issue?\n You must confirm your host username is the ohara and UID is 1000. Please refer to issue #2573 Please confirm the /tmp/configurator host path owner is ohara user and have to write permission.  Execute Manager $ docker run --rm -p 5050:5050 oharastream/manager:0.11.0-SNAPSHOT --port 5050 --configurator http://localhost:12345/v0   --port: bound by manager (default is 5050) --configurator: basic form of restful API of Ohara Configurator  Execute PostgreSQL Instance $ docker run -d --rm --name postgresql -p 5432:5432 --env POSTGRES_DB=${DB_NAME} --env POSTGRES_USER=${USER_NAME} --env POSTGRES_PASSWORD=${PASSWORD} -it islandsystems/postgresql:9.2.24   POSTGRES_DB: PostgreSQL DataBase name POSTGRES_USER: PostgreSQL login user name. POSTGRES_PASSWORD: PostgreSQL login password.   POSTGRES_USER=\u0026quot;user\u0026rdquo; is illegal to postgresql   Execute FTP Instance $ docker run --rm -p 10000-10011:10000-10011 oharastream/backend:0.11.0-SNAPSHOT oharastream.ohara.testing.service.FtpServer --controlPort 10000 --dataPorts 10001-10011 --user ${UserName} --password ${Password} --hostname ${hostIP or hostName}   controlPort: bound by FTP Server dataPorts: bound by data transportation in FTP Server   Ohara Configurator Ohara consists of many services, and Ohara Configurator plays the most important rule which coordinates all services and offers a bunch of restful APIs to user to get all under control. The brief architecture of Ohara Configurator is shown below.\n  Configurator architecture   The introduction of each components are shown below. Feel free to trace the component in which you have interest.\n  Route of Ohara Configurator  Store of Ohara Configurator  Cache of Ohara Configurator  Collie of Ohara Configurator  Client of Ohara Configurator  Route of Configurator Ohara Configurator leverages the akka-http to implements the rest server and handle the conversion of json objects. You can click our RESTful API docs to see all public APIs and introduction.\nThe APIs supported by Ohara Configurator is only the Restful APIs. Of course, you can raise a question to us - why we choose the Restful APIs rather than pure Java APIs? The answer is - We all hate the other programming language except for the one we are using. However, we always need to work with other people who are typing terrible and weird code, and all they want to do is to call your APIs. In order to save our time from co-working with them, providing the Restful APIs is always to be our solution. For another reason, Ohara Configurator is not in charge of I/O flow. Coordinating all services requires small bandwidth only. We don\u0026rsquo;t need to care for the performance issue about Restful APIs.\n You can use our internal scala APIs to control Configurator. The library is called ohara-client and it covers all Restful APIs of Configurator. However, we don\u0026rsquo;t guarantee any compatibility for ohara-client.   Store of Configurator All settings you request to Ohara Configurator are saved in Store, such as connector settings, cluster information and pipeline description. The default implementation of Store is RocksDB which offers fast in-memory access and persists all data on disk. Please read this section about mounting host\u0026rsquo;s folder on docker container.\nCache of Configurator The cost of coordinating countless services is the big latency. For example, Topic APIs allows you to fetch metrics from different broker clusters. Ohara Configurator has to file a bunch of connections to different clusters to retrieve all requisite information, and, of course, the connections bring the large latency to the GET request. Hence, Ohara Configurator sets up a inner cache which stores the data from remote clusters. It reduces the latency from seconds to milliseconds and allay your anger. In order to make all data up-to-date as much as possible, the cache auto-refreshes timeout data in the background. It brings some extra cost of building connections to remote clusters.\nCollie of Configurator Apart from the data flow, Ohara Configurator is also doable to manage clusters for you. For instance, you can\n add node to Ohara Configurator deploy a zookeeper cluster on the node deploy a broker cluster on the node as well deploy a worker cluster on the node finally, you can run a connector to stream your data and all services you have created are hosted by Ohara Configurator  In order to host your services safely and quickly, Ohara Configurator leverages the Docker technique that all services are packaged to a container and executed on the node(s) specified by you. As a good software stack, Ohara Configurator creates a container manager, which is called collie, to wrap Restful APIs of k8s and ssh command to Scala APIs.\nClient of Configurator As a good programmer, we all love to reuse the code. However, it is hard to trust all third-party libraries guarantee the suitable compatibility policy. The Client code in Ohara is a collection of wrap for all client codes to services, such as broker and worker, so as not to be badly hurt by the update of services.\n Ohara Manager Ohara Manager is the user interface (UI) of Ohara. It\u0026rsquo;s built with the standard web technologies and so can be run in almost all the modern browsers (We recommend you to use Google chrome though). Ohara Manager talks to Ohara Configurator via its RESTful APIs under the hook which then connects with the rest of Ohara services.\nOhara Manager was built and designed with the user\u0026rsquo;s needs in mind. We aimed to reduce the pain of complex operations that often required in a big data system. With Ohara Manager, you can create your own services, pipelines and working with data streaming without touching a single line of code.\nFollowing is a quick walk through of Ohara Manager\u0026rsquo;s user interface:\nPipelines Pipeline list page is where you can view, create, edit and delete pipelines.\n  Inside the new/edit pipeline page, you can create and play around with your pipelines here. This is also where you can run and stop your pipelines. The pipeline graph helps you to easily visualize the pipeline that you\u0026rsquo;re working on. Also, you can edit and tweak a connector\u0026rsquo;s configuration by clicking on the graph and edit the configuration form which will be displayed in the sidebar. We know it\u0026rsquo;s sometimes tedious and time consuming to edit the configuration and it\u0026rsquo;s also frustrating when you lose all of your configuration without saving them! That\u0026rsquo;s why we made these configuration forms automatically save changes for you. Whenever you type in a text field, choose a new topic form a dropdown, the changes will be saved immediately.\n  Please note that a pipeline can only be added to a workspace, so before creating pipelines, you will need to create a workspace first\nNodes This is where you create and edit Ohara Nodes. These nodes are usually your VMs. When you\u0026rsquo;re starting a new Ohara Configurator. You can optionally supply some node information with the CLI command. The node you supplied to the CLI will then be listed in this page.\n  Workspaces A workspace contains multiple Ohara services including: Zookeepers, Brokers and Workers. You can create a workspace and add new node, topic and stream application in these pages.\n  Ohara Manager Workspaces page     Overview:\nOverview page is like a dashboard of the workspace. You can view the services, connectors, topics and stream jars that are using in this workspace\n    Nodes:\nWhen creating a workspace, you can choose which node to deploy your services. But you tweak the node settings here.\n    Topics:\nYou can add new topics to your workspace as well as deleting them here.\n    Stream jars:\nSame like the topics page, you can add and delete stream jars in this page\n    If you\u0026rsquo;d like to learn more about the development setup or have issue starting/working with it. Please see Ohara Manager Development Guideline\n Zookeeper  Zookeeper plays an important role in Ohara that it persists metadata for kafka and monitors the running nodes of kafka. Setting up a zookeeper cluster is always the first phase before you start to use Ohara to host your clusters. It may be weird, however, to you since this cryptic service is almost transparent to you. Currently, zookeeper cluster exists only for kafka. At any rate, you are still doable to access zookeeper via any zk client if you have to.\nAs a result of algorithm used by zookeeper, we recommend your zookeeper cluster should have 2n + 1 nodes which can address the best reliability and availability ( related discussion). In most cases, running a zookeeper cluster with 3 servers is enough to your production because we don\u0026rsquo;t put our data flow on zookeeper cluster. However, you should consider higher number of nodes if your production does care for the recovery time of node crash. More nodes in zookeeper cluster brings more time to you for fixing your broken zookeeper cluster.\nOhara is responsible for creating your zookeeper cluster, and hence Ohara also auto-generate most configs used by a zookeeper cluster. A basic auto-generated configs file to zookeeper cluster is shown below.\ntickTime=2000 initLimit=10 syncLimit=5 maxClientCnxns=60 clientPort=2181 dataDir=/tmp/zookeeper/data server.0=node00:2888:3888  Most options are auto-generated by Ohara Configurator, and Zookeeper APIs displays the configurable settings to user. Feel free to file an issue to Ohara community if you have better configs for zookeeper.\n Broker After setting up a Zookeeper cluster, you have to build a broker cluster before going on your streaming trip. Broker is the streaming center of Ohara that all applications on Ohara goes through brokers to switch data. There are many stories about Ohara leverages the broker to complete countless significant works. But the most important usage of Brokers for Ohara is the Topic. Each endpoint in Pipeline must connect to/from a topic, and each topic in Ohara is mapped to a topic in broker. It means all data sent/received to/from topic is implemented by a true connection to a broker.\nAs a result of addressing scalability, a topic is split to many partitions distributed on different brokers. It implies the number of brokers directly impact the performance of Ohara Pipeline. If you are streaming a bunch of data and there is only a broker in your broker cluster, you will get a slow streaming since all data in the streaming are processed by the single broker. Hence, please be careful on deploying your broker cluster. But you don\u0026rsquo;t worry about the incorrect settings to cluster. Ohara provides many flexible Broker APIs to increase/decrease nodes of a running broker cluster. You are able to scale your cluster up/down arbitrarily via Ohara APIs.\nIn order to simplify your life, Ohara auto-generate most configs for your broker cluster.\nnum.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 num.partitions=1 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0 broker.id=0 listeners=PLAINTEXT://:9092 log.dirs=/tmp/broker/data zookeeper.connect=node00:2181 advertised.listeners=PLAINTEXT://node00:9092  Most options are auto-generated by Ohara Configurator, and Broker APIs displays the configurable settings to user. Ohara community always welcomes user to raise issue about we should give a better default configs or we should enable user to change xxx config.\n Worker In contrast with Broker, Worker takes charge of hosting and distributing your applications. Via Ohara Configurator you can deploy applications on a worker cluster. Worker executes your application on a single thread and handle following issues for you.\n tolerance - worker cluster auto-migrate your application from a dead node to another live one. distribution - you can decide the number of threads invoked by worker cluster to run your applications. Of course, the threads are distributed across whole cluster. Data - Worker is in charge of fetching/pushing data from/to topics specified by your application. All you have to do is to process the data. consistency - The offset of data in/from topics are auto-record by worker. Also, for advanced user, there are a lot of offset-related APIs, which is exposed to your application, that you can control the offsets of data. 1.balance - worker cluster keeps tracing the loading for each worker node and auto-balance the loading for heavy one. Via Ohara APIs, you can increase the node of a running worker cluster easily if you do want to scala the throughput up.  Setting up a worker cluster also requires many configurations. Ohara Configurator auto-fill the following settings for you when you request to create a worker cluster.\nkey.converter=org.apache.kafka.connect.json.JsonConverter value.converter=org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable=true value.converter.schemas.enable=true offset.flush.interval.ms=10000 internal.key.converter=org.apache.kafka.connect.json.JsonConverter internal.value.converter=org.apache.kafka.connect.json.JsonConverter internal.key.converter.schemas.enable=false internal.value.converter.schemas.enable=false group.id=339f4352b3 offset.storage.topic=offset-8e5c68825d offset.storage.replication.factor=1 offset.storage.partitions=1 config.storage.topic=setting-2b86167398 config.storage.replication.factor=1 status.storage.topic=status-4841be564b status.storage.replication.factor=1 status.storage.partitions=1 plugin.path=/tmp/plugins bootstrap.servers=node00:9092 rest.port=8083 rest.advertised.host.name=node00 rest.advertised.port=8083  Most options are auto-generated by Ohara Configurator, and Worker APIs displays the configurable settings to user. Welcome you to file an issue to request more control right of worker cluster.\n Docker All services host by Ohara are based on docker containers, such as Configurator, Manager, Zookeeper, Broker and Worker. You should install suggested version of Docker before enjoying Ohara service (see how to build for prerequisite).\nThe post-installation for all docker nodes are listed below.\n  Install the supported version of docker - Ohara community does not support the legacy docker.  download all ohara images - Ohara Configurator expect all images are available from local disk rather than network.  create a user account which can access docker without sudo - Ohara Configurator may use ssh to control docker of remote node.   all containers created by Ohara, which is on docker mode, have a specific label - createdByOhara - this label enables Ohara to ignore the unrelated containers. For examples, You request Ohara Configurator (of course, it is on docker mode) to create a zookeeper service. The container of zookeeper service will have label - createdByOhara=docker.    Kubernetes Kubernetes is a managed container platform. It can across different container communication of a node. solve more deploy multiple a node container problems, below is Kubernetes advantage:\n Automatically deploy Docker container Docker container resource manage and scaling Orcherstrate docker container on multiple hosts  About details please refer: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\nOhara builds multiple docker images. This includes zookeeper, broker, and connect-worker. These services can be run and controlled through Kubernetes and making container management a lot easier. Before running any Ohara containers, you need to install Kubernetes first. We\u0026rsquo;ll walk you through this process with a few k8s commands:\n all containers created by Ohara, which is on k8s mode, have a specific label - createdByOhara - this label enables Ohara to ignore the unrelated containers. For examples, You request Ohara Configurator (of course, it is on k8s mode) to create a zookeeper service. The container of zookeeper service will have label - createdByOhara=k8s.   Install distribution mode for Kubernetes Hardware requirement  2 CPUs or more 2 GB or more of RAM per machine Full network connectivity between all machines in the cluster Swap disabled    Ohara support install Kubernetes shell script OS only CentOS7 More details is here    Step 1. Install Kubernetes master node  Switch to root user $ su root     Why change to root user? Use the root user to install Kubernetes is simple and convenient. Avoid changing not an admin user. Of course, you can use the admin user and add the \u0026ldquo;sudo\u0026rdquo; keyword to execute install the Kubernetes shell script.     Change directory to kubernetes/distribute\n# cd $OHARA_HOME/kubernetes/distribute    Run bash k8s-master-install.sh ${Your_K8S_Master_Host_IP} to install Kubernetes master\n# bash k8s-master-install.sh ${Your_K8S_Master_Host_IP}    Token and hash will be used in worker installation later on\n# cat /tmp/k8s-install-info.txt    The token and hash should look like the following:\n# kubeadm join 10.100.0.178:6443 --token 14aoza.xpgpa26br32sxwl8 --discovery-token-ca-cert-hash sha256:f5614e6b6376f7559910e66bc014df63398feb7411fe6d0e7057531d7143d47b  Step 2. Install Kubernetes worker node   Switch to root\n$ su root    Goto kubernetes/distribute directory\n# cd $OHARA_HOME/kubernetes/distribute    Run following command in your terminal:\n# bash k8s-worker-install.sh ${Your_K8S_Master_Host_IP} ${TOKEN} ${HASH_CODE}     TOKEN and HASH_CODE can be found in the /tmp/k8s-install-info.txt file of Kubernetes master, the one we mention in the previous steps\nFor example:\n# bash k8s-worker-install.sh 10.100.0.178 14aoza.xpgpa26br32sxwl8 sha256:f5614e6b6376f7559910e66bc014df63398feb7411fe6d0e7057531d7143d47b    Step 3. Ensure the K8S API server is running properly  Log into Kubernetes master and use the following command to see if these Kubernetes nodes are running properly: # kubectl get nodes   You can check Kubernetes node status like the following: # curl -X GET http://${Your_K8S_Master_Host_IP}:8080/api/v1/nodes    Troubleshooting How to autostart the Kubernetes API proxy server after reboot server?\n  Copy \u0026ldquo;ohara/kubernetes/k8sproxyserver.service\u0026rdquo; file to your Kubernetes master server \u0026ldquo;/etc/systemd/system\u0026rdquo; path\n  Below is setting autostart the command to run the Kubernetes API proxy server (default port is 8080):\n# systemctl enable k8sproxyserver.service # systemctl start k8sproxyserver.service    How to use Kubernetes in Ohara?   You must create the service to Kubernetes for DNS use in kubernetes master host, Below is the command:\n# cd $OHARA_HOME/kubernetes # kubectl create -f dns-service.yaml    Below is an example command to run Ohara configurator service for K8S mode:\n# docker run --rm \\ -p 5000:5000 \\ --add-host ${K8S_WORKER01_HOSTNAME}:${K8S_WORKER01_IP} \\ --add-host ${K8S_WORKER02_HOSTNAME}:${K8S_WORKER02_IP} \\ oharastream/configurator:$|version| \\ --port 5000 \\ --hostname ${Start Configurator Host Name} \\ --k8s http://${Your_K8S_Master_Host_IP}:8080/api/v1   --add-host: Add all k8s worker hostname and ip information to configurator container /etc/hosts file. If you have DNS server, you can just ignore parameter of --add-host. --k8s-namespace: If you don\u0026rsquo;t use the Kubernetes default namespace, you can assign the \u0026ndash;k8s-namespace argument to set other the Kubernetes namespace. Kubernetes namespace default value is \u0026ldquo;default\u0026rdquo; string --k8s-metrics-server: If you have installed the Kubernetes metrics server, you can set metrics server URL to monitor your Kubernetes node resource. Example: --k8s-metrics-server http://ohara-kubernetes:8080/apis --k8s: Assignment your K8S API server HTTP URL    Use Ohara configurator to create a zookeeper and broker in Kubernetes pod for the test:\n# Add Ohara Node example $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;hostname\u0026quot;: \u0026quot;${K8S_WORKER01_HOSTNAME}\u0026quot;, \\ \u0026quot;port\u0026quot;: 22, \\ \u0026quot;user\u0026quot;: \u0026quot;${USERNAME}\u0026quot;, \\ \u0026quot;password\u0026quot;: \u0026quot;${PASSWORD}\u0026quot;}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/nodes $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;hostname\u0026quot;: \u0026quot;${K8S_WORKER02_HOSTNAME}\u0026quot;, \\ \u0026quot;port\u0026quot;: 22, \\ \u0026quot;user\u0026quot;: \u0026quot;${USERNAME}\u0026quot;, \\ \u0026quot;password\u0026quot;: \u0026quot;${PASSWORD}\u0026quot;}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/nodes # You must pre pull docker image in the ${K8S_WORKER01_HOSTNAME} and ${K8S_WORKER02_HOSTNAME} host, Below is command: docker pull oharastream/zookeeper:0.11.0-SNAPSHOT docker pull oharastream/broker:0.11.0-SNAPSHOT # Create Zookeeper cluster service $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot;, \\ \u0026quot;clientPort\u0026quot;: 2181, \\ \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:$|version|\u0026quot;, \\ \u0026quot;peerPort\u0026quot;: 2000, \\ \u0026quot;electionPort\u0026quot;: 2001, \\ \u0026quot;nodeNames\u0026quot;: [\u0026quot;${K8S_WORKER01_HOSTNAME}\u0026quot;]}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/zookeepers # Start Zookeeper cluster service $ curl -H \u0026quot;Content-Type: application/json\u0026quot; -X PUT http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/zookeepers/zk/start # Create Broker service example $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot;, \\ \u0026quot;clientPort\u0026quot;: 9092, \\ \u0026quot;zookeeperClusterName\u0026quot;: \u0026quot;zk\u0026quot;, \\ \u0026quot;nodeNames\u0026quot;: [\u0026quot;${K8S_WORKER02_HOSTNAME}\u0026quot;]}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/brokers # Start Broker cluster service $ curl -H \u0026quot;Content-Type: application/json\u0026quot; -X PUT http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/brokers/bk/start    You can use the kubectl command to get zookeeper and broker pod status with the following command:\n# kubectl get pods    How to install K8S metrics server?   You must install the git command to pull the Kubernetes metrics server source code from the repository to deploy metrics server, below is sample command:\n# yum install -y git    After complete install git, you can pull the K8S metrics server source code, below is sample command:\n# git clone https://github.com/kubernetes-sigs/metrics-server.git # git checkout tags/v0.3.7 -b v0.3.7    There should encounter an issue that kubelet and apiserver unable to communicate with metric-server with default setting. Please use following YAML setting to override the content of deploy/1.8+/metrics-server-deployment.yaml file.\n--- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server command: - /metrics-server - --kubelet-preferred-address-types=InternalIP - --kubelet-insecure-tls image: k8s.gcr.io/metrics-server-amd64:v0.3.6 args: - --cert-dir=/tmp - --secure-port=4443 ports: - name: main-port containerPort: 4443 protocol: TCP imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp     For more details please refer to here.    Deploy the Kubernetes metrics server, below is the command: # kubectl apply -f deploy/1.8+   Confirm the Kubernetes metrics service is installed complete, you can input the URL to the browser, below is the example: http://${Your_Kubernetes_Master_HostName_OR_IP}:8080/apis/metrics.k8s.io/v1beta1/nodes    You maybe wait seconds time to receive the Kubernetes node metrics data.\nHow to revert K8S environment setting?  You must stop the K8S API server with this command: kubeadm reset command. More details is here.  How to get the log info in container for debug?  First, log into Kubernetes\u0026rsquo; master server List all Kubernetes pod name to query # kubectl get pods   Get log info in container # kubectl logs ${Your_K8S_Pod_Name}    Other  Ohara K8SClient ImagePullPolicy default is IfNotPresent. Please remember to start K8S API server after you reboot the K8S master server: # nohup kubectl proxy --accept-hosts=^*$ --address=$Your_master_host_IP --port=8080 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;    ","date":1592348400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"f49843a387f7ee91d9d32ec681f38666","permalink":"https://oharastream.github.io/en/docs/0.11.x/user_guide/","publishdate":"2020-06-17T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/user_guide/","section":"docs","summary":"This documentation is for Ohara users who try to exercise or test Ohara without writing any code. Ohara team design and implement Ohara to provide a unparalleled platform which enable everyone to build streaming easily and quickly.","tags":null,"title":"User Guide","type":"docs"},{"authors":null,"categories":null,"content":"This documentation is for Ohara users who try to exercise or test Ohara without writing any code. Ohara team design and implement Ohara to provide a unparalleled platform which enable everyone to build streaming easily and quickly. For normal users, you can manipulate Ohara through UI interface even if you have no idea about the magic infra of Ohara. For advanced users trying to build custom streaming, they are encouraged to design and write application based on Ohara\u0026rsquo;s various and powerful APIs (see Custom Connector and Custom Stream).\nOhara consists of many services, such as\n  Ohara Configurator \u0026mdash; the controller of Ohara. It cooperates other services and provides the Restful APIs  Ohara Manager \u0026mdash; the UI service of Ohara. It offers a streaming flow called pipeline  Zookeeper \u0026mdash; works for Broker. It has charge of managing the configuration of topics and health of node  Broker \u0026mdash; It provides the access of topics, topics\u0026rsquo; data durability and balance.  Worker \u0026mdash; It hosts and execute Custom Connector  Volumes \u0026mdash; Support the persistent data.  Docker \u0026mdash; It packages the configs, dependencies and binary required by services and execute them in a isolated environments  Kubernetes \u0026mdash; a management tool of docker instances  Ohara has a complicated software stack, but most services are almost transparent to you. For example, before creating a topic on Ohara, you ought to set up a zookeeper cluster and a broker cluster. There are , unfortunately, a bunch of configs which you have to design and write for both cluster. Ohara auto-generates most configs for you as best as it can, and Ohara offers the the readable Restful APIs and friendly UI to you. All complicated configs are replaced by some simple steps showed on UI. The Quick Start section teach you to exercise Ohara easily and quickly.\n Quick Start The core component of Ohara is Configurator. After installing related tools, you can set up a Configurator via following docker command.\n$ docker run --rm -p 12345:12345 oharastream/configurator:0.11.0-SNAPSHOT --port 12345 --hostname ${host}   click here to see more options for configurator   And then you can also create a manager to provide a beautiful UI based on above Ohara Configurator.\n$ docker run --rm -p 5050:5050 oharastream/manager:0.11.0-SNAPSHOT --port 5050 --configurator http://$ip:12345/v0   Please replace the ip by your host\u0026rsquo;s address   Open your browser (we recommend Google Chrome) and link to http://localhost:5050/\n Installation We all love docker, right? All Ohara services are executed by docker container. However, it is ok to run Ohara services through assembly file if you really really really hate docker.\nNetwork Configurations We are trying to do everything for you. However, your network your problem (reference to Hadoop\u0026rsquo;s motto. A bad network configurations can bring any kind of exception in any time, and it is hard to diagnose your network problems. In order to make each container be able to find each other, please ensure following common problems (reference to Hadoop again) don\u0026rsquo;t happen on your nodes.\n DNS and reverse DNS broken/non-existent. Host tables in the machines invalid. Firewalls in the hosts blocking connections. Routers blocking traffic. Hosts with multiple network cards listening/talking on the wrong NIC. Difference between the hadoop configuration files\u0026rsquo; definition of the cluster (especially hostnames and ports) from that of the actual cluster setup.  After validating your network configurations layer by layer, you could try filing issue on github if you still can\u0026rsquo;t get Ohara to work.\nWe often encounter problems with network problems\nAfter install Docker-ce package in CentOS,the network default policy is block docker\u0026rsquo;s bridge to host network, You must add a rule on the firewall:\n$ sudo firewall-cmd --zone=trusted --permanent --add-interface=docker0  Install Docker-ce on CentOS Docker has provided a great docs about installing docker-ce. Please click this link.\nDownload Ohara Images Ohara deploys docker images on docker hub. You can download images via docker pull command. All images are list below.\n oharastream/broker:0.11.0-SNAPSHOT oharastream/zookeeper:0.11.0-SNAPSHOT oharastream/connect-worker:0.11.0-SNAPSHOT oharastream/configurator:0.11.0-SNAPSHOT oharastream/manager:0.11.0-SNAPSHOT oharastream/stream:0.11.0-SNAPSHOT oharastream/shabondi:0.11.0-SNAPSHOT  Execute Configurator $ docker run --rm -p ${port}:${port} --add-host ${nodeHostName}:${nodeHostIP} oharastream/configurator:0.11.0-SNAPSHOT --port ${port} --hostname ${host}   --folder: the folder used to store data (default is random). Mount the volume if you want to keep your data after restarting Configurator --port: bound by Configurator (default is random) --add-host: add a host mapping to /etc/hosts in Ohara Configurator (nodeHostName:nodeHostIP). If you have DNS server, you can just ignore parameter of add-host. --hostname: hostname to run Ohara Configurator (defaults to 0.0.0.0)   You can enable the jmx reporter via inputting two env variables - \u0026ldquo;JMX_HOSTNAME\u0026rdquo; and \u0026ldquo;JMX_PORT\u0026rdquo;.\n \u0026ldquo;JMX_HOSTNAME\u0026rdquo; should be same as the host running Ohara Configurator container so as to access the jmx service in docker from outside. \u0026ldquo;JMX_PORT\u0026rdquo; should be opened by docker (for example, add \u0026ldquo;-p $JMX_PORT:$JMX_PORT\u0026rdquo;)    All services host by Ohara Configurator are based on docker technique. By default Ohara Configurator use ssh to control the docker containers from remote nodes (see Docker section). In this mode, please make sure the ssh account added by Node APIs should have sudo permission to run docker command (see here for related steps).\nKeep the data of Configurator Ohara Configurator demand a folder to store data and jars. As Ohara Configurator is running in docker container, you have to mount the volume, which is located on container host, on the home folder of Ohara Configurator if you want to keep all data of Ohara Configurator. The following example is to mount a local folder (/tmp/configurator) on /home/ohara/configurator of Ohara Configurator\u0026rsquo;s container.\n$ mkdir /tmp/configurator $ docker run -v /tmp/configurator:/home/ohara/configurator \\ -p 12345:12345 \\ oharastream/configurator:0.11.0-SNAPSHOT \\ --port 12345 \\ --hostname ${host} \\ --folder /home/ohara/configurator  The user account in docker container is ohara, and hence it would be better to set the folder under the /home/ohara. Otherwise, you will encounter the permission error. Noted that you have tell Ohara Configurator to save data in the folder referencing to the outside folder. Otherwise, Ohara Configurator flush all data to a random folder.\nHow to solve the start configurator container permission denied issue?\n You must confirm your host username is the ohara and UID is 1000. Please refer to issue #2573 Please confirm the /tmp/configurator host path owner is ohara user and have to write permission.  Execute Manager $ docker run --rm -p 5050:5050 oharastream/manager:0.11.0-SNAPSHOT --port 5050 --configurator http://localhost:12345/v0   --port: bound by manager (default is 5050) --configurator: basic form of restful API of Ohara Configurator  Execute PostgreSQL Instance $ docker run -d --rm --name postgresql -p 5432:5432 --env POSTGRES_DB=${DB_NAME} --env POSTGRES_USER=${USER_NAME} --env POSTGRES_PASSWORD=${PASSWORD} -it islandsystems/postgresql:9.2.24   POSTGRES_DB: PostgreSQL DataBase name POSTGRES_USER: PostgreSQL login user name. POSTGRES_PASSWORD: PostgreSQL login password.   POSTGRES_USER=\u0026quot;user\u0026rdquo; is illegal to postgresql   Execute FTP Instance $ docker run --rm -p 10000-10011:10000-10011 oharastream/backend:0.11.0-SNAPSHOT oharastream.ohara.testing.service.FtpServer --controlPort 10000 --dataPorts 10001-10011 --user ${UserName} --password ${Password} --hostname ${hostIP or hostName}   controlPort: bound by FTP Server dataPorts: bound by data transportation in FTP Server   Ohara Configurator Ohara consists of many services, and Ohara Configurator plays the most important rule which coordinates all services and offers a bunch of restful APIs to user to get all under control. The brief architecture of Ohara Configurator is shown below.\n  Configurator architecture   The introduction of each components are shown below. Feel free to trace the component in which you have interest.\n  Route of Ohara Configurator  Store of Ohara Configurator  Cache of Ohara Configurator  Collie of Ohara Configurator  Client of Ohara Configurator  Route of Configurator Ohara Configurator leverages the akka-http to implements the rest server and handle the conversion of json objects. You can click our RESTful API docs to see all public APIs and introduction.\nThe APIs supported by Ohara Configurator is only the Restful APIs. Of course, you can raise a question to us - why we choose the Restful APIs rather than pure Java APIs? The answer is - We all hate the other programming language except for the one we are using. However, we always need to work with other people who are typing terrible and weird code, and all they want to do is to call your APIs. In order to save our time from co-working with them, providing the Restful APIs is always to be our solution. For another reason, Ohara Configurator is not in charge of I/O flow. Coordinating all services requires small bandwidth only. We don\u0026rsquo;t need to care for the performance issue about Restful APIs.\n You can use our internal scala APIs to control Configurator. The library is called ohara-client and it covers all Restful APIs of Configurator. However, we don\u0026rsquo;t guarantee any compatibility for ohara-client.   Store of Configurator All settings you request to Ohara Configurator are saved in Store, such as connector settings, cluster information and pipeline description. The default implementation of Store is RocksDB which offers fast in-memory access and persists all data on disk. Please read this section about mounting host\u0026rsquo;s folder on docker container.\nCache of Configurator The cost of coordinating countless services is the big latency. For example, Topic APIs allows you to fetch metrics from different broker clusters. Ohara Configurator has to file a bunch of connections to different clusters to retrieve all requisite information, and, of course, the connections bring the large latency to the GET request. Hence, Ohara Configurator sets up a inner cache which stores the data from remote clusters. It reduces the latency from seconds to milliseconds and allay your anger. In order to make all data up-to-date as much as possible, the cache auto-refreshes timeout data in the background. It brings some extra cost of building connections to remote clusters.\nCollie of Configurator Apart from the data flow, Ohara Configurator is also doable to manage clusters for you. For instance, you can\n add node to Ohara Configurator deploy a zookeeper cluster on the node deploy a broker cluster on the node as well deploy a worker cluster on the node finally, you can run a connector to stream your data and all services you have created are hosted by Ohara Configurator  In order to host your services safely and quickly, Ohara Configurator leverages the Docker technique that all services are packaged to a container and executed on the node(s) specified by you. As a good software stack, Ohara Configurator creates a container manager, which is called collie, to wrap Restful APIs of k8s and ssh command to Scala APIs.\nClient of Configurator As a good programmer, we all love to reuse the code. However, it is hard to trust all third-party libraries guarantee the suitable compatibility policy. The Client code in Ohara is a collection of wrap for all client codes to services, such as broker and worker, so as not to be badly hurt by the update of services.\n Ohara Manager Ohara Manager is the user interface (UI) of Ohara. It\u0026rsquo;s built with the standard web technologies and so can be run in almost all the modern browsers (We recommend you to use Google chrome though). Ohara Manager talks to Ohara Configurator via its RESTful APIs under the hook which then connects with the rest of Ohara services.\nOhara Manager was built and designed with the user\u0026rsquo;s needs in mind. We aimed to reduce the pain of complex operations that often required in a big data system. With Ohara Manager, you can create your own services, pipelines and working with data streaming without touching a single line of code.\nFollowing is a quick walk through of Ohara Manager\u0026rsquo;s user interface:\nPipelines Pipeline list page is where you can view, create, edit and delete pipelines.\n  Inside the new/edit pipeline page, you can create and play around with your pipelines here. This is also where you can run and stop your pipelines. The pipeline graph helps you to easily visualize the pipeline that you\u0026rsquo;re working on. Also, you can edit and tweak a connector\u0026rsquo;s configuration by clicking on the graph and edit the configuration form which will be displayed in the sidebar. We know it\u0026rsquo;s sometimes tedious and time consuming to edit the configuration and it\u0026rsquo;s also frustrating when you lose all of your configuration without saving them! That\u0026rsquo;s why we made these configuration forms automatically save changes for you. Whenever you type in a text field, choose a new topic form a dropdown, the changes will be saved immediately.\n  Please note that a pipeline can only be added to a workspace, so before creating pipelines, you will need to create a workspace first\nNodes This is where you create and edit Ohara Nodes. These nodes are usually your VMs. When you\u0026rsquo;re starting a new Ohara Configurator. You can optionally supply some node information with the CLI command. The node you supplied to the CLI will then be listed in this page.\n  Workspaces A workspace contains multiple Ohara services including: Zookeepers, Brokers and Workers. You can create a workspace and add new node, topic and stream application in these pages.\n  Ohara Manager Workspaces page     Overview:\nOverview page is like a dashboard of the workspace. You can view the services, connectors, topics and stream jars that are using in this workspace\n    Nodes:\nWhen creating a workspace, you can choose which node to deploy your services. But you tweak the node settings here.\n    Topics:\nYou can add new topics to your workspace as well as deleting them here.\n    Stream jars:\nSame like the topics page, you can add and delete stream jars in this page\n    If you\u0026rsquo;d like to learn more about the development setup or have issue starting/working with it. Please see Ohara Manager Development Guideline\n Zookeeper  Zookeeper plays an important role in Ohara that it persists metadata for kafka and monitors the running nodes of kafka. Setting up a zookeeper cluster is always the first phase before you start to use Ohara to host your clusters. It may be weird, however, to you since this cryptic service is almost transparent to you. Currently, zookeeper cluster exists only for kafka. At any rate, you are still doable to access zookeeper via any zk client if you have to.\nAs a result of algorithm used by zookeeper, we recommend your zookeeper cluster should have 2n + 1 nodes which can address the best reliability and availability ( related discussion). In most cases, running a zookeeper cluster with 3 servers is enough to your production because we don\u0026rsquo;t put our data flow on zookeeper cluster. However, you should consider higher number of nodes if your production does care for the recovery time of node crash. More nodes in zookeeper cluster brings more time to you for fixing your broken zookeeper cluster.\nOhara is responsible for creating your zookeeper cluster, and hence Ohara also auto-generate most configs used by a zookeeper cluster. A basic auto-generated configs file to zookeeper cluster is shown below.\ntickTime=2000 initLimit=10 syncLimit=5 maxClientCnxns=60 clientPort=2181 dataDir=/tmp/zookeeper/data server.0=node00:2888:3888  Most options are auto-generated by Ohara Configurator, and Zookeeper APIs displays the configurable settings to user. Feel free to file an issue to Ohara community if you have better configs for zookeeper.\n Broker After setting up a Zookeeper cluster, you have to build a broker cluster before going on your streaming trip. Broker is the streaming center of Ohara that all applications on Ohara goes through brokers to switch data. There are many stories about Ohara leverages the broker to complete countless significant works. But the most important usage of Brokers for Ohara is the Topic. Each endpoint in Pipeline must connect to/from a topic, and each topic in Ohara is mapped to a topic in broker. It means all data sent/received to/from topic is implemented by a true connection to a broker.\nAs a result of addressing scalability, a topic is split to many partitions distributed on different brokers. It implies the number of brokers directly impact the performance of Ohara Pipeline. If you are streaming a bunch of data and there is only a broker in your broker cluster, you will get a slow streaming since all data in the streaming are processed by the single broker. Hence, please be careful on deploying your broker cluster. But you don\u0026rsquo;t worry about the incorrect settings to cluster. Ohara provides many flexible Broker APIs to increase/decrease nodes of a running broker cluster. You are able to scale your cluster up/down arbitrarily via Ohara APIs.\nIn order to simplify your life, Ohara auto-generate most configs for your broker cluster.\nnum.network.threads=3 num.io.threads=8 socket.send.buffer.bytes=102400 socket.receive.buffer.bytes=102400 socket.request.max.bytes=104857600 num.partitions=1 num.recovery.threads.per.data.dir=1 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 log.retention.hours=168 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connection.timeout.ms=6000 group.initial.rebalance.delay.ms=0 broker.id=0 listeners=PLAINTEXT://:9092 log.dirs=/tmp/broker/data zookeeper.connect=node00:2181 advertised.listeners=PLAINTEXT://node00:9092  Most options are auto-generated by Ohara Configurator, and Broker APIs displays the configurable settings to user. Ohara community always welcomes user to raise issue about we should give a better default configs or we should enable user to change xxx config.\n Worker In contrast with Broker, Worker takes charge of hosting and distributing your applications. Via Ohara Configurator you can deploy applications on a worker cluster. Worker executes your application on a single thread and handle following issues for you.\n tolerance - worker cluster auto-migrate your application from a dead node to another live one. distribution - you can decide the number of threads invoked by worker cluster to run your applications. Of course, the threads are distributed across whole cluster. Data - Worker is in charge of fetching/pushing data from/to topics specified by your application. All you have to do is to process the data. consistency - The offset of data in/from topics are auto-record by worker. Also, for advanced user, there are a lot of offset-related APIs, which is exposed to your application, that you can control the offsets of data. 1.balance - worker cluster keeps tracing the loading for each worker node and auto-balance the loading for heavy one. Via Ohara APIs, you can increase the node of a running worker cluster easily if you do want to scala the throughput up.  Setting up a worker cluster also requires many configurations. Ohara Configurator auto-fill the following settings for you when you request to create a worker cluster.\nkey.converter=org.apache.kafka.connect.json.JsonConverter value.converter=org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable=true value.converter.schemas.enable=true offset.flush.interval.ms=10000 internal.key.converter=org.apache.kafka.connect.json.JsonConverter internal.value.converter=org.apache.kafka.connect.json.JsonConverter internal.key.converter.schemas.enable=false internal.value.converter.schemas.enable=false group.id=339f4352b3 offset.storage.topic=offset-8e5c68825d offset.storage.replication.factor=1 offset.storage.partitions=1 config.storage.topic=setting-2b86167398 config.storage.replication.factor=1 status.storage.topic=status-4841be564b status.storage.replication.factor=1 status.storage.partitions=1 plugin.path=/tmp/plugins bootstrap.servers=node00:9092 rest.port=8083 rest.advertised.host.name=node00 rest.advertised.port=8083  Most options are auto-generated by Ohara Configurator, and Worker APIs displays the configurable settings to user. Welcome you to file an issue to request more control right of worker cluster.\n Docker All services host by Ohara are based on docker containers, such as Configurator, Manager, Zookeeper, Broker and Worker. You should install suggested version of Docker before enjoying Ohara service (see how to build for prerequisite).\nThe post-installation for all docker nodes are listed below.\n  Install the supported version of docker - Ohara community does not support the legacy docker.  download all ohara images - Ohara Configurator expect all images are available from local disk rather than network.  create a user account which can access docker without sudo - Ohara Configurator may use ssh to control docker of remote node.   all containers created by Ohara, which is on docker mode, have a specific label - createdByOhara - this label enables Ohara to ignore the unrelated containers. For examples, You request Ohara Configurator (of course, it is on docker mode) to create a zookeeper service. The container of zookeeper service will have label - createdByOhara=docker.    Kubernetes Kubernetes is a managed container platform. It can across different container communication of a node. solve more deploy multiple a node container problems, below is Kubernetes advantage:\n Automatically deploy Docker container Docker container resource manage and scaling Orcherstrate docker container on multiple hosts  About details please refer: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\nOhara builds multiple docker images. This includes zookeeper, broker, and connect-worker. These services can be run and controlled through Kubernetes and making container management a lot easier. Before running any Ohara containers, you need to install Kubernetes first. We\u0026rsquo;ll walk you through this process with a few k8s commands:\n all containers created by Ohara, which is on k8s mode, have a specific label - createdByOhara - this label enables Ohara to ignore the unrelated containers. For examples, You request Ohara Configurator (of course, it is on k8s mode) to create a zookeeper service. The container of zookeeper service will have label - createdByOhara=k8s.   Install distribution mode for Kubernetes Hardware requirement  2 CPUs or more 2 GB or more of RAM per machine Full network connectivity between all machines in the cluster Swap disabled    Ohara support install Kubernetes shell script OS only CentOS7 More details is here    Step 1. Install Kubernetes master node  Switch to root user $ su root     Why change to root user? Use the root user to install Kubernetes is simple and convenient. Avoid changing not an admin user. Of course, you can use the admin user and add the \u0026ldquo;sudo\u0026rdquo; keyword to execute install the Kubernetes shell script.     Change directory to kubernetes/distribute\n# cd $OHARA_HOME/kubernetes/distribute    Run bash k8s-master-install.sh ${Your_K8S_Master_Host_IP} to install Kubernetes master\n# bash k8s-master-install.sh ${Your_K8S_Master_Host_IP}    Token and hash will be used in worker installation later on\n# cat /tmp/k8s-install-info.txt    The token and hash should look like the following:\n# kubeadm join 10.100.0.178:6443 --token 14aoza.xpgpa26br32sxwl8 --discovery-token-ca-cert-hash sha256:f5614e6b6376f7559910e66bc014df63398feb7411fe6d0e7057531d7143d47b  Step 2. Install Kubernetes worker node   Switch to root\n$ su root    Goto kubernetes/distribute directory\n# cd $OHARA_HOME/kubernetes/distribute    Run following command in your terminal:\n# bash k8s-worker-install.sh ${Your_K8S_Master_Host_IP} ${TOKEN} ${HASH_CODE}     TOKEN and HASH_CODE can be found in the /tmp/k8s-install-info.txt file of Kubernetes master, the one we mention in the previous steps\nFor example:\n# bash k8s-worker-install.sh 10.100.0.178 14aoza.xpgpa26br32sxwl8 sha256:f5614e6b6376f7559910e66bc014df63398feb7411fe6d0e7057531d7143d47b    Step 3. Ensure the K8S API server is running properly  Log into Kubernetes master and use the following command to see if these Kubernetes nodes are running properly: # kubectl get nodes   You can check Kubernetes node status like the following: # curl -X GET http://${Your_K8S_Master_Host_IP}:8080/api/v1/nodes    Troubleshooting How to autostart the Kubernetes API proxy server after reboot server?\n  Copy \u0026ldquo;ohara/kubernetes/k8sproxyserver.service\u0026rdquo; file to your Kubernetes master server \u0026ldquo;/etc/systemd/system\u0026rdquo; path\n  Below is setting autostart the command to run the Kubernetes API proxy server (default port is 8080):\n# systemctl enable k8sproxyserver.service # systemctl start k8sproxyserver.service    How to use Kubernetes in Ohara?   You must create the service to Kubernetes for DNS use in kubernetes master host, Below is the command:\n# cd $OHARA_HOME/kubernetes # kubectl create -f dns-service.yaml    Below is an example command to run Ohara configurator service for K8S mode:\n# docker run --rm \\ -p 5000:5000 \\ --add-host ${K8S_WORKER01_HOSTNAME}:${K8S_WORKER01_IP} \\ --add-host ${K8S_WORKER02_HOSTNAME}:${K8S_WORKER02_IP} \\ oharastream/configurator:$|version| \\ --port 5000 \\ --hostname ${Start Configurator Host Name} \\ --k8s http://${Your_K8S_Master_Host_IP}:8080/api/v1   --add-host: Add all k8s worker hostname and ip information to configurator container /etc/hosts file. If you have DNS server, you can just ignore parameter of --add-host. --k8s-namespace: If you don\u0026rsquo;t use the Kubernetes default namespace, you can assign the \u0026ndash;k8s-namespace argument to set other the Kubernetes namespace. Kubernetes namespace default value is \u0026ldquo;default\u0026rdquo; string --k8s-metrics-server: If you have installed the Kubernetes metrics server, you can set metrics server URL to monitor your Kubernetes node resource. Example: --k8s-metrics-server http://ohara-kubernetes:8080/apis --k8s: Assignment your K8S API server HTTP URL    Use Ohara configurator to create a zookeeper and broker in Kubernetes pod for the test:\n# Add Ohara Node example $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;hostname\u0026quot;: \u0026quot;${K8S_WORKER01_HOSTNAME}\u0026quot;, \\ \u0026quot;port\u0026quot;: 22, \\ \u0026quot;user\u0026quot;: \u0026quot;${USERNAME}\u0026quot;, \\ \u0026quot;password\u0026quot;: \u0026quot;${PASSWORD}\u0026quot;}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/nodes $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;hostname\u0026quot;: \u0026quot;${K8S_WORKER02_HOSTNAME}\u0026quot;, \\ \u0026quot;port\u0026quot;: 22, \\ \u0026quot;user\u0026quot;: \u0026quot;${USERNAME}\u0026quot;, \\ \u0026quot;password\u0026quot;: \u0026quot;${PASSWORD}\u0026quot;}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/nodes # You must pre pull docker image in the ${K8S_WORKER01_HOSTNAME} and ${K8S_WORKER02_HOSTNAME} host, Below is command: docker pull oharastream/zookeeper:0.11.0-SNAPSHOT docker pull oharastream/broker:0.11.0-SNAPSHOT # Create Zookeeper cluster service $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;name\u0026quot;: \u0026quot;zk\u0026quot;, \\ \u0026quot;clientPort\u0026quot;: 2181, \\ \u0026quot;imageName\u0026quot;: \u0026quot;oharastream/zookeeper:$|version|\u0026quot;, \\ \u0026quot;peerPort\u0026quot;: 2000, \\ \u0026quot;electionPort\u0026quot;: 2001, \\ \u0026quot;nodeNames\u0026quot;: [\u0026quot;${K8S_WORKER01_HOSTNAME}\u0026quot;]}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/zookeepers # Start Zookeeper cluster service $ curl -H \u0026quot;Content-Type: application/json\u0026quot; -X PUT http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/zookeepers/zk/start # Create Broker service example $ curl -H \u0026quot;Content-Type: application/json\u0026quot; \\ -X POST \\ -d '{\u0026quot;name\u0026quot;: \u0026quot;bk\u0026quot;, \\ \u0026quot;clientPort\u0026quot;: 9092, \\ \u0026quot;zookeeperClusterName\u0026quot;: \u0026quot;zk\u0026quot;, \\ \u0026quot;nodeNames\u0026quot;: [\u0026quot;${K8S_WORKER02_HOSTNAME}\u0026quot;]}' \\ http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/brokers # Start Broker cluster service $ curl -H \u0026quot;Content-Type: application/json\u0026quot; -X PUT http://${CONFIGURATOR_HOSTNAME_OR_IP}:5000/v0/brokers/bk/start    You can use the kubectl command to get zookeeper and broker pod status with the following command:\n# kubectl get pods    How to install K8S metrics server?   You must install the git command to pull the Kubernetes metrics server source code from the repository to deploy metrics server, below is sample command:\n# yum install -y git    After complete install git, you can pull the K8S metrics server source code, below is sample command:\n# git clone https://github.com/kubernetes-sigs/metrics-server.git # git checkout tags/v0.3.7 -b v0.3.7    There should encounter an issue that kubelet and apiserver unable to communicate with metric-server with default setting. Please use following YAML setting to override the content of deploy/1.8+/metrics-server-deployment.yaml file.\n--- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server command: - /metrics-server - --kubelet-preferred-address-types=InternalIP - --kubelet-insecure-tls image: k8s.gcr.io/metrics-server-amd64:v0.3.6 args: - --cert-dir=/tmp - --secure-port=4443 ports: - name: main-port containerPort: 4443 protocol: TCP imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp     For more details please refer to here.    Deploy the Kubernetes metrics server, below is the command: # kubectl apply -f deploy/1.8+   Confirm the Kubernetes metrics service is installed complete, you can input the URL to the browser, below is the example: http://${Your_Kubernetes_Master_HostName_OR_IP}:8080/apis/metrics.k8s.io/v1beta1/nodes    You maybe wait seconds time to receive the Kubernetes node metrics data.\nHow to revert K8S environment setting?  You must stop the K8S API server with this command: kubeadm reset command. More details is here.  How to get the log info in container for debug?  First, log into Kubernetes\u0026rsquo; master server List all Kubernetes pod name to query # kubectl get pods   Get log info in container # kubectl logs ${Your_K8S_Pod_Name}    Other  Ohara K8SClient ImagePullPolicy default is IfNotPresent. Please remember to start K8S API server after you reboot the K8S master server: # nohup kubectl proxy --accept-hosts=^*$ --address=$Your_master_host_IP --port=8080 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;    ","date":1592348400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606703241,"objectID":"73a1c8df23b6f1f7cfd67167278c91e4","permalink":"https://oharastream.github.io/en/docs/master/user_guide/","publishdate":"2020-06-17T00:00:00+01:00","relpermalink":"/en/docs/master/user_guide/","section":"docs","summary":"This documentation is for Ohara users who try to exercise or test Ohara without writing any code. Ohara team design and implement Ohara to provide a unparalleled platform which enable everyone to build streaming easily and quickly.","tags":null,"title":"User Guide","type":"docs"},{"authors":null,"categories":null,"content":"Why we need docker-compose Ohara is good at connecting to various systems to collect, transform, aggregate (and other operations you can imagine) data. In order to test Ohara, we need a way to run a bunch of systems simultaneously. We can build a heavy infra to iron out this problem. Or we can leverage docker-compose to host various systems \u0026ldquo;locally\u0026rdquo; (yes, you need a powerful machine to use Ohara\u0026rsquo;s docker-compose file).\nPrerequisites  Centos 7.6+ (supported by official community. However, other GNU/Linux should work well also) Docker 18.09+ Docker-compose 1.23.2+  How to install Install docker  This section is a clone of https://docs.docker.com/install/linux/docker-ce/centos/\n Uninstall old versions\n$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine  Install required packages\n$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2  Install using the repository\n$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo  Install docker-ce\n$ sudo yum install docker-ce  Install Docker-compose $ wget https://github.com/docker/compose/releases/download/1.23.2/docker-compose-Linux-x86_64 -O docker-compose $ sudo chmod +x ./docker-compose   see https://github.com/docker/compose/releases for more details   How to \u0026hellip; Start services by docker-compose file Before start services, you must set postgresql connection info for environment variable, example:\nexport POSTGRES_DB=postgres export POSTGRES_USER=username export POSTGRES_PASSWORD=password  Start services command\n$ ./docker-compose -f {docker-compose file} up  Stop services $ ctrl+c  We are talking about tests, right? We don\u0026rsquo;t care about how to shut down services gracefully\nClean up all containers $ docker rm -f $(docker ps -q -a)  We are talking about tests, right? You should have a machine for testing only so it is ok to remove all containers quickly. That does simplify your work and life.\nEnable IPv4 IP Forwarding $ sudo vi /usr/lib/sysctl.d/00-system.conf  Add the following line:\nnet.ipv4.ip_forward=1  Save and exit the file. Restart network:\n$ sudo systemctl restart network  ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"42e412747afa9ad6eca3cd48c28a4c7f","permalink":"https://oharastream.github.io/en/docs/0.11.x/docker/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/docker/","section":"docs","summary":"Why we need docker-compose Ohara is good at connecting to various systems to collect, transform, aggregate (and other operations you can imagine) data. In order to test Ohara, we need a way to run a bunch of systems simultaneously.","tags":null,"title":"Docker and Docker-compose","type":"docs"},{"authors":null,"categories":null,"content":"Why we need docker-compose Ohara is good at connecting to various systems to collect, transform, aggregate (and other operations you can imagine) data. In order to test Ohara, we need a way to run a bunch of systems simultaneously. We can build a heavy infra to iron out this problem. Or we can leverage docker-compose to host various systems \u0026ldquo;locally\u0026rdquo; (yes, you need a powerful machine to use Ohara\u0026rsquo;s docker-compose file).\nPrerequisites  Centos 7.6+ (supported by official community. However, other GNU/Linux should work well also) Docker 18.09+ Docker-compose 1.23.2+  How to install Install docker  This section is a clone of https://docs.docker.com/install/linux/docker-ce/centos/\n Uninstall old versions\n$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine  Install required packages\n$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2  Install using the repository\n$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo  Install docker-ce\n$ sudo yum install docker-ce  Install Docker-compose $ wget https://github.com/docker/compose/releases/download/1.23.2/docker-compose-Linux-x86_64 -O docker-compose $ sudo chmod +x ./docker-compose   see https://github.com/docker/compose/releases for more details   How to \u0026hellip; Start services by docker-compose file Before start services, you must set postgresql connection info for environment variable, example:\nexport POSTGRES_DB=postgres export POSTGRES_USER=username export POSTGRES_PASSWORD=password  Start services command\n$ ./docker-compose -f {docker-compose file} up  Stop services $ ctrl+c  We are talking about tests, right? We don\u0026rsquo;t care about how to shut down services gracefully\nClean up all containers $ docker rm -f $(docker ps -q -a)  We are talking about tests, right? You should have a machine for testing only so it is ok to remove all containers quickly. That does simplify your work and life.\nEnable IPv4 IP Forwarding $ sudo vi /usr/lib/sysctl.d/00-system.conf  Add the following line:\nnet.ipv4.ip_forward=1  Save and exit the file. Restart network:\n$ sudo systemctl restart network  ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592797985,"objectID":"a095b71c24ccdf724b2f764998524595","permalink":"https://oharastream.github.io/en/docs/master/docker/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/master/docker/","section":"docs","summary":"Why we need docker-compose Ohara is good at connecting to various systems to collect, transform, aggregate (and other operations you can imagine) data. In order to test Ohara, we need a way to run a bunch of systems simultaneously.","tags":null,"title":"Docker and Docker-compose","type":"docs"},{"authors":null,"categories":null,"content":"How to deploy ohara integration test to QA environment?\nNode 裡需要安裝的工具   安裝 JDK 11, 需要設定以下的 link\n$ sudo yum install -y java-11-openjdk-devel    在 CentOS 的 Node 需要安裝 jq\n$ sudo yum install -y epel-release $ sudo yum install -y jq    ssh server\n$ sudo yum install -y openssh-server    Docker: Please follow docker official tutorial\n  防火牆設定允許 docker container 的 port, 並且重新 reload 防火牆的服務\n# sudo firewall-cmd --permanent --zone=trusted --add-interface={docker network} # sudo firewall-cmd --reload    Jenkins 需要做的設定   確認 jenkins 是否加入了登入的帳號密碼設定\nCredentials -\u0026gt; global -\u0026gt; Add Credentials -\u0026gt; 輸入 Username, Password, Description. ID 不用輸入 -\u0026gt; OK\n  把 Node 加入到 Jenkins 裡\n管理 Jenkins -\u0026gt; 管理節點 -\u0026gt; 新增節點 -\u0026gt; 輸入節點名稱的 hostname -\u0026gt; 選複製既有節點 -\u0026gt; 複製來源選一台現有的 slave 來輸入, 例如：ohara-it01 -\u0026gt; OK\n  把 Node 加入到 ssh remote hosts\n管理 Jenkins -\u0026gt; 設定系統 -\u0026gt; SSH remote hosts -\u0026gt; 往下拉會看到新增按鈕 -\u0026gt; 之後輸入 Hostname, port, 選 Credentials -\u0026gt; 新增\n  設定 PreTest 和 PreCommit Job\n 修改標籤表示式, 避免 IT 的 node 跑到 UT 上面 增加 node 在 shell script 裡 NODE01_HOSTNAME=\u0026quot;ohara-it02\u0026quot; NODE01_IP=$(getent hosts $NODE01_HOSTNAME | cut -d\u0026quot; \u0026quot; -f 1) NODE01_INFO=\u0026quot;$NODE_USER_NAME:$NODE_PASSWORD@$NODE01_HOSTNAME:22\u0026quot; EXTRA_PROPERTIES=\u0026quot;-Pohara.it.docker=$NODE00_INFO,$NODE01_INFO\u0026quot;   建立 Execute shell script on remote host using ssh, 用來在 IT 的 Node 拉 docker image 新增建置步驟 -\u0026gt; Execute shell script on remote host using ssh -\u0026gt; 輸入拉 docker image 的 command    ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"8dc9c63e6c6f29e9861c52e2496b8303","permalink":"https://oharastream.github.io/en/docs/0.11.x/integration_test/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/integration_test/","section":"docs","summary":"How to deploy ohara integration test to QA environment? Node 裡需要安裝的工具 安裝 JDK 11, 需要設定以下的 link","tags":null,"title":"Integration Test","type":"docs"},{"authors":null,"categories":null,"content":"How to deploy ohara integration test to QA environment?\nNode 裡需要安裝的工具   安裝 JDK 11, 需要設定以下的 link\n$ sudo yum install -y java-11-openjdk-devel    在 CentOS 的 Node 需要安裝 jq\n$ sudo yum install -y epel-release $ sudo yum install -y jq    ssh server\n$ sudo yum install -y openssh-server    Docker: Please follow docker official tutorial\n  防火牆設定允許 docker container 的 port, 並且重新 reload 防火牆的服務\n# sudo firewall-cmd --permanent --zone=trusted --add-interface={docker network} # sudo firewall-cmd --reload    Jenkins 需要做的設定   確認 jenkins 是否加入了登入的帳號密碼設定\nCredentials -\u0026gt; global -\u0026gt; Add Credentials -\u0026gt; 輸入 Username, Password, Description. ID 不用輸入 -\u0026gt; OK\n  把 Node 加入到 Jenkins 裡\n管理 Jenkins -\u0026gt; 管理節點 -\u0026gt; 新增節點 -\u0026gt; 輸入節點名稱的 hostname -\u0026gt; 選複製既有節點 -\u0026gt; 複製來源選一台現有的 slave 來輸入, 例如：ohara-it01 -\u0026gt; OK\n  把 Node 加入到 ssh remote hosts\n管理 Jenkins -\u0026gt; 設定系統 -\u0026gt; SSH remote hosts -\u0026gt; 往下拉會看到新增按鈕 -\u0026gt; 之後輸入 Hostname, port, 選 Credentials -\u0026gt; 新增\n  設定 PreTest 和 PreCommit Job\n 修改標籤表示式, 避免 IT 的 node 跑到 UT 上面 增加 node 在 shell script 裡 NODE01_HOSTNAME=\u0026quot;ohara-it02\u0026quot; NODE01_IP=$(getent hosts $NODE01_HOSTNAME | cut -d\u0026quot; \u0026quot; -f 1) NODE01_INFO=\u0026quot;$NODE_USER_NAME:$NODE_PASSWORD@$NODE01_HOSTNAME:22\u0026quot; EXTRA_PROPERTIES=\u0026quot;-Pohara.it.docker=$NODE00_INFO,$NODE01_INFO\u0026quot;   建立 Execute shell script on remote host using ssh, 用來在 IT 的 Node 拉 docker image 新增建置步驟 -\u0026gt; Execute shell script on remote host using ssh -\u0026gt; 輸入拉 docker image 的 command    ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592797985,"objectID":"267bcf5f467bfae9db5071d7d16b1ee4","permalink":"https://oharastream.github.io/en/docs/master/integration_test/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/master/integration_test/","section":"docs","summary":"How to deploy ohara integration test to QA environment? Node 裡需要安裝的工具 安裝 JDK 11, 需要設定以下的 link","tags":null,"title":"Integration Test","type":"docs"},{"authors":null,"categories":null,"content":"This module contains Ohara manager (an HTTP server powered by Node.js) and Ohara manager client (A web-based user interface built with React.js ). In the following docs, we will refer Server as Ohara manager and Client as Ohara manager client.\nInitial machine setup  Install Node.js 10.16.3 or node.js \u0026gt;=10.16.3 and \u0026lt; 13.0.0 Install Yarn 1.13.0 or greater Make sure you\u0026rsquo;re in the ohara-manager root and use this command to setup the app: yarn setup. This will install all the dependencies for both the Server and Client as well as creating a production build for the client. Optional: If you\u0026rsquo;re using Visual Studio Code as your editor, have a look at our Editors section.  Mac Make sure you have watchman installed on your machine. You can do this with homebrew:\n$ brew install watchman  Linux Install these dependencies for cypress:\n$ yum install -y xorg-x11-server-Xvfb gtk2-2.24* libXtst* libXScrnSaver* GConf2* alsa-lib*  Development  If this is your first time running this project, you need to complete the Initial machine setup section above 👆   Quick start guide Make sure you\u0026rsquo;re at the Ohara manager root, then start it with:\n$ yarn start --configurator ${http://host:port/v0}  Note that the configurator option is required, and you should have configurator running before starting Ohara manager. You can see the user guide on how to spin up a Configurator\nOpen another terminal tab, and start the Client:\n$ yarn start:client  Now, go to http://localhost:3000 and start your development, happy hacking 😎\nFull development guide In development, you need to start both the Ohara manager and Ohara manager client servers before you can start your development. Follow the instructions below:\nServer Make sure you\u0026rsquo;re at the ohara-manager root:\n$ yarn start --configurator ${http://host:port/v0}   Note that the --configurator argument is required, you should pass in Ohara configurator API URL.   You can also override the default port 5050 by passing in --port like the following:\n$ yarn start --configurator ${http://host:port/v0} --port ${1234}  After starting the server, visit http://localhost:${PORT} in your browser.\n Passing the CLI option -c has the same effect as --configurator and -p for --port as well   Double check the --configurator spelling and API URL, the URL should contain the API version number: /v0\nClient Start the Client development server with:\n$ yarn start:client  After starting the dev server, visit http://localhost:3000 in your browser and start you development.\nYou can override the default port 3000 by passing in an environment variable:\n$ PORT=7777 yarn start:client  The dev server will then start at http://localhost:7777\nTesting Unit test You can run Client unit test with a single npm script:\n$ yarn test:unit:ci  Please note that this is a one-off run command, often when you\u0026rsquo;re in the development, you would run test and stay in Jest\u0026rsquo;s watch mode which reloads the test once you save your changes:\n$ yarn test:unit:watch  $ yarn test:unit:ci  This command will generate test coverage reports, which can be found in ohara-manager/client/coverage/ut/\n We will automate check the threshold of code coverage by Statements \u0026gt; 40% if you issued the test:unit:ci command.   API test We\u0026rsquo;re using Cypress to test our RESTful API, this ensures our backend API is always compatible with Ohara manager and won\u0026rsquo;t break our UI (ideally). You can run the test in different modes:\nGUI mode: this will open Cypress test runner, you can then run your test manually through the UI.\n$ yarn test:api:open  Electron mode(headless): since we\u0026rsquo;re running our API test on CI under this mode. You might often want to run your tests in this mode locally as well.\n$ yarn test:api:ci --configurator ${http://host:port/v0 --port 0}    Generated test coverage reports could be found in ohara-manager/client/coverage/api if you executed test with Electron mode. The --port 0 means randomly choose a port for this test run.    We will automate check the threshold of code coverage by Statements \u0026gt; 80% if you issued the test:api:ci command.   IT test To test our UI flows, we use Cypress to test our UI flow with fake configurator. These tests focus on the behaviors of UI flow (by different operations from UI), so they should cover most of our UI logic. You can run the test in different modes:\nGUI mode: this will open Cypress test runner, you can then run your test manually through the UI.\n$ yarn test:it:open  Electron mode(headless): since we\u0026rsquo;re running our API test on CI under this mode. You might often want to run your tests in this mode locally as well.\n$ yarn test:it:ci --configurator ${http://host:port/v0 --port 0}  This command will generate test coverage reports, which can be found in ohara-manager/client/coverage/it/\n We will automate check the threshold of code coverage by Statements \u0026gt; 75% if you issued the test:it:ci command.   End-to-End test Just like API test, our End-to-End test also runs in two different modes:\nGUI mode: this will open Cypress test runner, you can then run your test manually through the UI.\n$ yarn test:e2e:open  Electron mode(headless): since we\u0026rsquo;re running our E2E test on CI under this mode. You might often want to run your tests in this mode locally as well.\n$ yarn test:e2e:ci --configurator ${http://host:port/v0}    Before running in this mode we advise that you run yarn setup prior to the tests as the dev server is not running, so you might have stale build asserts in your build directory\n  You also need to create a cypress.env.json under the /ohara-manager/client/, these are the config that Cypress will be using when running tests:\n  { \u0026quot;nodeHost\u0026quot;: \u0026quot;ohara-dev-01\u0026quot;, \u0026quot;nodePort\u0026quot;: 22, \u0026quot;nodeUser\u0026quot;: \u0026quot;nodeUserName\u0026quot;, \u0026quot;nodePass\u0026quot;: \u0026quot;nodePassword\u0026quot;, \u0026quot;servicePrefix\u0026quot;: \u0026quot;prPrefix\u0026quot; }  Unlike API test, the test should run in production environment  Code Coverage As the above test phases are tend to cover different range of our source code, after you executed the following commands:\n yarn test:unit:ci yarn test:api:ci yarn test:it:ci  we will generate the corresponded coverage reports in relative path as each test section described, and you could combine them to see the overall picture:\n$ yarn report:combined  The combined coverage report could be found at /ohara-manager/client/coverage/index.html\n You could also check the combined report by yourself:\n$ yarn test:coverage:check    Linting We use ESLint to lint all the JavaScript:\nServer:\n$ yarn lint:server  It\u0026rsquo;s usually helpful to run linting while developing and that\u0026rsquo;s included in yarn start command:\n$ yarn start --configurator ${http://host:port/v0}  This will start the server with nodemon and run the linting script whenever nodemon reloads.\nClient:\nSince our client is bootstrapped with create-react-app, so the linting part is already taken care. When starting the Client dev server with yarn start:client, the linting will be starting automatically.\nNote that due to create-react-app doesn\u0026rsquo;t support custom eslint rules. You need to use your text editor plugin to display the custom linting rule warnings or errors. For more info about this, please take a look at the create-react-app docs\nFormat We use Prettier to format our code. You can format all .js files with:\n$ yarn format   You can ignore files or folders when running yarn format by editing the .prettierignore in the Ohara-manager root.  Build Note that this step is only required for the Client NOT THE SERVER\nYou can get production-ready static files by using the following command:\n$ yarn build   These static files will be built and put into the /ohara-manager/client/build directory.   Ohara manager image Run the following command to get the production ready build of both the Server and Client.\n$ yarn setup  After the build, copy/use these files and directories to the destination directory (Note this step is automatically done by Ohara-${module} module):\n start.js config.js client \u0026ndash; only build directory is needed  build   constants node_modules routes utils   Note that if you add new files or dirs to the Server or Client and these files and dirs are required for production build, please list that file in the above list as well as editing the gradle file under ohara/ohara-manager/build.gradle. Skipping this step will cause production build failed!   From the Ohara manager project root, use the following command to start the manager:\n$ yarn start:prod --configurator ${http://host:port/v0}  CI server integration In order to run tests on Jenkins, Ohara manager provides a few npm scripts that are used in Gradle, these scripts generate test reports in /ohara-manager/test-reports which can be consumed by Jenkins to determine if a test passes or not, you will sometimes need to run these commands locally if you edit related npm scripts in Ohara manager or want to reproduce fail build on CI:\nUnit test:\n$ ./gradlew test   Run Client\u0026rsquo;s unit tests. The test reports can be found in ohara-manager/test-reports/  API test:\n$ ./gradlew api -Pohara.manager.api.configurator=${http://host:port/v0}  End-to-End test:\n$ ./gradlew e2e -Pohara.manager.e2e.port=5050 -Pohara.manager.e2e.configurator=${http://host:port/v0}-Pohara.manager.e2e.nodeHost=${slaveNodeName} -Pohara.manager.e2e.nodePort=${slaveNodePort} -Pohara.manager.e2e.nodeUser=${slaveNodeUsername} -Pohara.manager.e2e.nodePass=${slaveNodePassword} -Pohara.it.container.prefix=${pullRequestNumber}  Let\u0026rsquo;s take a close look at these options:\n -Pohara.manager.e2e.port=5050: start Ohara manager at this port in the test run -Pohara.manager.e2e.configurator=${http://host:port/v0}: Configurator URL, Ohara manager will hit this API endpoint when running test -Pohara.manager.e2e.nodeHost=${slaveNodeName}: K8s\u0026rsquo; slave node, the services started in the test will be deploy on this node -Pohara.manager.e2e.nodePort=${slaveNodePort}: port of the given node -Pohara.manager.e2e.nodeUser=${slaveNodeUsername}: username of the given node -Pohara.manager.e2e.nodePass=${slaveNodePassword}: password of the given node -Pohara.it.container.prefix=${pullRequestNumber}: pull request number of this test run, this prefix is used by Jenkins to do the cleanup after the test is done  There are more gradle tasks that are not listed in the above, you can view them in /ohara-manager/build.gradle\nClean Clean up all running processes, removing test-reports/ in the Server and /build directory in the Client:\n$ yarn clean  Clean all running processes started with node.js\n$ yarn clean:process  This is useful when you want to kill all node.js processes locally\nPrepush We also provide a npm script to run Client\u0026rsquo;s unit test, linting, and format all the JavaScript files with. Ideally, you\u0026rsquo;d run this before pushing your code to the remote repo:\n$ yarn prepush  Editors We highly recommended that you use Visual Studio Code (vscode for short) to edit and author Ohara manager code.\nRecommended vscode settings\n{ \u0026quot;editor.tabSize\u0026quot;: 2, \u0026quot;editor.formatOnSave\u0026quot;: true, \u0026quot;editor.formatOnSaveTimeout\u0026quot;: 2000, \u0026quot;editor.tabCompletion\u0026quot;: true, \u0026quot;emmet.triggerExpansionOnTab\u0026quot;: true, \u0026quot;emmet.includeLanguages\u0026quot;: { \u0026quot;javascript\u0026quot;: \u0026quot;javascriptreact\u0026quot;, \u0026quot;markdown\u0026quot;: \u0026quot;html\u0026quot; }, \u0026quot;search.exclude\u0026quot;: { \u0026quot;**/node_modules\u0026quot;: true, \u0026quot;**/bower_components\u0026quot;: true, \u0026quot;**/coverage\u0026quot;: true }, \u0026quot;javascript.updateImportsOnFileMove.enabled\u0026quot;: \u0026quot;always\u0026quot;, \u0026quot;eslint.workingDirectories\u0026quot;: [ \u0026quot;./client\u0026quot; ] }  Recommend extensions\n   ESLint \u0026mdash; install this so vscode can display linting errors right in the editor\n   vscode-styled-components \u0026mdash; syntax highlighting support for styled component\n   Prettier - Code formatter \u0026mdash; code formatter, it consumes the config in .prettierrc\n   DotENV \u0026mdash; .env file syntax highlighting support\n   Color Highlight \u0026mdash; Highlight web colors in VSCode\n  Switch different version of Node.js Oftentimes you would need to switch between different Node.js versions for debugging. There\u0026rsquo;s a handy npm package that can reduce the pain of managing different version of Node.js on your machine:\nFirst, let\u0026rsquo;s install this package n, note that we\u0026rsquo;re installing it globally so it\u0026rsquo;s can be used throughout your projects:\n$ npm install -g n # or yarn global add n  Second, let\u0026rsquo;s use n to install a specific version of Node.js:\n$ n 8.16.0  Switch between installed NodeJS versions:\n$ n # Yep, just type n in your terminal...,  For more info, you can read the docs here.\nHaving issues?   Got an error while starting up the server: Error: Cannot find module ${module-name}\nIf you\u0026rsquo;re running into this, it\u0026rsquo;s probably that this module is not correctly installed on your machine. You can fix this by simply run:\n$ yarn # If this doesn't work, try `yarn add ${module-name}`  After the installation is completed, start the server again.\n  Got an error while starting up the server or client on a Linux machine: ENOSPC\nYou can run this command to increase the limit on the number of files Linux will watch. Read more here.\n$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf \u0026amp;\u0026amp; sudo sysctl -p.    Node.js processes cannot be stopped even after using kill -9\nWe\u0026rsquo;re using forever to start our node.js servers on CI, and nodemon while in development, so you need to use the following commands to kill them. kill -9 or fuser might not work as you expected.\nuse yarn clean:processes command or pkill node to kill all the node.js processes\n  While running test in jest\u0026rsquo;s watch modal, an error is thrown\nError watching file for changes: EMFILE    Try installing watchman for your mac with the instruction\nFor more info: https://github.com/facebook/jest/issues/1767\n  Ohara manager is not able to connect to Configurator And I\u0026rsquo;m seeing something like:\n--configurator: we're not able to connect to `{http://host:port/v0}` Please make sure your Configurator is running at `${http://host:port/v0}` [nodemon] app crashed - waiting for file changes before starting...    This could happen due to several reasons:\n  Configurator hasn\u0026rsquo;t fully started yet: after you start the configurator container. The container needs some time to fully initialize the service. This usually takes about a minute or so. And as we\u0026rsquo;re doing the API check by hitting the real API in Ohara manager. This results to the error in the above.\n  You\u0026rsquo;re not using the correct IP in Manager container: if you start a configurator container in your local as well as a manager. You should specify an IP instead of something like localhost in: --configurator http://localhost:12345/v0 This won\u0026rsquo;t work as the manager is started in the container so it won\u0026rsquo;t be able to connect to the configurator without a real IP\n  As we mentioned in the previous sections. Please double check your configurator URL spelling. This is usually the cause of the above-mentioned error.\n  ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"2ec5e48e7713053b7d25efc21e180897","permalink":"https://oharastream.github.io/en/docs/0.11.x/manager_dev_guide/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/manager_dev_guide/","section":"docs","summary":"This module contains Ohara manager (an HTTP server powered by Node.js) and Ohara manager client (A web-based user interface built with React.js ). In the following docs, we will refer Server as Ohara manager and Client as Ohara manager client.","tags":null,"title":"Ohara Manager Development Guideline","type":"docs"},{"authors":null,"categories":null,"content":"This module contains Ohara manager (an HTTP server powered by Node.js) and Ohara manager client (A web-based user interface built with React.js ). In the following docs, we will refer Server as Ohara manager and Client as Ohara manager client.\nInitial machine setup  Install Node.js 10.16.3 or node.js \u0026gt;=10.16.3 and \u0026lt; 13.0.0 Install Yarn 1.13.0 or greater Make sure you\u0026rsquo;re in the ohara-manager root and use this command to setup the app: yarn setup. This will install all the dependencies for both the Server and Client as well as creating a production build for the client. Optional: If you\u0026rsquo;re using Visual Studio Code as your editor, have a look at our Editors section.  Mac Make sure you have watchman installed on your machine. You can do this with homebrew:\n$ brew install watchman  Linux Install these dependencies for cypress:\n$ yum install -y xorg-x11-server-Xvfb gtk2-2.24* libXtst* libXScrnSaver* GConf2* alsa-lib*  Development  If this is your first time running this project, you need to complete the Initial machine setup section above 👆   Quick start guide Make sure you\u0026rsquo;re at the Ohara manager root, then start it with:\n$ yarn start --configurator ${http://host:port/v0}  Note that the configurator option is required, and you should have configurator running before starting Ohara manager. You can see the user guide on how to spin up a Configurator\nOpen another terminal tab, and start the Client:\n$ yarn start:client  Now, go to http://localhost:3000 and start your development, happy hacking 😎\nFull development guide In development, you need to start both the Ohara manager and Ohara manager client servers before you can start your development. Follow the instructions below:\nServer Make sure you\u0026rsquo;re at the ohara-manager root:\n$ yarn start --configurator ${http://host:port/v0}   Note that the --configurator argument is required, you should pass in Ohara configurator API URL.   You can also override the default port 5050 by passing in --port like the following:\n$ yarn start --configurator ${http://host:port/v0} --port ${1234}  After starting the server, visit http://localhost:${PORT} in your browser.\n Passing the CLI option -c has the same effect as --configurator and -p for --port as well   Double check the --configurator spelling and API URL, the URL should contain the API version number: /v0\nClient Start the Client development server with:\n$ yarn start:client  After starting the dev server, visit http://localhost:3000 in your browser and start you development.\nYou can override the default port 3000 by passing in an environment variable:\n$ PORT=7777 yarn start:client  The dev server will then start at http://localhost:7777\nTesting Unit test You can run Client unit test with a single npm script:\n$ yarn test:unit:ci  Please note that this is a one-off run command, often when you\u0026rsquo;re in the development, you would run the test and stay in Jest\u0026rsquo;s watch mode which reloads the test once you save your changes:\n$ yarn test:unit:watch  $ yarn test:unit:ci  This command will generate test coverage reports, which can be found in ohara-manager/client/coverage/ut/\n We will automatically check the code coverage threshold by Statements \u0026gt; 40% if you issue the test:unit:ci command.   API test We\u0026rsquo;re using Cypress to test our RESTful API, this ensures our backend API is always compatible with Ohara manager and won\u0026rsquo;t break our UI (ideally). You can run the test in different modes:\nGUI mode: this will open Cypress test runner, you can then run your test manually through the UI.\n$ yarn test:api:open  Electron mode(headless): since we\u0026rsquo;re running our API test on CI under this mode. You might often want to run your tests in this mode locally as well.\n$ yarn test:api:ci --configurator ${http://host:port/v0} --port 0    Generated test coverage reports could be found in ohara-manager/client/coverage/api if you executed test with Electron mode. The --port 0 means using a random port.    We will automate check the threshold of code coverage by Statements \u0026gt; 80% if you issued the test:api:ci command.   IT test To test our UI flows, we run these test via Cypress with a fake configurator. These tests focus on the behaviors of UI flow (by different operations from UI), so they should cover most of our UI logic. You can run the test in different modes:\nGUI mode: this will open Cypress test runner, you can then run your test manually through the UI.\n$ yarn test:it:open  Electron mode(headless): since we\u0026rsquo;re running our API test on CI under this mode. You might often want to run your tests in this mode locally as well.\n$ yarn test:it:ci --configurator ${http://host:port/v0} --port 0  This command will generate test coverage reports, which can be found in ohara-manager/client/coverage/it/\n We will automate check the code coverage threshold by Statements \u0026gt; 75% if you issue the test:it:ci command.   End-to-End test Just like API test, our End-to-End test also runs in two different modes:\nGUI mode: this will open Cypress test runner, you can then run your test manually through the UI.\n$ yarn test:e2e:open  Electron mode(headless): since we\u0026rsquo;re running our E2E test on CI under this mode. You might often want to run your tests in this mode locally as well.\n$ yarn test:e2e:ci --configurator ${http://host:port/v0}    Before running in this mode we advise that you run yarn setup prior to the tests as the dev server is not running, so you might have stale build asserts in your build directory\n  You also need to create a cypress.env.json under the /ohara-manager/client/, these are the config that Cypress will be using when running tests:\n  { \u0026quot;nodeHost\u0026quot;: \u0026quot;ohara-dev-01\u0026quot;, \u0026quot;nodePort\u0026quot;: 22, \u0026quot;nodeUser\u0026quot;: \u0026quot;nodeUserName\u0026quot;, \u0026quot;nodePass\u0026quot;: \u0026quot;nodePassword\u0026quot;, \u0026quot;servicePrefix\u0026quot;: \u0026quot;prPrefix\u0026quot; }  Unlike API test, the test should run in production environment  Code Coverage As the above test phases are tend to cover different range of our source code, after you executed the following commands:\n yarn test:unit:ci yarn test:api:ci yarn test:it:ci  we will generate the corresponded coverage reports in relative path as each test section described, and you could combine them to see the overall picture:\n$ yarn report:combined  The combined coverage report could be found at /ohara-manager/client/coverage/index.html\n You could also check the combined report by yourself:\n$ yarn test:coverage:check    Linting We use ESLint to lint all the JavaScript:\nServer:\n$ yarn lint:server  It\u0026rsquo;s usually helpful to run linting while developing and that\u0026rsquo;s included in yarn start command:\n$ yarn start --configurator ${http://host:port/v0}  This will start the server with nodemon and run the linting script whenever nodemon reloads.\nClient:\nSince our client is bootstrapped with create-react-app, so the linting part is already taken care. When starting the Client dev server with yarn start:client, the linting will be starting automatically.\nNote that due to create-react-app doesn\u0026rsquo;t support custom eslint rules. You need to use your text editor plugin to display the custom linting rule warnings or errors. For more info about this, please take a look at the create-react-app docs\nFormat We use Prettier to format our code. You can format all .js files with:\n$ yarn format   You can ignore files or folders when running yarn format by editing the .prettierignore in the Ohara-manager root.  Type checking We\u0026rsquo;re in the progress of moving from plain JavaScript to TypeScript. Although not all source are moved to TS, you can still type checking files:\n$ yarn typecheck  License We\u0026rsquo;re enforcing all of our source code files to have a license header, so when creating a new file like .js or .tsx etc. You will need to include the license header or it would fail on our CI. We provide two npm scripts that your can take advantage of and so you won\u0026rsquo;t need to add the license header manually\nChecking if all files include the license header\n$ yarn license:test  Add license header for files that miss it\n$ license:apply  Build Note that this step is only required for the Client NOT THE SERVER\nYou can get production-ready static files by using the following command:\n$ yarn build   These static files will be built and put into the /ohara-manager/client/build directory.   Ohara manager image Run the following command to get the production ready build of both the Server and Client.\n$ yarn setup  After the build, copy/use these files and directories to the destination directory (Note this step is automatically done by Ohara-${module} module):\n start.js config.js client \u0026ndash; only build directory is needed  build   constants node_modules routes utils   Note that if you add new files or dirs to the Server or Client and these files and dirs are required for production build, please list that file in the above list as well as editing the gradle file under ohara/ohara-manager/build.gradle. Skipping this step will cause production build failed!   From the Ohara manager project root, use the following command to start the manager:\n$ yarn start:prod --configurator ${http://host:port/v0}  CI server integration In order to run tests on Jenkins, Ohara manager provides a few npm scripts that are used in Gradle, these scripts generate test reports in /ohara-manager/test-reports which can be consumed by Jenkins to determine if a test passes or not, you will sometimes need to run these commands locally if you edit related npm scripts in Ohara manager or want to reproduce fail build on CI:\nUnit test:\n$ ./gradlew test   Run Client\u0026rsquo;s unit tests. The test reports can be found in ohara-manager/test-reports/  API test:\n$ ./gradlew api -Pohara.manager.api.configurator=${http://host:port/v0}  End-to-End test:\n$ ./gradlew e2e -Pohara.manager.e2e.port=5050 -Pohara.manager.e2e.configurator=${http://host:port/v0}-Pohara.manager.e2e.nodeHost=${slaveNodeName} -Pohara.manager.e2e.nodePort=${slaveNodePort} -Pohara.manager.e2e.nodeUser=${slaveNodeUsername} -Pohara.manager.e2e.nodePass=${slaveNodePassword} -Pohara.it.container.prefix=${pullRequestNumber}  Let\u0026rsquo;s take a close look at these options:\n -Pohara.manager.e2e.port=5050: start Ohara manager at this port in the test run -Pohara.manager.e2e.configurator=${http://host:port/v0}: Configurator URL, Ohara manager will hit this API endpoint when running test -Pohara.manager.e2e.nodeHost=${slaveNodeName}: K8s\u0026rsquo; slave node, the services started in the test will be deploy on this node -Pohara.manager.e2e.nodePort=${slaveNodePort}: port of the given node -Pohara.manager.e2e.nodeUser=${slaveNodeUsername}: username of the given node -Pohara.manager.e2e.nodePass=${slaveNodePassword}: password of the given node -Pohara.it.container.prefix=${pullRequestNumber}: pull request number of this test run, this prefix is used by Jenkins to do the cleanup after the test is done  There are more gradle tasks that are not listed in the above, you can view them in /ohara-manager/build.gradle\nClean Clean all running processes started with node.js\n$ yarn clean:processes  This is useful when you want to kill all node.js processes locally\nClean test test reports, coverage reports and build artifacts\n$ yarn clean:files  Remove both node_modules from Server and Client, this is useful when you want to do a clean install\n$ yarn clean:deps  Prepush We also provide a npm script to run Client\u0026rsquo;s unit test, linting, and format all the JavaScript files with. Ideally, you\u0026rsquo;d run this before pushing your code to the remote repo:\n$ yarn prepush  Editors We highly recommended that you use Visual Studio Code (vscode for short) to edit and author Ohara manager code.\nRecommended vscode settings\n{ \u0026quot;editor.tabSize\u0026quot;: 2, \u0026quot;editor.formatOnSave\u0026quot;: true, \u0026quot;editor.formatOnSaveTimeout\u0026quot;: 2000, \u0026quot;editor.tabCompletion\u0026quot;: true, \u0026quot;emmet.triggerExpansionOnTab\u0026quot;: true, \u0026quot;emmet.includeLanguages\u0026quot;: { \u0026quot;javascript\u0026quot;: \u0026quot;javascriptreact\u0026quot;, \u0026quot;markdown\u0026quot;: \u0026quot;html\u0026quot; }, \u0026quot;search.exclude\u0026quot;: { \u0026quot;**/node_modules\u0026quot;: true, \u0026quot;**/bower_components\u0026quot;: true, \u0026quot;**/coverage\u0026quot;: true }, \u0026quot;javascript.updateImportsOnFileMove.enabled\u0026quot;: \u0026quot;always\u0026quot;, \u0026quot;eslint.workingDirectories\u0026quot;: [\u0026quot;./client\u0026quot;] }  Recommend extensions\n   ESLint \u0026mdash; install this so vscode can display linting errors right in the editor\n   vscode-styled-components \u0026mdash; syntax highlighting support for styled component\n   Prettier - Code formatter \u0026mdash; code formatter, it consumes the config in .prettierrc\n   DotENV \u0026mdash; .env file syntax highlighting support\n   Color Highlight \u0026mdash; Highlight web colors in VSCode\n  Switch different version of Node.js Oftentimes you would need to switch between different Node.js versions for debugging. There\u0026rsquo;s a handy npm package that can reduce the pain of managing different version of Node.js on your machine:\nFirst, let\u0026rsquo;s install this package n, note that we\u0026rsquo;re installing it globally so it\u0026rsquo;s can be used throughout your projects:\n$ npm install -g n # or yarn global add n  Second, let\u0026rsquo;s use n to install a specific version of Node.js:\n$ n 8.16.0  Switch between installed NodeJS versions:\n$ n # Yep, just type n in your terminal...,  For more info, you can read the docs here.\nHaving issues?   Got an error while starting up the server: Error: Cannot find module ${module-name}\nIf you\u0026rsquo;re running into this, it\u0026rsquo;s probably that this module is not correctly installed on your machine. You can fix this by simply run:\n$ yarn # If this doesn't work, try `yarn add ${module-name}`  After the installation is completed, start the server again.\n  Got an error while starting up the server or client on a Linux machine: ENOSPC\nYou can run this command to increase the limit on the number of files Linux will watch. Read more here.\n$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf \u0026amp;\u0026amp; sudo sysctl -p.    Node.js processes cannot be stopped even after using kill -9\nWe\u0026rsquo;re using forever to start our node.js servers on CI, and nodemon while in development, so you need to use the following commands to kill them. kill -9 or fuser might not work as you expected.\nuse yarn clean:processes command or pkill node to kill all the node.js processes\n  While running test in jest\u0026rsquo;s watch modal, an error is thrown\nError watching file for changes: EMFILE    Try installing watchman for your mac with the instruction\nFor more info: https://github.com/facebook/jest/issues/1767\n  Ohara manager is not able to connect to Configurator And I\u0026rsquo;m seeing something like:\n--configurator: we're not able to connect to `{http://host:port/v0}` Please make sure your Configurator is running at `${http://host:port/v0}` [nodemon] app crashed - waiting for file changes before starting...    This could happen due to several reasons:\n  Configurator hasn\u0026rsquo;t fully started yet: after you start the configurator container. The container needs some time to fully initialize the service. This usually takes about a minute or so. And as we\u0026rsquo;re doing the API check by hitting the real API in Ohara manager. This results to the error in the above.\n  You\u0026rsquo;re not using the correct IP in Manager container: if you start a configurator container in your local as well as a manager. You should specify an IP instead of something like localhost in: --configurator http://localhost:12345/v0 This won\u0026rsquo;t work as the manager is started in the container so it won\u0026rsquo;t be able to connect to the configurator without a real IP\n  As we mentioned in the previous sections. Please double check your configurator URL spelling. This is usually the cause of the above-mentioned errors.\n  ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606448100,"objectID":"9bd27ba48f8b75e9eb3361cd586d2369","permalink":"https://oharastream.github.io/en/docs/master/manager_dev_guide/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/master/manager_dev_guide/","section":"docs","summary":"This module contains Ohara manager (an HTTP server powered by Node.js) and Ohara manager client (A web-based user interface built with React.js ). In the following docs, we will refer Server as Ohara manager and Client as Ohara manager client.","tags":null,"title":"Ohara Manager Development Guideline","type":"docs"},{"authors":null,"categories":null,"content":"Shabondi service play the role of a http proxy service in the Pipeline of Ohara. If you want to integrate Ohara pipeline with your application, Shabondi is a good choice. Just send the simple http request to Shabondi source service, you can hand over your data to Pipeline for processing. On the other hand, you can send the http request to Shabondi sink service to fetch the output data of the Pipeline.\nFollowing is a simple diagram of Pipeline to demonstrate about both the source and sink of Shabondi:\n Shabondi Pipeline   Data format Both Shabondi source and sink use Row in JSON format for data input and output. Row is a table structure data defined in Ohara code base. A row is comprised of multiple cells. Each cell has its name and value. Every row of your input data will be stored in the Topic.\nA table:\n   row # name age email career     (row 1) jason 42 jason@example.com Surgeon   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   (row n) \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    Example of row using Java:\nimport oharastream.ohara.common.data.Row; import oharastream.ohara.common.data.Cell; class ExampleOfRow { public static void main(String[] args) { Row row = Row.of( Cell.of(\u0026quot;name\u0026quot;, \u0026quot;jason\u0026quot;), Cell.of(\u0026quot;age\u0026quot;, \u0026quot;42\u0026quot;), Cell.of(\u0026quot;email\u0026quot;, \u0026quot;jason@example.com\u0026quot;), Cell.of(\u0026quot;career\u0026quot;, \u0026quot;Surgeon\u0026quot;) ); } }  Example of row in JSON format\n{ \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 42, \u0026quot;email\u0026quot;: \u0026quot;jason@example.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Surgeon\u0026quot; }  Deployment There are two ways to deploy Shabondi service\n Ohara Manager: (TBD) Use Configurator REST API to create and start Shabondi service.  After a Shabondi service is properly configured, deploy and successfully started. It\u0026rsquo;s ready to receive or send requests via HTTP.\nService REST API Shabondi source service receives single row message through HTTP requests and then writes to the connected Topic.\nSource API Send Row Send a JSON data of Row to Shabondi source service.\n  Request\nPOST /\n  Request body\nThe row data in JSON format\n  Example 1 (Succeed)\n  Request\nPOST http://node00:58456 HTTP/1.1 Content-Type: application/json { \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 42, \u0026quot;email\u0026quot;: \u0026quot;jason@example.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Surgeon\u0026quot; }    Response\nHTTP/1.1 200 OK Server: akka-http/10.1.11 Date: Tue, 19 May 2020 02:41:20 GMT Content-Type: text/plain; charset=UTF-8 Content-Length: 2 OK      Example 2 (Failure)\n  Request\nGET http://node00:58456 HTTP/1.1    Response\nHTTP/1.1 405 Method Not Allowed Server: akka-http/10.1.11 Date: Tue, 19 May 2020 02:45:56 GMT Content-Type: text/plain; charset=UTF-8 Content-Length: 90 Unsupported method, please reference: https://oharastream.github.io/docs/master/shabondi/      Sink API The Shabondi Sink service accepts the http request, and then reads the rows from the connected Topic and response it in JSON format.\nFetch Rows   Request\nGET /groups/$groupName\n  Response\nThe array of row in JSON format\n  Example 1 (Succeed)\n  Request\nGET http://node00:58458/groups/g1 HTTP/1.1    Response\nHTTP/1.1 200 OK Server: akka-http/10.1.11 Date: Wed, 20 May 2020 06:18:44 GMT Content-Type: application/json Content-Length: 115 [ { \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 42, \u0026quot;email\u0026quot;: \u0026quot;jason@example.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Surgeon\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;robert\u0026quot;, \u0026quot;age\u0026quot;: 36, \u0026quot;email\u0026quot;: \u0026quot;robert99@gmail.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Teacher\u0026quot; } ]      Example 2 - Failure response(Illegal group name)\n  Request\nGET http://node00:58458/groups/g1-h HTTP/1.1    Response\nHTTP/1.1 406 Not Acceptable Server: akka-http/10.1.11 Date: Wed, 20 May 2020 07:34:10 GMT Content-Type: text/plain; charset=UTF-8 Content-Length: 50 Illegal group name, only accept alpha and numeric.      ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"b40c1760f243cf61444cd9737f32cf7c","permalink":"https://oharastream.github.io/en/docs/0.11.x/shabondi/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/shabondi/","section":"docs","summary":"Shabondi service play the role of a http proxy service in the Pipeline of Ohara. If you want to integrate Ohara pipeline with your application, Shabondi is a good choice. Just send the simple http request to Shabondi source service, you can hand over your data to Pipeline for processing.","tags":null,"title":"Shabondi","type":"docs"},{"authors":null,"categories":null,"content":"Shabondi service play the role of a http proxy service in the Pipeline of Ohara. If you want to integrate Ohara pipeline with your application, Shabondi is a good choice. Just send the simple http request to Shabondi source service, you can hand over your data to Pipeline for processing. On the other hand, you can send the http request to Shabondi sink service to fetch the output data of the Pipeline.\nFollowing is a simple diagram of Pipeline to demonstrate about both the source and sink of Shabondi:\n Shabondi Pipeline   Data format Both Shabondi source and sink use Row in JSON format for data input and output. Row is a table structure data defined in Ohara code base. A row is comprised of multiple cells. Each cell has its name and value. Every row of your input data will be stored in the Topic.\nA table:\n   row # name age email career     (row 1) jason 42 jason@example.com Surgeon   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   (row n) \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    Example of row using Java:\nimport oharastream.ohara.common.data.Row; import oharastream.ohara.common.data.Cell; class ExampleOfRow { public static void main(String[] args) { Row row = Row.of( Cell.of(\u0026quot;name\u0026quot;, \u0026quot;jason\u0026quot;), Cell.of(\u0026quot;age\u0026quot;, \u0026quot;42\u0026quot;), Cell.of(\u0026quot;email\u0026quot;, \u0026quot;jason@example.com\u0026quot;), Cell.of(\u0026quot;career\u0026quot;, \u0026quot;Surgeon\u0026quot;) ); } }  Example of row in JSON format\n{ \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 42, \u0026quot;email\u0026quot;: \u0026quot;jason@example.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Surgeon\u0026quot; }  Deployment There are two ways to deploy Shabondi service\n Ohara Manager: (TBD) Use Configurator REST API to create and start Shabondi service.  After a Shabondi service is properly configured, deploy and successfully started. It\u0026rsquo;s ready to receive or send requests via HTTP.\nService REST API Shabondi source service receives single row message through HTTP requests and then writes to the connected Topic.\nSource API Send Row Send a JSON data of Row to Shabondi source service.\n  Request\nPOST /\n  Request body\nThe row data in JSON format\n  Example 1 (Succeed)\n  Request\nPOST http://node00:58456 HTTP/1.1 Content-Type: application/json { \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 42, \u0026quot;email\u0026quot;: \u0026quot;jason@example.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Surgeon\u0026quot; }    Response\nHTTP/1.1 200 OK Server: akka-http/10.1.11 Date: Tue, 19 May 2020 02:41:20 GMT Content-Type: text/plain; charset=UTF-8 Content-Length: 2 OK      Example 2 (Failure)\n  Request\nGET http://node00:58456 HTTP/1.1    Response\nHTTP/1.1 405 Method Not Allowed Server: akka-http/10.1.11 Date: Tue, 19 May 2020 02:45:56 GMT Content-Type: text/plain; charset=UTF-8 Content-Length: 90 Unsupported method, please reference: https://oharastream.github.io/docs/master/shabondi/      Sink API The Shabondi Sink service accepts the http request, and then reads the rows from the connected Topic and response it in JSON format.\nFetch Rows   Request\nGET /groups/$groupName\n  Response\nThe array of row in JSON format\n  Example 1 (Succeed)\n  Request\nGET http://node00:58458/groups/g1 HTTP/1.1    Response\nHTTP/1.1 200 OK Server: akka-http/10.1.11 Date: Wed, 20 May 2020 06:18:44 GMT Content-Type: application/json Content-Length: 115 [ { \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 42, \u0026quot;email\u0026quot;: \u0026quot;jason@example.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Surgeon\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;robert\u0026quot;, \u0026quot;age\u0026quot;: 36, \u0026quot;email\u0026quot;: \u0026quot;robert99@gmail.com\u0026quot;, \u0026quot;career\u0026quot;: \u0026quot;Teacher\u0026quot; } ]      Example 2 - Failure response(Illegal group name)\n  Request\nGET http://node00:58458/groups/g1-h HTTP/1.1    Response\nHTTP/1.1 406 Not Acceptable Server: akka-http/10.1.11 Date: Wed, 20 May 2020 07:34:10 GMT Content-Type: text/plain; charset=UTF-8 Content-Length: 50 Illegal group name, only accept alpha and numeric.      ","date":1592262000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597829291,"objectID":"e21bd2f5c5d86e3db9ee78fed35dd6ab","permalink":"https://oharastream.github.io/en/docs/master/shabondi/","publishdate":"2020-06-16T00:00:00+01:00","relpermalink":"/en/docs/master/shabondi/","section":"docs","summary":"Shabondi service play the role of a http proxy service in the Pipeline of Ohara. If you want to integrate Ohara pipeline with your application, Shabondi is a good choice. Just send the simple http request to Shabondi source service, you can hand over your data to Pipeline for processing.","tags":null,"title":"Shabondi","type":"docs"},{"authors":null,"categories":null,"content":"All we love is only pull request so we have some rules used to make your PR looks good for reviewers.\n Note that you should file a new issue to discuss the PR detail with us before submitting a PR.   Quick start  Fork and clone the oharastream/ohara repo Install dependencies. See our how_to_build_Ohara for development machine setup Create a branch with your PR with:\ngit checkout -b ${your-branch-name} Push your PR to remote:\ngit push origin ${your-branch-name} Create the PR with GitHub web UI and wait for reviews from our committers  Testing commands in the pull request These commands will come in handy when you want to test your PR on our QA(CI server). To start a QA run, you can simply leave a comment with one of the following commands in the PR:\n Note that the comment should contain the exact command as listed below, comments like Please retry my PR or Bot, retry -fae won\u0026rsquo;t work:     retry\ntrigger a full QA run\n  retry -fae\ntrigger a full QA run even if there\u0026rsquo;s fail test during the run\n  retry ${moduleName}\ntrigger a QA run for a specific module. If a module is named ohara-awesome, you can enter retry awesome to run the QA against this specific module. Note that the module prefix ohara- is not needed. Following are some examples:\n retry manager: run ohara-manager\u0026rsquo;s unit test. retry configurator: run ohara-configurator\u0026rsquo;s unit test.  Ohara manager has a couple of different tests and can be run separately by using the above-mentioned retry command.\n retry manager-api: run manager\u0026rsquo;s API tests retry manager-ut: run manager\u0026rsquo;s unit tests retry manager-e2e: run manager\u0026rsquo;s end-to-end tests    run: start both Configurator and Manager on jenkins server. If the specified PR makes some changes to UI, you can run this command to see the changes.\n  The QA build status can be seen at the bottom of your PR.\nImportant things about pull request A pull request must\u0026hellip;\n  Pass all tests\n  Your PR should not make ohara unstable, if it does. It should be reverted ASAP.\n  You can either run these tests on your local (see our how_to_build_Ohara for more info on how to run tests) or by opening the PR on our repo. These tests will be running on your CI server.\n  Pass code style check. You can automatically fix these issues with a single command:\n$ ./gradlew spotlessApply    Address all reviewers\u0026rsquo; comments\n  A pull request should\u0026hellip;\n Be as small in scope as possible. Large PR is often hard to review. Add new tests  A pull request should not\u0026hellip;\n Bring in new libraries (or updating libraries to new version) without prior discussion. Do not update the dependencies unless you have a good reason. Bring in the new module without prior discussion Bring in new APIs for Configurator without prior discussion  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"72a82bbdcfc0a4a0044f2a019264cfcf","permalink":"https://oharastream.github.io/en/docs/0.11.x/contributing/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/contributing/","section":"docs","summary":"All we love is only pull request so we have some rules used to make your PR looks good for reviewers.\n Note that you should file a new issue to discuss the PR detail with us before submitting a PR.","tags":null,"title":"Contributing","type":"docs"},{"authors":null,"categories":null,"content":"All we love is only pull request so we have some rules used to make your PR looks good for reviewers.\n Note that you should file a new issue to discuss the PR detail with us before submitting a PR.   Quick start  Fork and clone the oharastream/ohara repo Install dependencies. See our how_to_build_Ohara for development machine setup Create a branch with your PR with:\ngit checkout -b ${your-branch-name} Push your PR to remote:\ngit push origin ${your-branch-name} Create the PR with GitHub web UI and wait for reviews from our committers  Testing commands in the pull request These commands will come in handy when you want to test your PR on our QA(CI server). To start a QA run, you can simply leave a comment with one of the following commands in the PR:\n Note that the comment should contain the exact command as listed below, comments like Please retry my PR or Bot, retry -fae won\u0026rsquo;t work:     retry\ntrigger a full QA run\n  retry -fae\ntrigger a full QA run even if there\u0026rsquo;s fail test during the run\n  retry ${moduleName}\ntrigger a QA run for a specific module. If a module is named ohara-awesome, you can enter retry awesome to run the QA against this specific module. Note that the module prefix ohara- is not needed. Following are some examples:\n retry manager: run ohara-manager\u0026rsquo;s unit test. retry configurator: run ohara-configurator\u0026rsquo;s unit test.  Ohara manager has a couple of different tests and can be run separately by using the above-mentioned retry command.\n retry manager-api: run manager\u0026rsquo;s API tests retry manager-ut: run manager\u0026rsquo;s unit tests retry manager-e2e: run manager\u0026rsquo;s end-to-end tests    run: start both Configurator and Manager on jenkins server. If the specified PR makes some changes to UI, you can run this command to see the changes.\n  The QA build status can be seen at the bottom of your PR.\nImportant things about pull request A pull request must\u0026hellip;\n  Pass all tests\n  Your PR should not make ohara unstable, if it does. It should be reverted ASAP.\n  You can either run these tests on your local (see our how_to_build_Ohara for more info on how to run tests) or by opening the PR on our repo. These tests will be running on your CI server.\n  Pass code style check. You can automatically fix these issues with a single command:\n$ ./gradlew spotlessApply    Address all reviewers\u0026rsquo; comments\n  A pull request should\u0026hellip;\n Be as small in scope as possible. Large PR is often hard to review. Add new tests  A pull request should not\u0026hellip;\n Bring in new libraries (or updating libraries to new version) without prior discussion. Do not update the dependencies unless you have a good reason. Bring in the new module without prior discussion Bring in new APIs for Configurator without prior discussion  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592897539,"objectID":"06be7bdc2aed8a91e4a5262941d25fce","permalink":"https://oharastream.github.io/en/docs/master/contributing/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/master/contributing/","section":"docs","summary":"All we love is only pull request so we have some rules used to make your PR looks good for reviewers.\n Note that you should file a new issue to discuss the PR detail with us before submitting a PR.","tags":null,"title":"Contributing","type":"docs"},{"authors":null,"categories":null,"content":"  Create a new workspace with three nodes  Create two topic and upload a stream app jar into the workspace  Create a new pipeline, add some connectors, topics and a stream app  FTP source -\u0026gt; Topic -\u0026gt; Stream -\u0026gt; Topic -\u0026gt; HDFS sink  Prepare the required folders and test data on the FTP server  Start all connectors and stream app  Verify which test data was successfully dumped to HDFS  Create a new workspace with three nodes Add three nodes:\n On the Nodes page, click the NEW NODE button. Enter \u0026lt;ohara_node_host\u0026gt; in the Node field. Enter \u0026lt;ohara_node_port\u0026gt; in the Port field. Enter \u0026lt;node_user\u0026gt; in the User field. Enter \u0026lt;node_password\u0026gt; in the Password field. Click TEST CONNECTION to test your connection. If the test passes, click ADD. Repeat the above steps to add three nodes.  Create a new workspace:\n On the Workspaces page, click the NEW WORKSPACE button. Enter “wk00” in the Name field. Select all available nodes from the Node List. Click ADD.  Create two topics and upload a stream app jar in the workspace Add two topics:\n On the Workspaces \u0026gt; wk00 \u0026gt; TOPICS tab, click the NEW TOPIC button. Enter “t1” in the Topic name field and enter default value in other fields, click ADD. Enter “t2” in the Topic name field and enter default value in other fields, click ADD.  Add a stream jar:\n On the Workspaces \u0026gt; wk00 \u0026gt; STREAM JARS tab, click the NEW JAR button. Browse to the location of your jar file ohara-it-stream.jar, click the file, and click Open.  if You don\u0026rsquo;t know how to build a stream app jar, see this link for how\n   Create a new pipeline, add some connectors, topics and a stream app  On the Pipelines list page, click the NEW PIPELINE button. Enter “firstpipeline” in the Pipeline name field and select “wk00” from the Workspace name dropdown. Then, click ADD. Click the Add a source connector icon and select oharastream.ohara.connector.ftp.FTPSource from the list, then click ADD. Enter “ftpsource” in the myconnector field and click ADD. Click the Add a topic icon and select t1 from the dropdown and click ADD. Click the Add a stream app icon and select ohara-it-stream.jar from the dropdown, then click ADD. Enter “dumb” in the mystream field and click ADD. Click the Add a topic icon and select t2 from the dropdown, then click ADD. Click the Add a sink connector icon and select oharastream.ohara.connector.hdfs.sink.HDFSSink from the list, then click ADD. Enter “hdfssink” in the myconnector field and click ADD.  FTP source -\u0026gt; Topic -\u0026gt; Stream -\u0026gt; Topic -\u0026gt; HDFS sink Set up ftpsource connector:\n On the firstpipeline page, click the ftpsource graph in the pipeline graph. Select the COMMON tab and fill out the following config:  Enter “/demo/input” in the Input Folder field. Enter “/demo/completed” in the Completed Folder field. Enter “/demo/error” in the Error Folder field. Enter \u0026lt;ftp_server_ip\u0026gt; in the Hostname of Ftp Server field. Enter \u0026lt;ftp_server_port\u0026gt; in the Port of Ftp Server field. Enter ftp_username in the User of Ftp Server field. Enter ftp_password in the Password of Ftp Server field.   Select the CORE tab and choose t1 from the Topics dropdown.  Set up dumb stream app:\n Click the dumb graph in the pipeline graph. Enter “ohara1” to filter value Click the CORE tab. Select t1 from the From topic of data consuming from dropdown. Select t2 from the To topic of data produce to dropdown.  Set up hdfssink connector:\n Click the hdfssink graph in the pipeline graph. Select the COMMON tab and fill out the following config:  Enter “/data“ in the Output Folder field Change Flush Size value from “1000” to “5” Check the File Need Header checkbox Enter “hdfs://\u0026lt;hdfs_host\u0026gt;:\u0026lt;hdfs_port\u0026gt;” in the HDSF URL field   Select the CORE tab and choose t2 from the Topics dropdown.  Prepare the required folders and test data on the FTP server  Open a terminal, login to FTP server (or use a FTP client of your choice)  $ ftp `ftp_server_ip` Name: `ftp_username` Password: `ftp_password`  Create the following folders on your FTP server  ftp\u0026gt; mkdir demo ftp\u0026gt; cd demo ftp\u0026gt; mkdir input ftp\u0026gt; mkdir completed ftp\u0026gt; mkdir error ftp\u0026gt; bye  Copy the test file demo.csv to demo/input folder. See this link to create demo CSV files  Start all connectors and stream app  On the firstpipeline page. Click the Start pipeline icon.  Verify which test data was successfully dumped to HDFS  Open a terminal and ssh to HDFS server. List all CSV files in /data/wk00-t2/partition0 folder:  $ hdfs dfs -ls /data/wk00-t2/partition0 # You should see something similar like this in your terminal: /data/wk00-t2/partition0/part-000000000-000000005.csv  View the content of part-000000000-000000005.csv:  $ hdfs dfs -cat /data/wk00-t2/partition0/part-000000000-000000005.csv # The result should be like the following: ID,NAME,CREATE_AT 1,ohara1,2019-03-01 00:00:01  How to create demo.csv?  Create the txt copy the date save to demo.csv  ID,NAME,CREATE_AT 1,ohara1,2019-03-01 00:00:01 2,ohara2,2019-03-01 00:00:02 3,ohara3,2019-03-01 00:00:03 4,ohara4,2019-03-01 00:00:04 5,ohara5,2019-03-01 00:00:05  How to get ohara-it-stream.jar? Open a new terminal from your machine and go to Ohara\u0026rsquo;s source folder.\ncd ohara/  Then cd to Stream DumbStream source folder.\ncd ohara-it/src/main/scala/oharastream/ohara/it/stream/  Use Vi to edit DumbStream.scala\nvi DumbStream.scala  Enter \u0026ldquo;I\u0026rdquo; key from your keyboard to activate the \u0026ldquo;INSERT\u0026rdquo; mode\n-- INSERT --  Overwrite DumbStream class from\nclass DumbStream extends Stream { override def start(ostream: OStream[Row], configs: StreamDefinitions): Unit = { // do nothing but only start stream and write exactly data to output topic ostream.start() } }  To\n/* * Copyright 2019 is-land * * Licensed under the Apache License, Version 2.0 (the \u0026quot;License\u0026quot;); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \u0026quot;AS IS\u0026quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package oharastream.ohara.it.stream import oharastream.ohara.common.data.Row import oharastream.ohara.common.setting.SettingDef import oharastream.ohara.stream.config.StreamSetting import oharastream.ohara.stream.{OStream, Stream} class DumbStream extends Stream { override def config(): StreamDefinitions = StreamDefinitions .`with`( SettingDef .builder() .key(\u0026quot;filterValue\u0026quot;) .displayName(\u0026quot;filter value\u0026quot;) .documentation(\u0026quot;filter the row that contains this value\u0026quot;) .build()) override def start(ostream: OStream[Row], configs: StreamDefinitions): Unit = { ostream // we filter row which the cell values contain the pre-defined \u0026quot;filterValue\u0026quot;. Note: // 1) configs.string(\u0026quot;filterValue\u0026quot;) will try to get the value from env and it should NOT be null // 2) we ignore case for the value (i.e., \u0026quot;aa\u0026quot; = \u0026quot;AA\u0026quot;) .filter( r =\u0026gt; r.cells() .stream() .anyMatch(c =\u0026gt; { c.value().toString.equalsIgnoreCase(configs.string(\u0026quot;filterValue\u0026quot;)) })) .start() } }  Press \u0026ldquo;Esc\u0026rdquo; key to leave the insert mode and enter :wq to save and exit from this file:\n:wq  Build stream jar\n# Make sure you're at the project root `/ohara`, then build the jar with: ./gradlew clean :ohara-it:jar -PskipManager  Go to stream jar folder and list the jars that you have\ncd ohara-it/build/libs/ \u0026amp;\u0026amp; ls # You should see something like this in your terminal: ohara-it-sink.jar ohara-it-source.jar ohara-it-stream.jar  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592818616,"objectID":"d49b541c49ccacb4891fb9a6e6ea7fb5","permalink":"https://oharastream.github.io/en/testcases/pipeline-ftp-topic-stream-hdfs/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/testcases/pipeline-ftp-topic-stream-hdfs/","section":"testcases","summary":"Create a new workspace with three nodes  Create two topic and upload a stream app jar into the workspace  Create a new pipeline, add some connectors, topics and a stream app  FTP source -\u0026gt; Topic -\u0026gt; Stream -\u0026gt; Topic -\u0026gt; HDFS sink  Prepare the required folders and test data on the FTP server  Start all connectors and stream app  Verify which test data was successfully dumped to HDFS  Create a new workspace with three nodes Add three nodes:","tags":null,"title":"FTP to Topic to Stream to HDFS","type":"docs"},{"authors":null,"categories":null,"content":"Prerequisites  OpenJDK 11 Scala 2.13.2 Gradle 6.5 Node.js 8.12.0 Yarn 1.13.0 or greater Docker 19.03.8 (Docker multi-stage, which is supported by Docker 17.05 or higher, is required in building ohara images. see Use multi-stage builds for more details) Kubernetes 1.18.1 (Official QA uses the Kubernetes version is 1.18.1)   Gradle Commands Ohara build is based on gradle. Ohara has defined many gradle tasks to simplify the development of Ohara.\nBuild Binary $ ./gradlew clean build -x test   the tar file is located at ohara-${module}/build/distributions   Run All UTs $ ./gradlew clean test   Ohara IT tests requires specific envs, and all IT tests will be skipped if you don\u0026rsquo;t pass the related setting to IT. Ohara recommends you testing your code on official QA which offers the powerful machine and IT envs.    add flag \u0026ldquo;-PskipManager\u0026rdquo; to skip the tests of Ohara Manager. Ohara Manager is a module in charge of Ohara UI. Feel free to skip it if you are totally a backend developer. By the way, the prerequisites of testing Ohara Manager is shown in here \u0026lt;managerdev\u0026gt;    add flag \u0026ldquo;-PmaxParallelForks=6\u0026rdquo; to increase the number of test process to start in parallel. The default value is number of cores / 2, and noted that too many tests running in parallel may easily produce tests timeout.   Code Style Auto-Apply Use this task to make sure your added code will have the same format and conventions with the rest of codebase.\n$ ./gradlew spotlessApply   we have this style check in early QA build.   License Auto-Apply If you have added any new files in a PR. This task will automatically insert an Apache 2.0 license header in each one of these newly created files\n$ ./gradlew licenseApply   Note that a file without the license header will fail at early QA build   Publish Artifacts $ ./gradlew clean bintrayUpload -PskipManager -PbintrayUser=$user -PbintrayKey=$key    bintrayUser: the account that has write permission to the repository bintrayKey: the account API Key public: whether to auto published after uploading. default is false override: whether to override version artifacts already published. default is false     Note: Only release manager has permission to upload artifacts\n Publish Artifacts $ ./gradlew clean build publishToMavenLocal -PskipManager -x test  Installation see User Guide\n","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597894011,"objectID":"3382f0b0177169a69aff7d385068858b","permalink":"https://oharastream.github.io/en/docs/0.11.x/build/build-ohara/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/0.11.x/build/build-ohara/","section":"docs","summary":"Prerequisites  OpenJDK 11 Scala 2.13.2 Gradle 6.5 Node.js 8.12.0 Yarn 1.13.0 or greater Docker 19.03.8 (Docker multi-stage, which is supported by Docker 17.05 or higher, is required in building ohara images.","tags":null,"title":"How to build Ohara","type":"docs"},{"authors":null,"categories":null,"content":"Prerequisites  OpenJDK 11 Scala 2.13.2 Gradle 6.5 Node.js 8.12.0 Yarn 1.13.0 or greater Docker 19.03.8 (Docker multi-stage, which is supported by Docker 17.05 or higher, is required in building ohara images. see Use multi-stage builds for more details) Kubernetes 1.18.1 (Official QA uses the Kubernetes version is 1.18.1)   Gradle Commands Ohara build is based on gradle. Ohara has defined many gradle tasks to simplify the development of Ohara.\nBuild Binary $ ./gradlew clean build -x test   the tar file is located at ohara-${module}/build/distributions   Run All UTs $ ./gradlew clean test   Ohara IT tests requires specific envs, and all IT tests will be skipped if you don\u0026rsquo;t pass the related setting to IT. Ohara recommends you testing your code on official QA which offers the powerful machine and IT envs.    add flag \u0026ldquo;-PskipManager\u0026rdquo; to skip the tests of Ohara Manager. Ohara Manager is a module in charge of Ohara UI. Feel free to skip it if you are totally a backend developer. By the way, the prerequisites of testing Ohara Manager is shown in here \u0026lt;managerdev\u0026gt;    add flag \u0026ldquo;-PmaxParallelForks=6\u0026rdquo; to increase the number of test process to start in parallel. The default value is number of cores / 2, and noted that too many tests running in parallel may easily produce tests timeout.   Code Style Auto-Apply Use this task to make sure your added code will have the same format and conventions with the rest of codebase.\n$ ./gradlew spotlessApply   we have this style check in early QA build.   License Auto-Apply If you have added any new files in a PR. This task will automatically insert an Apache 2.0 license header in each one of these newly created files\n$ ./gradlew licenseApply   Note that a file without the license header will fail at early QA build   Publish Artifacts $ ./gradlew clean bintrayUpload -PskipManager -PbintrayUser=$user -PbintrayKey=$key    bintrayUser: the account that has write permission to the repository bintrayKey: the account API Key public: whether to auto published after uploading. default is false override: whether to override version artifacts already published. default is false     Note: Only release manager has permission to upload artifacts\n Publish Artifacts $ ./gradlew clean build publishToMavenLocal -PskipManager -x test  Installation see User Guide\n","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592897539,"objectID":"eb5d28f18aae5772fd06be8a447e5a5b","permalink":"https://oharastream.github.io/en/docs/master/build/build-ohara/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/master/build/build-ohara/","section":"docs","summary":"Prerequisites  OpenJDK 11 Scala 2.13.2 Gradle 6.5 Node.js 8.12.0 Yarn 1.13.0 or greater Docker 19.03.8 (Docker multi-stage, which is supported by Docker 17.05 or higher, is required in building ohara images.","tags":null,"title":"How to build Ohara","type":"docs"},{"authors":null,"categories":null,"content":"  Create a new workspace with three nodes  Create new topic into the workspace  Create a new pipeline, add a jdbc source connector, topics and a ftp sink connector  連接 JDBC source -\u0026gt; Topic -\u0026gt; FTP sink  Prepare the required table and data on the PostgreSQL server  Prepare the required output folder on the FTP server  Start all connectors on the secondpipeline page  Verify which test data was successfully dumped to FTP server  Create a new workspace with three nodes Add three nodes:\n On the Nodes page, click the NEW NODE button. Enter \u0026lt;ohara_node_host\u0026gt; in the Node field. Enter \u0026lt;ohara_node_port\u0026gt; in the Port field. Enter \u0026lt;node_user\u0026gt; in the User field. Enter \u0026lt;node_password\u0026gt; in the Password field. Click TEST CONNECTION to test your connection. If the test passes, click ADD. Repeat the above steps to add three nodes.  Create a new workspace:\n On the Workspaces page, click the NEW WORKSPACE button. Enter “wk01” in the Name field. Select all available nodes from the Node List. Add plugins:  Click the NEW PLUGIN button. Select postgresql-9.1-901-1.jdbc4.jar form your disk (download the jar here), the jar should be displayed in the Plugin List, make sure you check the jar you just uploaded.   Click ADD.  Create a new topic in the workspace Add a new topic:\n On the Workspaces \u0026gt; wk01 \u0026gt; TOPICS tab, click the NEW TOPIC button. Enter “t3” in the Topic name field and enter default value in other fields, click ADD.  Create a new pipeline, add a jdbc source connector, topics and a ftp sink connector  On the Pipelines list page, click the NEW PIPLINE button. Enter “secondpipeline” in the Pipeline name field and select “wk01” from the Workspace name dropdown. Then, click ADD. Click the Add a source connector icon and select oharastream.ohara.connector.jdbc.source.JDBCSourceConnector from the list, then click ADD. Enter “jdbcsource” in the myconnector field and click ADD. Click the Add a topic icon and select t3 from the dropdown and click ADD. Click the Add a sink connector icon and select oharastream.ohara.connector.ftp.FtpSink from the list, then click ADD. Enter “ftpsink” in the myconnector field and click ADD button.  連接 JDBC source -\u0026gt; Topic -\u0026gt; FTP sink Set up jdbcsource connector:\n On the secondpipeline page, click the jdbcsource graph in the pipeline graph. Select the COMMON tab, fill out the form with the following config:  Enter “\u0026lt;jdbc_url\u0026gt;” (jdbc url for PostgreSQL server) in the jdbc url field. Enter “\u0026lt;database_username\u0026gt;“ in the user name field. Enter “\u0026lt;database_password\u0026gt;“ in the password field. Enter “\u0026lt;database_table\u0026gt;” in the table name field. Enter “\u0026lt;table_timestamp\u0026gt;” in the timestamp column name field. Change flush size from 1000 to 10 in the JDBC flush Size field.   Select the CORE tab, select the t3 from the Topics dropdown.  Set up ftpsink connector:\n Click the ftpsink object in pipeline graph. Select the COMMON tab, enter the following in the fields.  Enter “/demo/output” in the Output Folder field. Click the write header checkbox, make it checked. Enter \u0026lt;ftp_server_ip\u0026gt; in the Hostname of FTP Server field. Enter \u0026lt;ftp_server_port\u0026gt; in the Port of FTP Server field. Enter ftp_username in the User of FTP Server field. Enter ftp_password in the Password of FTP Server field.   Select the CORE tab and select t3 from the Topics dropdown.  Prepare the required output folder on the FTP server  Open a terminal, login to FTP server (or use a FTP client of your choice)  $ ftp `ftp_server_ip` Name: `ftp_username` Password: `ftp_password`  Create the following folders.  ftp\u0026gt; mkdir demo ftp\u0026gt; cd demo ftp\u0026gt; mkdir output ftp\u0026gt; bye  Prepare the required table and data on the PostgreSQL server Check database has table and data:\n Open a terminal and log into the PostgreSQL server.  $ psql -h \u0026lt;PostgreSQL_server_ip\u0026gt; -W \u0026lt;database_name\u0026gt; -U \u0026lt;user_name\u0026gt;  check table is exist  postgres=# \\dt List of relations Schema | Name | Type | Owner --------+-------------+-------+------- public | person_data | table | ohara public | test_data | table | ohara (2 rows)  check table info  postgres=# \\d person_data Table \u0026quot;public.person_data\u0026quot; Column | Type | Collation | Nullable | Default -----------+-----------------------------+-----------+----------+--------- index | integer | | not null | name | character varying | | | age | integer | | | id | character varying | | | timestamp | timestamp without time zone | | | now()  check table has data  postgres=# select * from person_data; index | name | age | id | timestamp -------+---------+-----+------------+--------------------- 1 | Sam | 33 | H123378803 | 2019-03-08 18:52:00 2 | Jay | 25 | A159330943 | 2019-03-08 18:53:00 3 | Leon | 31 | J156498160 | 2019-03-08 19:52:00 4 | Stanley | 40 | D113134484 | 2019-03-08 20:00:00 5 | Jordan | 21 | U141236791 | 2019-03-08 20:10:20 6 | Kayden | 20 | E290773637 | 2019-03-09 18:52:59 7 | Dillon | 28 | M225842758 | 2019-03-09 20:52:59 8 | Ross | 33 | F229128254 | 2019-03-09 20:15:59 9 | Gunnar | 50 | Q107872026 | 2019-03-09 21:00:59 10 | Tyson | 26 | N197744193 | 2019-03-09 21:05:59 ..........   How to create table and insert data?\nStart all connectors on the secondpipeline page  On the secondpipeline page. Click the Start pipeline icon.  Verify which test data was successfully dumped to FTP server  Open a terminal, login to FTP server.  $ ftp `ftp_server_ip` Name: `ftp_username` Password: `ftp_password`  list all result CSV files in /demo/output/wk01-t3/partition0 folder.  $ ftp ls /demo/output/wk01-t3/partition0 /demo/output/wk01-t3/partition0/wk01-t3-0-000000000.csv  get the CSV file from ftp server to local.  $ ftp cd /demo/output/wk01-t3/wk01 $ ftp get wk01-t3-0-000000000.csv ./wk01-t3-0-000000000.csv $ ftp bye  View the content of wk01-t3-0-000000000.csv.  $ cat wk01-t3-0-000000000.csv index,name,age,id,timestamp 1,Sam,33,H123378803,2019-03-08 18:52:00.0 2,Jay,25,A159330943,2019-03-08 18:53:00.0 3,Leon,31,J156498160,2019-03-08 19:52:00.0 4,Stanley,40,D113134484,2019-03-08 20:00:00.0 5,Jordan,21,U141236791,2019-03-08 20:10:20.0 6,Kayden,20,E290773637,2019-03-09 18:52:59.0 8,Ross,33,F229128254,2019-03-09 20:15:59.0 7,Dillon,28,M225842758,2019-03-09 20:52:59.0 9,Gunnar,50,Q107872026,2019-03-09 21:00:59.0 10,Tyson,26,N197744193,2019-03-09 21:05:59.0  How to create table and insert data?  Create table person_data.  postgres=# create table person_data ( postgres=# index INTEGER NOT NULL, postgres=# name character varying, postgres=# age INTEGER, postgres=# id character varying, postgres=# timestamp timestamp without time zone DEFAULT NOW() postgres=# );  insert data into table person_data.  postgres=# insert into person_data (index,name,age,id)values(1,'Sam',33,'H123378803'), (2,'Jay',25,'A159330943'), (3,'Leon',31,'J156498160'), (4,'Stanley',40,'D113134484'), (5,'Jordan',21,'U141236791'), (6,'Kayden',20,'E290773637'), (7,'Dillon',28,'M225842758'), (8,'Ross',33,'F229128254'), (9,'Gunnar',50,'Q107872026'), (10,'Tyson',26,'N197744193');  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592818616,"objectID":"4ed64cde03ea6148331a6cb801fe9f36","permalink":"https://oharastream.github.io/en/testcases/pipeline-jdbc-topic-ftp/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/testcases/pipeline-jdbc-topic-ftp/","section":"testcases","summary":"Create a new workspace with three nodes Create new topic into the workspace Create a new pipeline, add a jdbc source connector, topics and a ftp sink connector 連接","tags":null,"title":"JDBC to Topic to FTP","type":"docs"},{"authors":null,"categories":null,"content":"K8S Mode   Create new workspace with three nodes  Create two topics into the workspace  Create new pipeline  Add jdbc source connector, topics and a hdfs sink connector  Link JDBC source -\u0026gt; Stream -\u0026gt; HDFS sink  Prepare the required table and data on the PostgreSQL server  Start all components on the pipeline page  Verify which test data was successfully dumped to HDFS  Docker Mode Will accomplish this section in another issue\n Create new workspace with three nodes  Click QUICK START button. In the About this workspace step, enter \u0026ldquo;wk01\u0026rdquo; in the text field. Click NEXT button. In the Select nodes step, click Select nodes button. Select three of available nodes from the Nodes list. Click SAVE button. In the Upload or select worker plugins(Optional) step, Click NEXT. In the Create this workspace step, make sure  Workspace Name is \u0026ldquo;wk01\u0026rdquo;. Node Names has three nodes which are decided by us. Plugins is empty.   Click FINISH button. Wait the creation finished, the url will be redirected to http://{url}/wk01  Create two topics in the workspace  Click th down arrow of workspace \u0026ldquo;wk01\u0026rdquo;. Select Topics. Click ADD TOPIC. Enter \u0026ldquo;topic1\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;1\u0026rdquo; in order of form fields. Click ADD TOPIC again. Enter \u0026ldquo;topic2\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;1\u0026rdquo; in order of form fields. Click the left arrow to go back the homepage.  Create new pipeline  Click the plus symbol besides the Pipelines to add a pipeline. Enter \u0026ldquo;pipeline1\u0026rdquo; in the text field. Click ADD button.  Add jdbc source connector, topics and hdfs sink connector  Click Source in the Toolbox. Drag and drop the JDBCSourceCOnnector in the paper. Enter \u0026ldquo;jdbc\u0026rdquo; in the text field. Click ADD button. Click Topic in the Toolbox. Drag and drop the topic1 and topic2 in the paper. Click Sink in the Toolbox. Drag and drop the HDFSSink in the paper. Enter \u0026ldquo;hdfs\u0026rdquo; in the text field. Click ADD button.  Add stream jar  Click Stream in the Toolbox.   Click Add streams plus symbol in the Toolbox.   Browse to the location of your jar file ohara-it-stream.jar, click the file, and click Open.  if You don\u0026rsquo;t know how to build a stream app jar, see this link for how\n  Press F5 to fetch stream class in the Toolbox.  Link JDBC source -\u0026gt; Stream -\u0026gt; HDFS sink Set up jdbc source connector:\n On the pipeline1 page, click the jdbc component in the pipeline graph. Select the COMMON tab, fill out the form with the following config:  Enter \u0026ldquo;\u0026lt;jdbc_url\u0026gt;\u0026rdquo; (jdbc url for PostgreSQL server) in the jdbc url field. Enter \u0026ldquo;\u0026lt;database_username\u0026gt;\u0026rdquo; in the user name field. Enter \u0026ldquo;\u0026lt;database_password\u0026gt;\u0026rdquo; in the password field. Enter \u0026ldquo;\u0026lt;database_table\u0026gt;\u0026rdquo; in the table name field. Enter \u0026ldquo;\u0026lt;table_timestamp\u0026gt;\u0026rdquo; in the timestamp column name field. Change flush size from 1000 to 10 in the JDBC flush Size field.   Select the CORE tab, select the t3 from the Topics dropdown.  Set up hdfs sink connector:\n On the pipeline1 page, click the hdfs component in the pipeline graph. Select the COMMON tab and fill out the following config:  Enter \u0026ldquo;/data\u0026rdquo; in the Output Folder field Change Flush Size value from \u0026ldquo;1000\u0026rdquo; to \u0026ldquo;5\u0026rdquo; Check the File Need Header checkbox Enter \u0026ldquo;hdfs://\u0026lt;hdfs_host\u0026gt;:\u0026lt;hdfs_port\u0026gt;\u0026rdquo; in the HDSF URL field   Select the CORE tab and choose t2 from the Topics dropdown.  Prepare the required table and data on the PostgreSQL server Check database has table and data:\n Open a terminal and log into the PostgreSQL server.  $ psql -h \u0026lt;PostgreSQL_server_ip\u0026gt; -W \u0026lt;database_name\u0026gt; -U \u0026lt;user_name\u0026gt;  check table is exist  postgres=# \\dt List of relations Schema | Name | Type | Owner --------+-------------+-------+------- public | person_data | table | ohara public | test_data | table | ohara (2 rows)  check table info  postgres=# \\d person_data Table \u0026quot;public.person_data\u0026quot; Column | Type | Collation | Nullable | Default -----------+-----------------------------+-----------+----------+--------- index | integer | | not null | name | character varying | | | age | integer | | | id | character varying | | | timestamp | timestamp without time zone | | | now()  check table has data  postgres=# select * from person_data; index | name | age | id | timestamp -------+---------+-----+------------+--------------------- 1 | Sam | 33 | H123378803 | 2019-03-08 18:52:00 2 | Jay | 25 | A159330943 | 2019-03-08 18:53:00 3 | Leon | 31 | J156498160 | 2019-03-08 19:52:00 4 | Stanley | 40 | D113134484 | 2019-03-08 20:00:00 5 | Jordan | 21 | U141236791 | 2019-03-08 20:10:20 6 | Kayden | 20 | E290773637 | 2019-03-09 18:52:59 7 | Dillon | 28 | M225842758 | 2019-03-09 20:52:59 8 | Ross | 33 | F229128254 | 2019-03-09 20:15:59 9 | Gunnar | 50 | Q107872026 | 2019-03-09 21:00:59 10 | Tyson | 26 | N197744193 | 2019-03-09 21:05:59 ..........   How to create table and insert data?\nStart all components on the pipeline page  On the pipeline1 page. Click the PIPELINE Actions on the top tab. Click Start all components.  Verify which test data was successfully dumped to HDFS  Open a terminal and ssh to HDFS server. List all CSV files in /data/wk00-t2/partition0 folder:  $ hdfs dfs -ls /data/wk00-t2/partition0 # You should see something similar like this in your terminal: /data/wk00-t2/partition0/part-000000000-000000005.csv  View the content of part-000000000-000000005.csv:  $ hdfs dfs -cat /data/wk00-t2/partition0/part-000000000-000000005.csv # The result should be like the following: ID,NAME,CREATE_AT 1,ohara1,2019-03-01 00:00:01  How to create table and insert data?  Create table person_data.  postgres=# create table person_data ( postgres=# index INTEGER NOT NULL, postgres=# name character varying, postgres=# age INTEGER, postgres=# id character varying, postgres=# timestamp timestamp without time zone DEFAULT NOW() postgres=# );  insert data into table person_data.  postgres=# insert into person_data (index,name,age,id)values(1,'Sam',33,'H123378803'), (2,'Jay',25,'A159330943'), (3,'Leon',31,'J156498160'), (4,'Stanley',40,'D113134484'), (5,'Jordan',21,'U141236791'), (6,'Kayden',20,'E290773637'), (7,'Dillon',28,'M225842758'), (8,'Ross',33,'F229128254'), (9,'Gunnar',50,'Q107872026'), (10,'Tyson',26,'N197744193');  How to get ohara-it-stream.jar? Open a new terminal from your machine and go to Ohara\u0026rsquo;s source folder.\ncd ohara/  Then cd to Stream DumbStream source folder.\ncd ohara-it/src/main/scala/oharastream/ohara/it/stream/  Use Vi to edit DumbStream.scala\nvi DumbStream.scala  Enter \u0026ldquo;I\u0026rdquo; key from your keyboard to activate the \u0026ldquo;INSERT\u0026rdquo; mode\n-- INSERT --  Overwrite DumbStream class from\nclass DumbStream extends Stream { override def start(ostream: OStream[Row], configs: StreamSetting): Unit = { // do nothing but only start stream and write exactly data to output topic ostream.start() } }  To\nimport oharastream.ohara.common.data.Row import oharastream.ohara.common.setting.SettingDef import oharastream.ohara.stream.config.StreamSetting import oharastream.ohara.stream.{OStream, Stream} class DumbStream extends Stream { override def config(): StreamDefinitions = StreamDefinitions .`with`( SettingDef .builder() .key(\u0026quot;filterValue\u0026quot;) .displayName(\u0026quot;filter value\u0026quot;) .documentation(\u0026quot;filter the row that contains this value\u0026quot;) .build()) override def start(ostream: OStream[Row], configs: StreamDefinitions): Unit = { ostream // we filter row which the cell values contain the pre-defined \u0026quot;filterValue\u0026quot;. Note: // 1) configs.string(\u0026quot;filterValue\u0026quot;) will try to get the value from env and it should NOT be null // 2) we ignore case for the value (i.e., \u0026quot;aa\u0026quot; = \u0026quot;AA\u0026quot;) .filter( r =\u0026gt; r.cells() .stream() .anyMatch(c =\u0026gt; { c.value().toString.equalsIgnoreCase(configs.string(\u0026quot;filterValue\u0026quot;)) })) .start() } }  Press \u0026ldquo;Esc\u0026rdquo; key to leave the insert mode and enter :wq to save and exit from this file:\n:wq  Build stream jar\n# Make sure you're at the project root `/ohara`, then build the jar with: ./gradlew clean :ohara-it:jar -PskipManager  Go to stream jar folder and list the jars that you have\ncd ohara-it/build/libs/ \u0026amp;\u0026amp; ls # You should see something like this in your terminal: ohara-it-sink.jar ohara-it-source.jar ohara-it-stream.jar  ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592818616,"objectID":"53154a81a8b17a7229ff6008eb449dc0","permalink":"https://oharastream.github.io/en/testcases/pipeline-jdbc-topic-stream-topic-hdfs/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/testcases/pipeline-jdbc-topic-stream-topic-hdfs/","section":"testcases","summary":"K8S Mode   Create new workspace with three nodes  Create two topics into the workspace  Create new pipeline  Add jdbc source connector, topics and a hdfs sink connector  Link JDBC source -\u0026gt; Stream -\u0026gt; HDFS sink  Prepare the required table and data on the PostgreSQL server  Start all components on the pipeline page  Verify which test data was successfully dumped to HDFS  Docker Mode Will accomplish this section in another issue","tags":null,"title":"JDBC to Topic to Stream to HDFS","type":"docs"},{"authors":null,"categories":null,"content":"  Use the hardware specification  Use version information  Install the Kubernetes  Running the Ohara Configuration service  Running the Ohara Manager service  Use the hardware specification   Preparation 5 nodes\n  K8S Master: 1 個 Node, CPU 4 Core, Memory 4 GB, Disk 100 GB\n  K8S Slave: 3 個 Node, CPU 4 Core, Memory 4 GB, Disk 100 GB\n  Ohara Configurator 和 Ohara Manager 在同一台 Node, CPU 4 Core, Memory 4 GB, Disk 100 GB\n    HostName and IP Information\n  k8s-m-test 10.100.0.140\n  k8s-s-test-0 10.100.0.169\n  k8s-s-test-1 10.100.0.167\n  k8s-s-test-2 10.100.0.114\n  ohara-configurator 10.2.0.32\n  ohara-manager 10.2.0.32\n    Environment   Operating System: CentOS 7.6.1810\n  Docker: 19.03.8\n  Kubernetes: 1.18.1\n  Ohara: ${ohara version name} 這裡以 0.7.0-SNAPSHOT 當做 example\n  Install Kubernetes   Please refer to the document\n  需要在每台 Kubernetes 的 Slave Node Pull zookeeper, broker, connector-worker 和 stream 的 docker image，指令如下：\n  $ docker pull oharastream/zookeeper:${ohara version name} $ docker pull oharastream/broker:${ohara version name} $ docker pull oharastream/connect-worker:${ohara version name} $ docker pull oharastream/stream:${ohara version name}  Example\n$ docker pull oharastream/zookeeper:0.7.0-SNAPSHOT $ docker pull oharastream/broker:0.7.0-SNAPSHOT $ docker pull oharastream/connect-worker:0.7.0-SNAPSHOT $ docker pull oharastream/stream:0.7.0-SNAPSHOT  Running Ohara Configuration service   使用 ssh 登入到要啟動 Ohara Configuration Service 的 node 裡\n  Pull Ohara Configurator docker image\n  $ docker pull oharastream/configurator:${ohara version name}  Example:\n$ docker pull oharastream/configurator:0.7.0-SNAPSHOT  執行 Ohara Configurator service Docker container  $ docker run --rm \\ -p 5000:5000 \\ --add-host ${K8S_WORKER01_HOSTNAME}:${K8S_WORKER01_IP} \\ --add-host ${K8S_WORKER02_HOSTNAME}:${K8S_WORKER02_IP} \\ oharastream/configurator:0.7.0-SNAPSHOT \\ --port 5000 \\ --hostname ${Start Configurator Host Name} \\ --k8s http://${Your_K8S_Master_Host_IP}:8080/api/v1  Example:\n$ docker run -d --rm \\ -p 5000:5000 \\ --add-host k8s-m-test:10.100.0.140 \\ --add-host k8s-s-test-0:10.100.0.169 \\ --add-host k8s-s-test-1:10.100.0.167 \\ --add-host k8s-s-test-2:10.100.0.114 \\ oharastream/configurator:0.7.0-SNAPSHOT \\ --port 5000 \\ --hostname 10.2.0.32 \\ --k8s http://10.100.0.140:8080/api/v1  使用 docker 指令確認 Ohara Configurator 的 container 是否有啟動  $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6127177b18f oharastream/configurator:0.7.0-SNAPSHOT \u0026quot;/tini -- configurat…\u0026quot; About an hour ago Up About an hour 0.0.0.0:5000-\u0026gt;5000/tcp adoring_bartik  使用以下指令確認 Ohara Configurator 的 Restful API 是否能顯示版本資訊  $ curl -X GET http://${Your Configurator Node name}:5000/v0/info  Example:\n$ curl -X GET http://10.2.0.32:5000/v0/info {\u0026quot;versionInfo\u0026quot;:{\u0026quot;branch\u0026quot;:\u0026quot;master\u0026quot;,\u0026quot;revision\u0026quot;:\u0026quot;1b38aaff103c7fa84440c2bbdfc7699eafb0716f\u0026quot;,\u0026quot;version\u0026quot;:\u0026quot;0.7.0-SNAPSHOT\u0026quot;,\u0026quot;date\u0026quot;:\u0026quot;2019-08-17 17:09:26\u0026quot;,\u0026quot;user\u0026quot;:\u0026quot;root\u0026quot;},\u0026quot;mode\u0026quot;:\u0026quot;K8S\u0026quot;}  以上步驟不能正常執行，請使用 docker logs 指令查看 log，並且回報到 ohara GitHub 的 Ohara issues page  $ docker logs ${CONFIGURATOR CONTAINER ID}  Running the Ohara Manager service   使用 ssh 登入到要啟動 Ohara Manager Service 的 node 裡\n  Pull Ohara Manager docker image\n  $ docker pull oharastream/manager:${ohara version name}  Example\n$ docker pull oharastream/manager:0.7.0-SNAPSHOT  執行 Ohara Manager service Docker container  $ docker run -d --rm \\ -p ${ohara-manager-port}:5050 \\ oharastream/manager:${ohara version name} \\ --port ${ohara-manager-port} \\ --configurator http://${ohara-configurator-host}:${ohara-configurator-port}/v0  Example\n$ docker run -d --rm \\ -p 5050:5050 \\ oharastream/manager:0.7.0-SNAPSHOT \\ --port 5050 \\ --configurator http://10.2.0.32:5000/v0  使用 docker ps 指令查看 ohara manager service 的 container 是否有正常執行  $ docker ps -a # Output CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1f1b2cfac5d8 oharastream/manager:0.7.0-SNAPSHOT \u0026quot;/tini -- manager.sh…\u0026quot; 4 seconds ago Up 3 seconds 0.0.0.0:5050-\u0026gt;5050/tcp sleepy_nightingale  使用 docker logs 指令查看 ohara manager service container 的 log  $ docker logs ${CONTAINER ID}  Example\n$ docker logs -f 1f1b2cfac5d8 # Manager's log [HPM] Proxy created: / -\u0026gt; http://10.2.0.32:5000/v0 [HPM] Proxy rewrite rule created: \u0026quot;/api\u0026quot; ~\u0026gt; \u0026quot;\u0026quot; Ohara manager is running at port: 5050 Successfully connected to the configurator: http://10.2.0.32:5000/v0  開啟 ohara manager 的 web 畫面，在 browser 的 URL 輸入以下網址：  http://10.2.0.32:5050  就可以看到 Ohara manager 的畫面\n","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592818616,"objectID":"f8017a4ffe15e8f5e5df3cdd36feb7b9","permalink":"https://oharastream.github.io/en/testcases/ohara-installation/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/testcases/ohara-installation/","section":"testcases","summary":"Use the hardware specification Use version information Install the Kubernetes Running the Ohara Configuration service Running the Ohara Manager service Use the hardware specification Preparation 5 nodes K8S Master: 1","tags":null,"title":"Ohara installation","type":"docs"},{"authors":null,"categories":null,"content":"Shortcode Hugo \u0026amp; Academic  https://sourcethemes.com/academic/docs/writing-markdown-latex/ https://gohugo.io/content-management/shortcodes/  Custom by Ohara  Branch: master Version: 0.11.0-SNAPSHOT Ohara file link: backend docker file Ohara issue link: #5266 Ohara file content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155  /* * Copyright 2019 is-land * * Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package oharastream.ohara.common.data; import java.util.Iterator; import java.util.List; import java.util.NoSuchElementException; import java.util.Objects; import java.util.function.IntBinaryOperator; import java.util.stream.Collectors; /** * a collection from {@link Cell}. Also, {@link Row} can carry variable tags which can be used to * add more \u0026#34;description\u0026#34; to the {@link Row}. NOTED. the default implementation from {@link Row} has * implemented the */ public interface Row extends Iterable\u0026lt;Cell\u0026lt;?\u0026gt;\u0026gt; { /** a empty row. It means no tag and cell exist in this row. */ Row EMPTY = Row.of(); /** @return a collection from name from cells */ List\u0026lt;String\u0026gt; names(); /** * seek the cell by specified index * * @param index cell\u0026#39;s index * @return a cell or throw NoSuchElementException */ Cell\u0026lt;?\u0026gt; cell(int index); /** * seek the cell by specified name * * @param name cell\u0026#39;s name * @return a cell or throw NoSuchElementException */ Cell\u0026lt;?\u0026gt; cell(String name); /** @return a immutable collection from cells */ List\u0026lt;Cell\u0026lt;?\u0026gt;\u0026gt; cells(); /** * the default order from cells from this method is same to {@link #cells()} * * @return an iterator over the {@link Cell} in this row in proper sequence */ @Override default Iterator\u0026lt;Cell\u0026lt;?\u0026gt;\u0026gt; iterator() { List\u0026lt;Cell\u0026lt;?\u0026gt;\u0026gt; cells = cells(); if (cells == null) cells = List.of(); return cells.iterator(); } /** @return a immutable collection from tags */ List\u0026lt;String\u0026gt; tags(); /** @return the number from elements in this row */ default int size() { return cells().size(); } /** * Compare all cells one-by-one. Noted: the order from cells doesn\u0026#39;t impact the comparison. * * @param that another row * @param includeTags true if the tags should be considered in the comparison * @return true if both rows have same cells and tags (if includeTags is true) */ default boolean equals(Row that, boolean includeTags) { if (cells().size() != that.cells().size()) return false;  if (includeTags \u0026amp;\u0026amp; !tags().stream().allMatch(tag -\u0026gt; that.tags().stream().anyMatch(tag::equals))) return false; return cells().stream().allMatch(cell -\u0026gt; that.cells().stream().anyMatch(cell::equals)); } static Row of(Cell\u0026lt;?\u0026gt;... cells) { return of(List.of(), cells); } static Row of(List\u0026lt;String\u0026gt; tags, Cell\u0026lt;?\u0026gt;... cells) { var tagsCopy = List.copyOf(tags); var cellsCopy = List.of(cells); // check duplicate names  int numberOfNames = cellsCopy.stream().map(Cell::name).collect(Collectors.toUnmodifiableSet()).size(); if (numberOfNames != cellsCopy.size()) throw new IllegalArgumentException(\u0026#34;Row can\u0026#39;t accept duplicate cell name\u0026#34;); return new Row() { @Override public List\u0026lt;String\u0026gt; names() { return cellsCopy.stream().map(Cell::name).collect(Collectors.toUnmodifiableList()); } @Override public Cell\u0026lt;?\u0026gt; cell(int index) { Cell\u0026lt;?\u0026gt; cell = cellsCopy.get(index); if (cell == null) throw new NoSuchElementException(\u0026#34;no cell exists with index:\u0026#34; + index); return cell; } @Override public Cell\u0026lt;?\u0026gt; cell(String name) { return cellsCopy.stream() .filter(c -\u0026gt; c.name().equals(name)) .findFirst() .orElseThrow(() -\u0026gt; new NoSuchElementException(\u0026#34;no cell exists with name:\u0026#34; + name)); } @Override public List\u0026lt;Cell\u0026lt;?\u0026gt;\u0026gt; cells() { return cellsCopy; } @Override public List\u0026lt;String\u0026gt; tags() { return tagsCopy; } @Override public int hashCode() { IntBinaryOperator accumulate = (hash, current) -\u0026gt; hash * 31 + current; return 31 * cells().stream().mapToInt(Objects::hashCode).reduce(1, accumulate) + tags.stream().mapToInt(Objects::hashCode).reduce(1, accumulate); } @Override public boolean equals(Object obj) { if (obj == this) return true; if (obj instanceof Row) return equals((Row) obj, true); return false; } @Override public String toString() { return \u0026#34;cells:\u0026#34; + cells() + \u0026#34;, tags:\u0026#34; + tags; } }; } }     ","date":1592175600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593038330,"objectID":"996eac46e5c81311d943e56ab4f5aa7b","permalink":"https://oharastream.github.io/en/docs/master/write-content/","publishdate":"2020-06-15T00:00:00+01:00","relpermalink":"/en/docs/master/write-content/","section":"docs","summary":"Shortcode Hugo \u0026amp; Academic  https://sourcethemes.com/academic/docs/writing-markdown-latex/ https://gohugo.io/content-management/shortcodes/  Custom by Ohara  Branch: master Version: 0.11.0-SNAPSHOT Ohara file link: backend docker file Ohara issue link: #5266 Ohara file content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155  /* * Copyright 2019 is-land * * Licensed under the Apache License, Version 2.","tags":null,"title":"Write content example","type":"docs"},{"authors":null,"categories":null,"content":"Ohara volume feature is persisting data by zookeeper container and broker container.\nNote: Confirm your user name is ohara and UUID is 1000 in the running cluster environment.\nThe properties which can be set by user are shown below.\n  name (String) \u0026mdash; The legal character is lowercase alphanumerics characters and number.\n  group (String) \u0026mdash; The legal character is lowercase alphanumerics characters and number.\n  path (String) \u0026mdash; The data folder path in the node.\n  nodeNames (Set[String]) \u0026mdash; The nodes to create ohara volume for zookeeper and broker containers.\n  tags (Object) \u0026mdash; User-defined parameters.\n  create a volume properties POST /v0/volumes\n  Example Request\n{ \u0026quot;name\u0026quot;: \u0026quot;volume\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;path\u0026quot;: \u0026quot;/tmp/workspace\u0026quot; }    Example Response\n{ \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;lastModified\u0026quot;: 1606457846012, \u0026quot;name\u0026quot;: \u0026quot;volume1\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot; ], \u0026quot;path\u0026quot;: \u0026quot;/tmp/workspace\u0026quot;, \u0026quot;tags\u0026quot;: {} }     The /tmp/workspace data folder own MUST set ohara user name and UID is 1000\nstart a ohara volume PUT /v0/volumes/$name/start?group=$group\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Resposne 202 Accepted     You should use Get volume info to show volume up-to-date status.\nget a volume info GET /v0/volumes/$name?group=$group\nGet volume info by name and group. This API could fetch all volume info. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Response { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;lastModified\u0026quot;: 1606457846012, \u0026quot;name\u0026quot;: \u0026quot;volume1\u0026quot;, \u0026quot;nodeNames\u0026quot;: [ \u0026quot;k8s-master\u0026quot; ], \u0026quot;path\u0026quot;: \u0026quot;/tmp/workspace1/bk12345\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;RUNNING\u0026quot;, \u0026quot;tags\u0026quot;: {} }    add a volume to a running volumes PUT /v0/volumes/$name/$nodeName?group=$group\nAdd a new volume for other node. We will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n Example Resposne 202 Accepted    update volume properties setting PUT /v0/volumes/$name?group=$group\nIf the required volume (group, name) doesn\u0026rsquo;t exists, we will try to use this request as POST.\n Example Request { \u0026quot;nodeNames\u0026quot;: [ \u0026quot;node00\u0026quot;, \u0026quot;node01\u0026quot; ], \u0026quot;path\u0026quot;: \u0026quot;/tmp/workspace2\u0026quot; }   Example Response { \u0026quot;group\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;lastModified\u0026quot;: 1606465299352, \u0026quot;name\u0026quot;: $name, \u0026quot;nodeNames\u0026quot;: [\u0026quot;node00\u0026quot;, \u0026quot;node01\u0026quot;], \u0026quot;path\u0026quot;: \u0026quot;/tmp/workspace2\u0026quot;, \u0026quot;tags\u0026quot;: {} }    stop a volume info Stop a running volume and remove the volume data folder (dangerous). It is disallowed to stop a volume cluster used by a running zookeeper or broker container.\nPUT /v0/volumes/$name/stop?group=$group[\u0026amp;force=true]\nWe will use the default value as the query parameter \u0026ldquo;?group=\u0026rdquo; if you don\u0026rsquo;t specify it.\n  Query Parameters\n force (boolean) - true if you don\u0026rsquo;t want to wait the graceful shutdown (it can save your time but may damage your data).    Example Response\n202 Accepted    delete a volume properties DELETE /v0/volumes/$name?group=$group\nAfter the stop volume, you could delete volume setting.\n Example Response 202 Accepted        ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606703241,"objectID":"07257ea724b87d2ca2194d84052bffbf","permalink":"https://oharastream.github.io/en/docs/master/rest-api/volumes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/docs/master/rest-api/volumes/","section":"docs","summary":"Ohara volume feature is persisting data by zookeeper container and broker container.\nNote: Confirm your user name is ohara and UUID is 1000 in the running cluster environment.\nThe properties which can be set by user are shown below.","tags":null,"title":"Volume","type":"docs"}]